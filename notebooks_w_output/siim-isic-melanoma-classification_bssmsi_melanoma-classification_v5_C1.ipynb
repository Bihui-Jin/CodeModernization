{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673ae9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:29.474420Z",
     "iopub.status.busy": "2025-08-19T03:25:29.474157Z",
     "iopub.status.idle": "2025-08-19T03:25:30.180372Z",
     "shell.execute_reply": "2025-08-19T03:25:30.179867Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9637bc9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:30.182393Z",
     "iopub.status.busy": "2025-08-19T03:25:30.182135Z",
     "iopub.status.idle": "2025-08-19T03:25:35.737035Z",
     "shell.execute_reply": "2025-08-19T03:25:35.736554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 03:25:30.450751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755573930.464247       8 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755573930.468563       8 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-19 03:25:30.486992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aaf3c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:35.739110Z",
     "iopub.status.busy": "2025-08-19T03:25:35.738837Z",
     "iopub.status.idle": "2025-08-19T03:25:36.086982Z",
     "shell.execute_reply": "2025-08-19T03:25:36.086548Z"
    }
   },
   "outputs": [
    {
     "ename": "BackendError",
     "evalue": "Unexpected response from the service. Response: {'errors': ['Unauthenticated'], 'error': {'code': 16}, 'wasSuccessful': False}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8/867104700.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# tf.config.experimental_run_functions_eagerly(True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mGCS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gcs_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kaggle_datasets.py\u001b[0m in \u001b[0;36mget_gcs_path\u001b[0;34m(self, dataset_dir)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;34m'IntegrationType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mintegration_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         }\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_post_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_GCS_PATH_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIMEOUT_SECS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'destinationBucket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kaggle_web_client.py\u001b[0m in \u001b[0;36mmake_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wasSuccessful'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'result'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     raise BackendError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                         f'Unexpected response from the service. Response: {response_json}.')\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBackendError\u001b[0m: Unexpected response from the service. Response: {'errors': ['Unauthenticated'], 'error': {'code': 16}, 'wasSuccessful': False}."
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27d79ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:36.088345Z",
     "iopub.status.busy": "2025-08-19T03:25:36.088228Z",
     "iopub.status.idle": "2025-08-19T03:25:36.091852Z",
     "shell.execute_reply": "2025-08-19T03:25:36.091567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e77360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:36.093137Z",
     "iopub.status.busy": "2025-08-19T03:25:36.092867Z",
     "iopub.status.idle": "2025-08-19T03:25:36.123740Z",
     "shell.execute_reply": "2025-08-19T03:25:36.123371Z"
    }
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('../input/siim-isic-melanoma-classification/train.csv')\n",
    "test_csv = pd.read_csv('../input/siim-isic-melanoma-classification/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48322282",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:36.124980Z",
     "iopub.status.busy": "2025-08-19T03:25:36.124868Z",
     "iopub.status.idle": "2025-08-19T03:25:36.128762Z",
     "shell.execute_reply": "2025-08-19T03:25:36.128492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    28471\n",
       "1      513\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.target.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa717ad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:36.129935Z",
     "iopub.status.busy": "2025-08-19T03:25:36.129748Z",
     "iopub.status.idle": "2025-08-19T03:25:36.136456Z",
     "shell.execute_reply": "2025-08-19T03:25:36.136182Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GCS_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8/1944068296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_replicas_in_sync\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCS_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/tfrecords/train*.tfrec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCS_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/tfrecords/test*.tfrec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GCS_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = [1024, 1024]\n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "train_files = tf.io.gfile.glob(GCS_PATH + '/tfrecords/train*.tfrec')\n",
    "test_files = tf.io.gfile.glob(GCS_PATH + '/tfrecords/test*.tfrec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f8d7ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:36.137910Z",
     "iopub.status.busy": "2025-08-19T03:25:36.137665Z",
     "iopub.status.idle": "2025-08-19T03:25:36.149281Z",
     "shell.execute_reply": "2025-08-19T03:25:36.148972Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mget_train_vald_dataset\u001b[0;34m(vald_split, ordered)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMG_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example, labeled):\n",
    "    tfrecord_format = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    } if labeled else {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    if labeled:\n",
    "        label = tf.cast(example['target'], tf.int32)\n",
    "        return image, label\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def get_train_vald_dataset(vald_split=0.2, ordered=False):\n",
    "    dataset = load_dataset(train_files, labeled=True, ordered=ordered)\n",
    "    n = sum(1 for record in dataset)\n",
    "    n_vald = int(vald_split * n)\n",
    "    n_train = n - n_vald\n",
    "    #train_dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    train_dataset = dataset.take(n_train)\n",
    "    train_dataset = train_dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    train_dataset = train_dataset.shuffle(2048)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "    train_dataset = train_dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    \n",
    "    vald_dataset = dataset.skip(n_train)\n",
    "    n1 = sum(1 for rec in vald_dataset)\n",
    "    vald_dataset = vald_dataset.batch(BATCH_SIZE)\n",
    "    vald_dataset = vald_dataset.cache()\n",
    "    vald_dataset = vald_dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    if n_vald != n1:\n",
    "        print(\"Validation Dataset sizes - \", n_vald, n1)\n",
    "    return n_train, train_dataset, n1, vald_dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(test_files, labeled=False, ordered=ordered)\n",
    "    n = sum(1 for record in dataset)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return n, dataset\n",
    "\n",
    "n_train, train_dataset, n_vald, vald_dataset = get_train_vald_dataset()\n",
    "n_test, test_dataset = get_test_dataset(True)\n",
    "print(f'Dataset: {n_train} training images, {n_vald} validation images, {n_test} unlabeled test images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c4357fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:36.150401Z",
     "iopub.status.busy": "2025-08-19T03:25:36.150209Z",
     "iopub.status.idle": "2025-08-19T03:25:36.152925Z",
     "shell.execute_reply": "2025-08-19T03:25:36.152618Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name=\"\"):\n",
    "    #pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
    "    pretrained_model = tf.keras.applications.Xception(input_shape=[*IMG_SIZE, 3], include_top=False, weights='imagenet')\n",
    "    #pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
    "    #pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
    "    #pretrained_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
    "    # EfficientNet can be loaded through efficientnet.tfkeras library (https://github.com/qubvel/efficientnet)\n",
    "    #pretrained_model = efficientnet.tfkeras.EfficientNetB0(weights='imagenet', include_top=False)\n",
    "    \n",
    "    pretrained_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        pretrained_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        #tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7aad47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:36.154055Z",
     "iopub.status.busy": "2025-08-19T03:25:36.153874Z",
     "iopub.status.idle": "2025-08-19T03:25:38.030712Z",
     "shell.execute_reply": "2025-08-19T03:25:38.030257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 03:25:36.183250: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = initialize_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6bc4931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:38.032753Z",
     "iopub.status.busy": "2025-08-19T03:25:38.032401Z",
     "iopub.status.idle": "2025-08-19T03:25:38.042419Z",
     "shell.execute_reply": "2025-08-19T03:25:38.042128Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8/1379509966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTRAIN_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mVALID_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_vald\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_train' is not defined"
     ]
    }
   ],
   "source": [
    "TRAIN_STEPS = n_train // BATCH_SIZE\n",
    "VALID_STEPS = n_vald // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e618ff67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:38.043654Z",
     "iopub.status.busy": "2025-08-19T03:25:38.043442Z",
     "iopub.status.idle": "2025-08-19T03:25:38.049326Z",
     "shell.execute_reply": "2025-08-19T03:25:38.049054Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8/3147999053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(train_dataset, epochs=2, steps_per_epoch=TRAIN_STEPS, class_weight={0: 1, 1: 2},\n\u001b[0m\u001b[1;32m      2\u001b[0m                     validation_data=vald_dataset, validation_steps=VALID_STEPS)#, callbacks=[lr_callback])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=TRAIN_STEPS, class_weight={0: 1, 1: 2},\n",
    "                    validation_data=vald_dataset, validation_steps=VALID_STEPS)#, callbacks=[lr_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f39092dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:38.050494Z",
     "iopub.status.busy": "2025-08-19T03:25:38.050314Z",
     "iopub.status.idle": "2025-08-19T03:25:38.276395Z",
     "shell.execute_reply": "2025-08-19T03:25:38.275901Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ae62a65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:38.278326Z",
     "iopub.status.busy": "2025-08-19T03:25:38.277965Z",
     "iopub.status.idle": "2025-08-19T03:25:38.288094Z",
     "shell.execute_reply": "2025-08-19T03:25:38.287773Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8/315881472.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midnum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "outs = model.predict(test_dataset.map(lambda image, idnum: image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adac4436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:38.289129Z",
     "iopub.status.busy": "2025-08-19T03:25:38.288943Z",
     "iopub.status.idle": "2025-08-19T03:25:38.295826Z",
     "shell.execute_reply": "2025-08-19T03:25:38.295511Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8/530033242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_csv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'outs' is not defined"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame({'image_name': test_csv['image_name'], 'target': outs.ravel()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebe5cb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:25:38.297105Z",
     "iopub.status.busy": "2025-08-19T03:25:38.296850Z",
     "iopub.status.idle": "2025-08-19T03:25:38.301993Z",
     "shell.execute_reply": "2025-08-19T03:25:38.301730Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8/2516245329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submissions.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "pred.to_csv('submissions.csv', header=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
