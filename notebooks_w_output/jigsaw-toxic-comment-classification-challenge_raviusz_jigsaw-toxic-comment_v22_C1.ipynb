{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f381a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:51.070361Z",
     "iopub.status.busy": "2025-08-23T09:51:51.070099Z",
     "iopub.status.idle": "2025-08-23T09:51:51.783767Z",
     "shell.execute_reply": "2025-08-23T09:51:51.783293Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb604de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:51.785655Z",
     "iopub.status.busy": "2025-08-23T09:51:51.785397Z",
     "iopub.status.idle": "2025-08-23T09:51:51.787502Z",
     "shell.execute_reply": "2025-08-23T09:51:51.787223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Version 1.55: Bag of Words\n",
    "#Try keeping all features again to see how\n",
    "#So far in order of decreasing score we have 5000 > 4000 > 3000 > 1000 > 2000 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039ffd79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:51.788753Z",
     "iopub.status.busy": "2025-08-23T09:51:51.788562Z",
     "iopub.status.idle": "2025-08-23T09:51:52.565586Z",
     "shell.execute_reply": "2025-08-23T09:51:52.565162Z"
    }
   },
   "outputs": [],
   "source": [
    "## Import\n",
    "\n",
    "#Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import catboost\n",
    "from catboost import CatBoost\n",
    "\n",
    "#Tuning\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "#Settings\n",
    "pd.set_option('display.max_rows',None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd79a46f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:52.567315Z",
     "iopub.status.busy": "2025-08-23T09:51:52.567111Z",
     "iopub.status.idle": "2025-08-23T09:51:54.217783Z",
     "shell.execute_reply": "2025-08-23T09:51:54.217176Z"
    }
   },
   "outputs": [],
   "source": [
    "## Upload Data\n",
    "train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a81238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:54.219732Z",
     "iopub.status.busy": "2025-08-23T09:51:54.219599Z",
     "iopub.status.idle": "2025-08-23T09:51:54.230393Z",
     "shell.execute_reply": "2025-08-23T09:51:54.230085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a734df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:54.231567Z",
     "iopub.status.busy": "2025-08-23T09:51:54.231361Z",
     "iopub.status.idle": "2025-08-23T09:51:54.233741Z",
     "shell.execute_reply": "2025-08-23T09:51:54.233472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b928b84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:54.235047Z",
     "iopub.status.busy": "2025-08-23T09:51:54.234807Z",
     "iopub.status.idle": "2025-08-23T09:51:54.239402Z",
     "shell.execute_reply": "2025-08-23T09:51:54.239137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation edits made username hardcore metallica fan reverted vandalisms closure gas voted new york dolls fac please remove template talk page since retired'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Preprocess Pipeline Function\n",
    "def preprocess_text(comment):\n",
    "#     print('Step0', comment)\n",
    "    \n",
    "    #1. Remove HTML tags with Beautiful Soup\n",
    "    processed_comment = BeautifulSoup(comment)\n",
    "#     print('Step1', processed_comment)\n",
    "    \n",
    "    #2. Remove punctuation\n",
    "    processed_comment = re.sub('[^a-zA-Z]', ' ', processed_comment.get_text())\n",
    "    processed_comment = re.sub('[\\n]', ' ', processed_comment)\n",
    "#     print('Step2', processed_comment)\n",
    "\n",
    "    #3. Convert all letters to lowercase\n",
    "    processed_comment = processed_comment.lower()\n",
    "#     print('Step3', processed_comment)\n",
    "    \n",
    "    #4. Convert comment into array of word strings\n",
    "    processed_comment = processed_comment.split()\n",
    "#     print('Step4', processed_comment)\n",
    "    \n",
    "    #5. Remove stopwords such as 'a' and 'the'\n",
    "    stops = set(stopwords.words('english'))\n",
    "    processed_comment = [w for w in processed_comment if w not in stops]\n",
    "    \n",
    "    #6. Split comment into a paragraph string\n",
    "    return (' ').join(processed_comment)\n",
    "\n",
    "preprocess_text(train['comment_text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c84227c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:51:54.240628Z",
     "iopub.status.busy": "2025-08-23T09:51:54.240403Z",
     "iopub.status.idle": "2025-08-23T09:52:55.601778Z",
     "shell.execute_reply": "2025-08-23T09:52:55.601228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 159571 processed\n",
      "Review 10000 of 159571 processed\n",
      "Review 20000 of 159571 processed\n",
      "Review 30000 of 159571 processed\n",
      "Review 40000 of 159571 processed\n",
      "Review 50000 of 159571 processed\n",
      "Review 60000 of 159571 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8/3039429941.py:6: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  processed_comment = BeautifulSoup(comment)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 70000 of 159571 processed\n",
      "Review 80000 of 159571 processed\n",
      "Review 90000 of 159571 processed\n",
      "Review 100000 of 159571 processed\n",
      "Review 110000 of 159571 processed\n",
      "Review 120000 of 159571 processed\n",
      "Review 130000 of 159571 processed\n",
      "Review 140000 of 159571 processed\n",
      "Review 150000 of 159571 processed\n",
      "Review 0 of 153164 processed\n",
      "Review 10000 of 153164 processed\n",
      "Review 20000 of 153164 processed\n",
      "Review 30000 of 153164 processed\n",
      "Review 40000 of 153164 processed\n",
      "Review 50000 of 153164 processed\n",
      "Review 60000 of 153164 processed\n",
      "Review 70000 of 153164 processed\n",
      "Review 80000 of 153164 processed\n",
      "Review 90000 of 153164 processed\n",
      "Review 100000 of 153164 processed\n",
      "Review 110000 of 153164 processed\n",
      "Review 120000 of 153164 processed\n",
      "Review 130000 of 153164 processed\n",
      "Review 140000 of 153164 processed\n",
      "Review 150000 of 153164 processed\n"
     ]
    }
   ],
   "source": [
    "## Preprocess Train and Test Dataset\n",
    "cleaned_train_reviews = []\n",
    "cleaned_test_reviews = []\n",
    "\n",
    "# for i in range(0,100):\n",
    "for i in range(0,len(train)):\n",
    "    if i%10000 == 0:\n",
    "        print('Review %d of %d processed' % (i,len(train)))\n",
    "    cleaned_review = preprocess_text(train['comment_text'][i])\n",
    "    cleaned_train_reviews.append(cleaned_review)\n",
    "    \n",
    "# for i in range(0,100):\n",
    "for i in range(0,len(test)):\n",
    "    if i%10000 == 0:\n",
    "        print('Review %d of %d processed' % (i,len(test)))\n",
    "    cleaned_review = preprocess_text(test['comment_text'][i])\n",
    "    cleaned_test_reviews.append(cleaned_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a8d7d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:52:55.603386Z",
     "iopub.status.busy": "2025-08-23T09:52:55.603249Z",
     "iopub.status.idle": "2025-08-23T09:52:55.605939Z",
     "shell.execute_reply": "2025-08-23T09:52:55.605658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- explanation edits made username hardcore metallica fan reverted vandalisms closure gas voted new york dolls fac please remove template talk page since retired\n",
      "- aww matches background colour seemingly stuck thanks talk january utc\n",
      "- hey man really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info\n",
      "- make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later one else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipedia good article nominations transport\n",
      "- sir hero chance remember page\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('-', cleaned_train_reviews[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8573974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:52:55.607042Z",
     "iopub.status.busy": "2025-08-23T09:52:55.606852Z",
     "iopub.status.idle": "2025-08-23T09:53:05.404248Z",
     "shell.execute_reply": "2025-08-23T09:53:05.403745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1616 1430 2663 4714 2021 1675 3800 1881 4805 2945 4986 1639 3300 3703\n",
      " 4438 4401 3147 4075 3786]\n"
     ]
    }
   ],
   "source": [
    "## Create Bag of Words Counter\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "#                             ngram_range = (1,2),\n",
    "                            max_features = 5000)\n",
    "\n",
    "hello = vectorizer.fit_transform(cleaned_train_reviews)\n",
    "train_data_features = (vectorizer.fit_transform(cleaned_train_reviews)).toarray()\n",
    "test_data_features = (vectorizer.fit_transform(cleaned_test_reviews)).toarray()\n",
    "print(hello[0].indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d4d1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.405812Z",
     "iopub.status.busy": "2025-08-23T09:53:05.405675Z",
     "iopub.status.idle": "2025-08-23T09:53:05.441392Z",
     "shell.execute_reply": "2025-08-23T09:53:05.441087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "ability\n",
      "able\n",
      "abortion\n",
      "absence\n",
      "absolute\n",
      "absolutely\n",
      "abstract\n",
      "absurd\n",
      "abuse\n",
      "abused\n",
      "abusing\n",
      "abusive\n",
      "ac\n",
      "academic\n",
      "academy\n",
      "accept\n",
      "acceptable\n",
      "accepted\n",
      "accepting\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "for i in range(20):\n",
    "    print(vectorizer.get_feature_names_out()[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72317d37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.442744Z",
     "iopub.status.busy": "2025-08-23T09:53:05.442489Z",
     "iopub.status.idle": "2025-08-23T09:53:05.444338Z",
     "shell.execute_reply": "2025-08-23T09:53:05.444002Z"
    }
   },
   "outputs": [],
   "source": [
    "# vocabulary = vectorizer.get_feature_names_out()\n",
    "# first_word = vocabulary[0]\n",
    "\n",
    "# print(f'The word in the first column is: {first_word}')\n",
    "# print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3905603d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.445560Z",
     "iopub.status.busy": "2025-08-23T09:53:05.445308Z",
     "iopub.status.idle": "2025-08-23T09:53:05.474609Z",
     "shell.execute_reply": "2025-08-23T09:53:05.474213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Cross-Validation Sets\n",
    "y = train.copy()\n",
    "drop_columns = ['id','comment_text']\n",
    "y = (y.drop(columns=drop_columns, axis=0)).astype(int)\n",
    "\n",
    "# X_cv_train, X_cv_test, y_cv_train, y_cv_test = train_test_split(train_data_features, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725d1899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.476217Z",
     "iopub.status.busy": "2025-08-23T09:53:05.476094Z",
     "iopub.status.idle": "2025-08-23T09:53:05.478023Z",
     "shell.execute_reply": "2025-08-23T09:53:05.477702Z"
    }
   },
   "outputs": [],
   "source": [
    "#Label Count\n",
    "# label_count = np.sum(y, axis=0)\n",
    "# print(label_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bec24233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.479246Z",
     "iopub.status.busy": "2025-08-23T09:53:05.479053Z",
     "iopub.status.idle": "2025-08-23T09:53:05.480901Z",
     "shell.execute_reply": "2025-08-23T09:53:05.480626Z"
    }
   },
   "outputs": [],
   "source": [
    "# CV Modeling\n",
    "\n",
    "# df_X_cv_train = (pd.DataFrame(X_cv_train)).astype(int)\n",
    "# df_X_cv_test = (pd.DataFrame(X_cv_test)).astype(int)\n",
    "\n",
    "# model = catboost.CatBoostClassifier(loss_function='MultiCrossEntropy',iterations=100, random_seed=0,verbose=False)\n",
    "# model.fit(df_X_cv_train, y_cv_train)\n",
    "# y_pred = model.predict(df_X_cv_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1cc32d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.482131Z",
     "iopub.status.busy": "2025-08-23T09:53:05.481891Z",
     "iopub.status.idle": "2025-08-23T09:53:05.483591Z",
     "shell.execute_reply": "2025-08-23T09:53:05.483330Z"
    }
   },
   "outputs": [],
   "source": [
    "# CV Modeling Scores\n",
    "\n",
    "# array_y_cv_test = y_cv_test.to_numpy()\n",
    "\n",
    "# hamming_score = 1 - hamming_loss(array_y_cv_test, y_pred)\n",
    "# roc = roc_auc_score(y_cv_test,y_pred,average='weighted',multi_class='ovr')\n",
    "# f1 = f1_score(y_cv_test,y_pred,average='weighted')\n",
    "\n",
    "# print('Hammings Score is ' + str(format(hamming_score, '.5f')))\n",
    "# print('ROC Score is ' + str(format(roc, '.5f')))\n",
    "# print('F1 Score is ' + str(format(f1, '.5f')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39916f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.484671Z",
     "iopub.status.busy": "2025-08-23T09:53:05.484574Z",
     "iopub.status.idle": "2025-08-23T09:53:05.486542Z",
     "shell.execute_reply": "2025-08-23T09:53:05.486227Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Feature importance modeling with RandomForestClassifier\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(train_data_features, y)\n",
    "# feature_importances = model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e648a037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.487713Z",
     "iopub.status.busy": "2025-08-23T09:53:05.487441Z",
     "iopub.status.idle": "2025-08-23T09:53:05.489174Z",
     "shell.execute_reply": "2025-08-23T09:53:05.488929Z"
    }
   },
   "outputs": [],
   "source": [
    "# # See all index-word-feature importance\n",
    "# feature_to_index_mapping = vectorizer.vocabulary_ #word is key, index is value\n",
    "# feature_to_word_mapping = list(vectorizer.get_feature_names_out())\n",
    "# dict_Index_to_FeatureImport = {}\n",
    "\n",
    "# for i in range(len(feature_importances)):\n",
    "#     word = feature_to_word_mapping[i]\n",
    "#     index = feature_to_index_mapping[word]\n",
    "#     dict_Index_to_FeatureImport[index] = feature_importances[i]\n",
    "# #     print(f'Index: {index} | Word: {word} | Importance: {feature_importances[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4a11a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.490238Z",
     "iopub.status.busy": "2025-08-23T09:53:05.489999Z",
     "iopub.status.idle": "2025-08-23T09:53:05.491857Z",
     "shell.execute_reply": "2025-08-23T09:53:05.491590Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Select Top 1000 Features\n",
    "# selected_feature_importances = []\n",
    "\n",
    "# all_feature_importances = list(feature_importances)\n",
    "# sorted_all_feature_importances = sorted(all_feature_importances, reverse=True)\n",
    "# for i in range(4001):\n",
    "#     selected_feature_importances.append(sorted_all_feature_importances[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bdf3bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.493131Z",
     "iopub.status.busy": "2025-08-23T09:53:05.492960Z",
     "iopub.status.idle": "2025-08-23T09:53:05.494597Z",
     "shell.execute_reply": "2025-08-23T09:53:05.494328Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Create List to Drop the Other 4000 Columns\n",
    "# removed_features = []\n",
    "# for index, feature_importance in dict_Index_to_FeatureImport.items():\n",
    "#     if feature_importance not in selected_feature_importances:\n",
    "#         removed_features.append(index)\n",
    "# print(len(removed_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94861027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.495736Z",
     "iopub.status.busy": "2025-08-23T09:53:05.495506Z",
     "iopub.status.idle": "2025-08-23T09:53:05.497135Z",
     "shell.execute_reply": "2025-08-23T09:53:05.496865Z"
    }
   },
   "outputs": [],
   "source": [
    "# CV Scores Log\n",
    "    ## Version 1.4\n",
    "# Hammings Score is 0.97980\n",
    "# ROC Score is 0.78235\n",
    "# F1 Score is 0.66949\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "900944e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.498217Z",
     "iopub.status.busy": "2025-08-23T09:53:05.498043Z",
     "iopub.status.idle": "2025-08-23T09:53:05.499750Z",
     "shell.execute_reply": "2025-08-23T09:53:05.499479Z"
    }
   },
   "outputs": [],
   "source": [
    "# See all words\n",
    "# vocab = vectorizer.vocabulary_.keys()\n",
    "# print(vocab)\n",
    "\n",
    "# indexes = vectorizer.vocabulary_.values()\n",
    "# for word, index in zip(vocab,indexes):\n",
    "#     if index in yolo:\n",
    "#         print(word, index)\n",
    "\n",
    "# See counts of all words\n",
    "# dist = np.sum(train_data_features,axis=0)\n",
    "# print(dist)\n",
    "\n",
    "# for word, count in zip(vocab,dist):\n",
    "#     print(word,count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0488a6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:53:05.500815Z",
     "iopub.status.busy": "2025-08-23T09:53:05.500652Z",
     "iopub.status.idle": "2025-08-23T09:55:21.652533Z",
     "shell.execute_reply": "2025-08-23T09:55:21.652136Z"
    }
   },
   "outputs": [],
   "source": [
    "## Modelling\n",
    "df_train_data_features = (pd.DataFrame(train_data_features)).astype(int)\n",
    "df_test_data_features = (pd.DataFrame(test_data_features)).astype(int)\n",
    "\n",
    "# Drop 4000 features\n",
    "# df_train_data_features = df_train_data_features.drop(columns=removed_features)\n",
    "\n",
    "model = catboost.CatBoostClassifier(loss_function='MultiCrossEntropy',iterations=100, random_seed=0,verbose=False)\n",
    "model.fit(df_train_data_features,y)\n",
    "predictions = model.predict_proba(df_test_data_features)\n",
    "# print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2120aacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:55:21.654188Z",
     "iopub.status.busy": "2025-08-23T09:55:21.654027Z",
     "iopub.status.idle": "2025-08-23T09:55:22.477281Z",
     "shell.execute_reply": "2025-08-23T09:55:22.476826Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions)\n",
    "output = pd.DataFrame(data={'id': test['id'],'toxic': predictions[0], 'severe_toxic': predictions[1],\n",
    "                           'obscene': predictions[2], 'threat': predictions[3], 'insult': predictions[4],\n",
    "                           'identity_hate': predictions[5]})\n",
    "output.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
