{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f381a81",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb604de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Version 1.55: Bag of Words\n",
        "#Try keeping all features again to see how\n",
        "#So far in order of decreasing score we have 5000 > 4000 > 3000 > 1000 > 2000 features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039ffd79",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import\n",
        "\n",
        "#Basic\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Data Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "import nltk.data\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import catboost\n",
        "from catboost import CatBoost\n",
        "\n",
        "#Tuning\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
        "\n",
        "#Settings\n",
        "pd.set_option('display.max_rows',None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd79a46f",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Upload Data\n",
        "train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
        "test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a81238",
      "metadata": {},
      "outputs": [],
      "source": [
        "train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a734df5",
      "metadata": {},
      "outputs": [],
      "source": [
        "train['comment_text'][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b928b84d",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Data Preprocess Pipeline Function\n",
        "def preprocess_text(comment):\n",
        "#     print('Step0', comment)\n",
        "    \n",
        "    #1. Remove HTML tags with Beautiful Soup\n",
        "    processed_comment = BeautifulSoup(comment)\n",
        "#     print('Step1', processed_comment)\n",
        "    \n",
        "    #2. Remove punctuation\n",
        "    processed_comment = re.sub('[^a-zA-Z]', ' ', processed_comment.get_text())\n",
        "    processed_comment = re.sub('[\\n]', ' ', processed_comment)\n",
        "#     print('Step2', processed_comment)\n",
        "\n",
        "    #3. Convert all letters to lowercase\n",
        "    processed_comment = processed_comment.lower()\n",
        "#     print('Step3', processed_comment)\n",
        "    \n",
        "    #4. Convert comment into array of word strings\n",
        "    processed_comment = processed_comment.split()\n",
        "#     print('Step4', processed_comment)\n",
        "    \n",
        "    #5. Remove stopwords such as 'a' and 'the'\n",
        "    stops = set(stopwords.words('english'))\n",
        "    processed_comment = [w for w in processed_comment if w not in stops]\n",
        "    \n",
        "    #6. Split comment into a paragraph string\n",
        "    return (' ').join(processed_comment)\n",
        "\n",
        "preprocess_text(train['comment_text'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c84227c",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Preprocess Train and Test Dataset\n",
        "cleaned_train_reviews = []\n",
        "cleaned_test_reviews = []\n",
        "\n",
        "# for i in range(0,100):\n",
        "for i in range(0,len(train)):\n",
        "    if i%10000 == 0:\n",
        "        print('Review %d of %d processed' % (i,len(train)))\n",
        "    cleaned_review = preprocess_text(train['comment_text'][i])\n",
        "    cleaned_train_reviews.append(cleaned_review)\n",
        "    \n",
        "# for i in range(0,100):\n",
        "for i in range(0,len(test)):\n",
        "    if i%10000 == 0:\n",
        "        print('Review %d of %d processed' % (i,len(test)))\n",
        "    cleaned_review = preprocess_text(test['comment_text'][i])\n",
        "    cleaned_test_reviews.append(cleaned_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a8d7d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print('-', cleaned_train_reviews[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8573974",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create Bag of Words Counter\n",
        "vectorizer = CountVectorizer(analyzer = 'word',\n",
        "                            tokenizer = None,\n",
        "                            preprocessor = None,\n",
        "                            stop_words = None,\n",
        "#                             ngram_range = (1,2),\n",
        "                            max_features = 5000)\n",
        "\n",
        "hello = vectorizer.fit_transform(cleaned_train_reviews)\n",
        "train_data_features = (vectorizer.fit_transform(cleaned_train_reviews)).toarray()\n",
        "test_data_features = (vectorizer.fit_transform(cleaned_test_reviews)).toarray()\n",
        "print(hello[0].indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d4d1ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "for i in range(20):\n",
        "    print(vectorizer.get_feature_names_out()[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72317d37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# vocabulary = vectorizer.get_feature_names_out()\n",
        "# first_word = vocabulary[0]\n",
        "\n",
        "# print(f'The word in the first column is: {first_word}')\n",
        "# print(vectorizer.vocabulary_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3905603d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Cross-Validation Sets\n",
        "y = train.copy()\n",
        "drop_columns = ['id','comment_text']\n",
        "y = (y.drop(columns=drop_columns, axis=0)).astype(int)\n",
        "\n",
        "# X_cv_train, X_cv_test, y_cv_train, y_cv_test = train_test_split(train_data_features, y, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725d1899",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Label Count\n",
        "# label_count = np.sum(y, axis=0)\n",
        "# print(label_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec24233",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CV Modeling\n",
        "\n",
        "# df_X_cv_train = (pd.DataFrame(X_cv_train)).astype(int)\n",
        "# df_X_cv_test = (pd.DataFrame(X_cv_test)).astype(int)\n",
        "\n",
        "# model = catboost.CatBoostClassifier(loss_function='MultiCrossEntropy',iterations=100, random_seed=0,verbose=False)\n",
        "# model.fit(df_X_cv_train, y_cv_train)\n",
        "# y_pred = model.predict(df_X_cv_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cc32d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CV Modeling Scores\n",
        "\n",
        "# array_y_cv_test = y_cv_test.to_numpy()\n",
        "\n",
        "# hamming_score = 1 - hamming_loss(array_y_cv_test, y_pred)\n",
        "# roc = roc_auc_score(y_cv_test,y_pred,average='weighted',multi_class='ovr')\n",
        "# f1 = f1_score(y_cv_test,y_pred,average='weighted')\n",
        "\n",
        "# print('Hammings Score is ' + str(format(hamming_score, '.5f')))\n",
        "# print('ROC Score is ' + str(format(roc, '.5f')))\n",
        "# print('F1 Score is ' + str(format(f1, '.5f')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39916f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Feature importance modeling with RandomForestClassifier\n",
        "# model = RandomForestClassifier()\n",
        "# model.fit(train_data_features, y)\n",
        "# feature_importances = model.feature_importances_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e648a037",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # See all index-word-feature importance\n",
        "# feature_to_index_mapping = vectorizer.vocabulary_ #word is key, index is value\n",
        "# feature_to_word_mapping = list(vectorizer.get_feature_names_out())\n",
        "# dict_Index_to_FeatureImport = {}\n",
        "\n",
        "# for i in range(len(feature_importances)):\n",
        "#     word = feature_to_word_mapping[i]\n",
        "#     index = feature_to_index_mapping[word]\n",
        "#     dict_Index_to_FeatureImport[index] = feature_importances[i]\n",
        "# #     print(f'Index: {index} | Word: {word} | Importance: {feature_importances[i]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a11a10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Select Top 1000 Features\n",
        "# selected_feature_importances = []\n",
        "\n",
        "# all_feature_importances = list(feature_importances)\n",
        "# sorted_all_feature_importances = sorted(all_feature_importances, reverse=True)\n",
        "# for i in range(4001):\n",
        "#     selected_feature_importances.append(sorted_all_feature_importances[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdf3bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Create List to Drop the Other 4000 Columns\n",
        "# removed_features = []\n",
        "# for index, feature_importance in dict_Index_to_FeatureImport.items():\n",
        "#     if feature_importance not in selected_feature_importances:\n",
        "#         removed_features.append(index)\n",
        "# print(len(removed_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94861027",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CV Scores Log\n",
        "    ## Version 1.4\n",
        "# Hammings Score is 0.97980\n",
        "# ROC Score is 0.78235\n",
        "# F1 Score is 0.66949\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900944e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# See all words\n",
        "# vocab = vectorizer.vocabulary_.keys()\n",
        "# print(vocab)\n",
        "\n",
        "# indexes = vectorizer.vocabulary_.values()\n",
        "# for word, index in zip(vocab,indexes):\n",
        "#     if index in yolo:\n",
        "#         print(word, index)\n",
        "\n",
        "# See counts of all words\n",
        "# dist = np.sum(train_data_features,axis=0)\n",
        "# print(dist)\n",
        "\n",
        "# for word, count in zip(vocab,dist):\n",
        "#     print(word,count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0488a6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Modelling\n",
        "df_train_data_features = (pd.DataFrame(train_data_features)).astype(int)\n",
        "df_test_data_features = (pd.DataFrame(test_data_features)).astype(int)\n",
        "\n",
        "# Drop 4000 features\n",
        "# df_train_data_features = df_train_data_features.drop(columns=removed_features)\n",
        "\n",
        "model = catboost.CatBoostClassifier(loss_function='MultiCrossEntropy',iterations=100, random_seed=0,verbose=False)\n",
        "model.fit(df_train_data_features,y)\n",
        "predictions = model.predict_proba(df_test_data_features)\n",
        "# print(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2120aacf",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame(predictions)\n",
        "output = pd.DataFrame(data={'id': test['id'],'toxic': predictions[0], 'severe_toxic': predictions[1],\n",
        "                           'obscene': predictions[2], 'threat': predictions[3], 'insult': predictions[4],\n",
        "                           'identity_hate': predictions[5]})\n",
        "output.to_csv('submission.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}