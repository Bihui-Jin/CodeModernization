{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fefc7a10",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da3f674a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os, cv2, random, time, shutil, csv\n",
        "\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
        "\n",
        "\n",
        "\n",
        "# !pip install termcolor\n",
        "import colorama\n",
        "from colorama import Fore, Style  # makes strings colored\n",
        "from termcolor import colored\n",
        "from termcolor import cprint\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tqdm import tqdm\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import (\n",
        "    Activation,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    Dense,\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    BatchNormalization\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_validate \n",
        "from sklearn.metrics import RocCurveDisplay,accuracy_score, f1_score, recall_score,\\\n",
        "                            precision_score, make_scorer,\\\n",
        "                            classification_report,confusion_matrix,\\\n",
        "                            ConfusionMatrixDisplay, average_precision_score,\\\n",
        "                            roc_curve, roc_auc_score, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from scikitplot.metrics import plot_roc, precision_recall_curve,average_precision_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Uncomment the following lines if you want to suppress warnings:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.warn(\"this will not show\")\n",
        "\n",
        "# Set it to None to display all rows in the dataframe:\n",
        "# pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Set it to None to display all columns in the dataframe:\n",
        "pd.set_option(\"display.max_columns\", None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce2ec25",
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_csv = pd.read_csv(\"../input/dog-breed-identification/labels.csv\")\n",
        "print(labels_csv.describe())\n",
        "print(labels_csv.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c578f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# How many images are there of each breed?\n",
        "labels_csv[\"breed\"].value_counts().plot.bar(figsize=(20, 10));\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c2219ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Image\n",
        "Image(\"/kaggle/input/dog-breed-identification/train/00693b8bc2470375cc744a6391d397ec.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86bec1d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our training file path for ease of use\n",
        "train_path = \"../input/dog-breed-identification/train/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b3df915",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pathnames from image ID's\n",
        "filenames = [train_path + fname + \".jpg\" for fname in labels_csv[\"id\"]]\n",
        "\n",
        "# Check the first 10 filenames\n",
        "filenames[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3061d5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check whether number of filenames matches number of actual image files\n",
        "import os\n",
        "if len(os.listdir(train_path)) == len(filenames):\n",
        "  print(\"Filenames match actual amount of files!\")\n",
        "else:\n",
        "  print(\"Filenames do not match actual amount of files, check the target directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c1a957",
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "\n",
        "random_images = random.sample(filenames, 9)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    \n",
        "    img_path = random_images[i]\n",
        "    img = Image.open(img_path)\n",
        "    label = labels_csv[labels_csv[\"id\"] == os.path.splitext(os.path.basename(img_path))[0]][\"breed\"].values[0]\n",
        "    \n",
        "    img = img.resize((100, 100))\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eabbdab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "labels = labels_csv[\"breed\"].to_numpy() # convert labels column to NumPy array\n",
        "labels[:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad757e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# See if number of labels matches the number of filenames\n",
        "if len(labels) == len(filenames):\n",
        "  print(\"Number of labels matches number of filenames!\")\n",
        "else:\n",
        "  print(\"Number of labels does not match number of filenames, check data directories.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fca44ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the unique label values\n",
        "unique_breeds = np.unique(labels)\n",
        "len(unique_breeds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa962cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn every label into a boolean array\n",
        "boolean_labels = [label == np.array(unique_breeds) for label in labels]\n",
        "boolean_labels[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e744d3a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Turning a boolean array into integers\n",
        "print(labels[1]) # original label\n",
        "print(np.where(unique_breeds == labels[1])[0][0]) # index where label occurs\n",
        "print(boolean_labels[1].argmax()) # index where label occurs in boolean array\n",
        "print(boolean_labels[0].astype(int)) # there will be a 1 where the sample label occurs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b731013a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup X & y variables\n",
        "X = filenames\n",
        "y = boolean_labels\n",
        "\n",
        "print(f\"Number of training images: {len(X)}\")\n",
        "print(f\"Number of labels: {len(y)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7cb7ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Setup X & y variables\n",
        "X = filenames\n",
        "y = [np.where(label)[0][0] for label in boolean_labels]\n",
        "\n",
        "# Create a DataFrame\n",
        "train_df = pd.DataFrame({'image': X, 'label': y})\n",
        "\n",
        "# Display the DataFrame\n",
        "train_df.sample(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d42db0",
      "metadata": {},
      "outputs": [],
      "source": [
        "list(train_df.iloc[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecee49d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "refactor_size = 64\n",
        "resized_image_list = []\n",
        "all_paths = []\n",
        "\n",
        "# Loop through the DataFrame to load and process images\n",
        "for i in range(len(train_df)):\n",
        "    image_path = train_df.iloc[i]['image']\n",
        "    label = train_df.iloc[i]['label']\n",
        "\n",
        "    # Load and process the image\n",
        "    img = tf.keras.utils.load_img(image_path, target_size=(refactor_size, refactor_size))\n",
        "    img_vals = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    imgarr = tf.keras.utils.img_to_array(img_vals)\n",
        "\n",
        "    # Append the processed image and label to the lists\n",
        "    resized_image_list.append(imgarr)\n",
        "    all_paths.append(image_path)\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "resized_image_list = np.asarray(resized_image_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea5d43e",
      "metadata": {},
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"Available GPUs:\", gpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c0ff7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "nrow = 5\n",
        "ncol = 4  \n",
        "fig1 = plt.figure(figsize=(20, 15))\n",
        "fig1.suptitle('After Resizing', size=32)\n",
        "\n",
        "for i in range(min(20, len(resized_image_list))):\n",
        "    plt.subplot(nrow, ncol, i + 1)\n",
        "    plt.imshow(resized_image_list[i])\n",
        "    plt.title('class = {x}, Dog is {y}'.format(x=train_df[\"label\"].iloc[i], y=labels[i]))\n",
        "    plt.axis('Off')\n",
        "    plt.grid(False)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de5925d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"), \n",
        "    layers.RandomRotation(0.3),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.5)\n",
        "], name='data_augmentation')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a49e6ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_images = data_augmentation(resized_image_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87d39b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "nrow = 4\n",
        "ncol = 5\n",
        "\n",
        "augmented_indices = range(min(20, len(resized_image_list)))\n",
        "\n",
        "fig2 = plt.figure(figsize=(20, 15))\n",
        "fig2.suptitle('After Augmentation', size=32)\n",
        "\n",
        "for i, idx in enumerate(augmented_indices):\n",
        "    augmented_image = data_augmentation(tf.expand_dims(resized_image_list[idx], 0), training=True)\n",
        "    plt.subplot(nrow, ncol, i + 1)\n",
        "    plt.imshow(augmented_image[0].numpy())\n",
        "    plt.title('class = {x}, Dog is {y}'.format(x=train_df[\"label\"].iloc[idx], y=labels[idx]))\n",
        "    plt.axis('Off')\n",
        "    plt.grid(False)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd7e06a",
      "metadata": {},
      "outputs": [],
      "source": [
        "class_values = train_df[\"label\"]\n",
        "filtered_values = class_values[class_values < 0]\n",
        "\n",
        "if not filtered_values.empty:\n",
        "    print(\"There are values in the series less than 0.\")\n",
        "else:\n",
        "    print(\"There are no values in the series less than 0.\")\n",
        "class_values.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb17dcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming you have NumPy arrays for augmented_images and selected_labels\n",
        "# Convert NumPy arrays to TensorFlow tensors\n",
        "augmented_images_tf = tf.convert_to_tensor(augmented_images)\n",
        "selected_labels_tf = tf.convert_to_tensor(train_df['label'])\n",
        "\n",
        "# Convert TensorFlow tensors back to NumPy arrays\n",
        "augmented_images_np = augmented_images_tf.numpy()\n",
        "selected_labels_np = selected_labels_tf.numpy()\n",
        "\n",
        "# Split them into training and validation using NUM_IMAGES \n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    augmented_images_np, \n",
        "    selected_labels_np,\n",
        "    test_size=0.3,\n",
        "    stratify = selected_labels_np,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X_train, \n",
        "#     y_train,\n",
        "#     test_size=0.1,  # You can adjust the validation split as needed\n",
        "#     stratify = y_train,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "print(\"Training Set Length:\", len(X_train))\n",
        "print(\"Test Set Length:\", len(X_test))                                  \n",
        "# print(\"Validation Set Length:\", len(X_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e48402db",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "\n",
        "\n",
        "class_weights_dict = {class_num: weight for class_num, weight in zip(np.unique(y_train_encoded), class_weights)}\n",
        "\n",
        "print(\"Class Weights Dictionary:\")\n",
        "print(class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cd8279",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3029b49",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=X_train.shape[1:], padding = 'same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation=\"relu\", padding = 'same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation=\"relu\", padding = 'same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(140, activation=\"relu\"))\n",
        "\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "\n",
        "model.add(Dense(120, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8828c09e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d00259",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare call backs\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_accuracy',mode = 'max', verbose = 1, patience=15, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535a16b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          epochs=400, #validation_split = 0.1,\n",
        "          batch_size = 64, callbacks=my_callback, class_weight = class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4829a169",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(model.history.history).plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631a4ba7",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, recall = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"loss: \", loss)\n",
        "print(\"recall: \", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c55422",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(pred_prob, axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3032d02",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "n_classes = 120\n",
        "\n",
        "# Sınıfları ikili formata dönüştür\n",
        "y_test_binary = label_binarize(y_test, classes=range(n_classes))\n",
        "y_pred_binary = label_binarize(y_pred, classes=range(n_classes))\n",
        "\n",
        "# Micro-averaging için precision_score kullanımı\n",
        "model_precision = precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "# Diğer performans metrikleri\n",
        "model_recall = recall_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "model_AP = average_precision_score(label_binarize(y_test, classes=range(n_classes)),\n",
        "                                          label_binarize(y_pred, classes=range(n_classes)),\n",
        "                                          average='weighted')\n",
        "\n",
        "\n",
        "print(f'Weighted-Averaged Precision: {model_precision:.2f}')\n",
        "print(f'Weighted-Averaged Recall: {model_recall:.2f}')\n",
        "print(f'Weighted-Averaged AP: {model_AP:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c50a8b4",
      "metadata": {},
      "outputs": [],
      "source": [
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e994ed9",
      "metadata": {},
      "outputs": [],
      "source": [
        "del X,y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0011c1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_num_files(path):\n",
        "    '''\n",
        "    Counts the number of files in a folder.\n",
        "    '''\n",
        "    if not os.path.exists(path):\n",
        "        return 0\n",
        "    return sum([len(files) for r, d, files in os.walk(path)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e945931",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os, cv2, random, time, shutil, csv\n",
        "#Data Paths\n",
        "train_dir = '/kaggle/input/dog-breed-identification/train'\n",
        "test_dir = '/kaggle/input/dog-breed-identification/test'\n",
        "#Count/Print train and test samples.\n",
        "data_size = get_num_files(train_dir)\n",
        "test_size = get_num_files(test_dir)\n",
        "print('Data samples size: ', data_size)\n",
        "print('Test samples size: ', test_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af38ca7",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Read train labels.\n",
        "labels_dataframe = pd.read_csv('/kaggle/input/dog-breed-identification/labels.csv')\n",
        "#Read sample_submission file to be modified by pridected labels.\n",
        "sample_df = pd.read_csv('/kaggle/input/dog-breed-identification/sample_submission.csv')\n",
        "#Incpect labels_dataframe.\n",
        "labels_dataframe.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb83132",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2a1642",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create list of alphabetically sorted labels.\n",
        "dog_breeds = sorted(list(set(labels_dataframe['breed'])))\n",
        "n_classes = len(dog_breeds)\n",
        "print(n_classes)\n",
        "dog_breeds[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1a4743",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Map each label string to an integer label.\n",
        "class_to_num = dict(zip(dog_breeds, range(n_classes)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5c8554",
      "metadata": {},
      "outputs": [],
      "source": [
        "def images_to_array(data_dir, labels_dataframe, img_size = (224,224,3)):\n",
        "    '''\n",
        "    1- Read image samples from certain directory.\n",
        "    2- Risize it, then stack them into one big numpy array.\n",
        "    3- Read sample's label form the labels dataframe.\n",
        "    4- One hot encode labels array.\n",
        "    5- Shuffle Data and label arrays.\n",
        "    '''\n",
        "    images_names = labels_dataframe['id']\n",
        "    images_labels = labels_dataframe['breed']\n",
        "    data_size = len(images_names)\n",
        "    #initailize output arrays.\n",
        "    X = np.zeros([data_size, img_size[0], img_size[1], img_size[2]], dtype=np.uint8)\n",
        "    y = np.zeros([data_size,1], dtype=np.uint8)\n",
        "    #read data and lables.\n",
        "    for i in tqdm(range(data_size)):\n",
        "        image_name = images_names[i]\n",
        "        img_dir = os.path.join(data_dir, image_name+'.jpg')\n",
        "        img_pixels = load_img(img_dir, target_size=img_size)\n",
        "        X[i] = img_pixels\n",
        "        \n",
        "        image_breed = images_labels[i]\n",
        "        y[i] = class_to_num[image_breed]\n",
        "    \n",
        "    #One hot encoder\n",
        "    y = to_categorical(y)\n",
        "    #shuffle    \n",
        "    ind = np.random.permutation(data_size)\n",
        "    X = X[ind]\n",
        "    y = y[ind]\n",
        "    print('Ouptut Data Size: ', X.shape)\n",
        "    print('Ouptut Label Size: ', y.shape)\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866f1278",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.image import load_img\n",
        "# from tqdm import tqdm\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "img_size = (300,300, 3)\n",
        "X, y = images_to_array(train_dir, labels_dataframe, img_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31279ee6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_features(model_name, data_preprocessor, input_size, data):\n",
        "    '''\n",
        "    1- Create a feature extractor to extract features from the data.\n",
        "    2- Returns the extracted features and the feature extractor.\n",
        "    '''\n",
        "    #Prepare pipeline.\n",
        "    input_layer = Input(input_size)\n",
        "    preprocessor = Lambda(data_preprocessor)(input_layer)\n",
        "    base_model = model_name(weights='imagenet', include_top=False,\n",
        "                            input_shape=input_size)(preprocessor)\n",
        "    avg = GlobalAveragePooling2D()(base_model)\n",
        "    feature_extractor = Model(inputs = input_layer, outputs = avg)\n",
        "    #Extract feature.\n",
        "    feature_maps = feature_extractor.predict(data, batch_size=64, verbose=1)\n",
        "    print('Feature maps shape: ', feature_maps.shape)\n",
        "    return feature_maps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f821e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features using InceptionV3 as extractor.\n",
        "from keras.models import Model\n",
        "from keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "inception_preprocessor = preprocess_input\n",
        "inception_features = get_features(InceptionV3,\n",
        "                                  inception_preprocessor,\n",
        "                                  img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0634b846",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features using Xception as extractor.\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "xception_preprocessor = preprocess_input\n",
        "xception_features = get_features(Xception,\n",
        "                                 xception_preprocessor,\n",
        "                                 img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b021be3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features using NASNetLarge as extractor.\n",
        "from keras.applications.nasnet import NASNetLarge, preprocess_input\n",
        "nasnet_preprocessor = preprocess_input\n",
        "nasnet_features = get_features(NASNetLarge,\n",
        "                               nasnet_preprocessor,\n",
        "                               img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0fdbbd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features using InceptionResNetV2 as extractor.\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "inc_resnet_preprocessor = preprocess_input\n",
        "inc_resnet_features = get_features(InceptionResNetV2,\n",
        "                                   inc_resnet_preprocessor,\n",
        "                                   img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6495d59c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "vgg16_preprocessor = preprocess_input\n",
        "vgg16_features = get_features(VGG16,\n",
        "                                   vgg16_preprocessor,\n",
        "                                   img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf2a021",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "\n",
        "resnet50_preprocessor = preprocess_input\n",
        "resnet50_features = get_features(ResNet50,\n",
        "                                   resnet50_preprocessor,\n",
        "                                   img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23cb81fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
        "mobilenet_v2_preprocessor = preprocess_input\n",
        "mobilenet_v2_features = get_features(MobileNetV2,\n",
        "                                   mobilenet_v2_preprocessor,\n",
        "                                   img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdff465",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications.densenet import DenseNet121, preprocess_input\n",
        "densenet_preprocessor = preprocess_input\n",
        "densenet_features = get_features(DenseNet121,\n",
        "                                   densenet_preprocessor,\n",
        "                                   img_size, X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9954bb6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#It's a good habit to free up some RAM memory.\n",
        "#X variable won't be needed anymore, so let's get rid of it.\n",
        "del X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9525441d",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_features = np.concatenate([inception_features,\n",
        "                                 xception_features,\n",
        "                                 nasnet_features,\n",
        "                                 inc_resnet_features,], axis=-1)\n",
        "print('Final feature maps shape', final_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9e7424",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_value_counts = pd.Series(y.argmax(axis=1)).value_counts(normalize=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
        "\n",
        "# y_train_indices = np.argmax(y_train, axis=1)\n",
        "# y_val_indices = np.argmax(y_val, axis=1)\n",
        "# y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# train_value_counts = pd.Series(y_train_indices).value_counts(normalize=True)\n",
        "# val_value_counts = pd.Series(y_val_indices).value_counts(normalize=True)\n",
        "# test_value_counts = pd.Series(y_test_indices).value_counts(normalize=True)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "# bar_width = 0.2\n",
        "# index = np.arange(len(original_value_counts))\n",
        "\n",
        "# bar1 = ax.barh(index, original_value_counts, bar_width, label='Main Data')\n",
        "# bar2 = ax.barh(index, train_value_counts, bar_width, label='Train Set', left=original_value_counts)\n",
        "# bar3 = ax.barh(index, val_value_counts, bar_width, label='Validation Set', left=original_value_counts + train_value_counts)\n",
        "# bar4 = ax.barh(index, test_value_counts, bar_width, label='Test Set', left=original_value_counts + train_value_counts + val_value_counts)\n",
        "\n",
        "# ax.set_xlabel('Percentages')\n",
        "# ax.set_title('Class Distribution')\n",
        "# ax.set_yticks(index)\n",
        "# ax.set_yticklabels(original_value_counts.index)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8803da72",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(np.argmax(y_train, axis=1))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "\n",
        "class_weights_dict = {class_num: weight for class_num, weight in zip(np.unique(y_train_encoded), class_weights)}\n",
        "\n",
        "print(\"Class Weights Dictionary:\")\n",
        "print(class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df065b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 80\n",
        "epochs = 400\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed905a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare call backs\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_recall', verbose=1,mode = 'max', patience=15, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceef34f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare DNN model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model_1 = keras.models.Sequential([\n",
        "    InputLayer(X_train.shape[1:]),\n",
        "    Dropout(0.7),\n",
        "    Dense(n_classes, activation='softmax', #kernel_regularizer=regularizers.l2(0.01)\n",
        "         )])\n",
        "\n",
        "optimizer = Adam(learning_rate = 0.0001)\n",
        "model_1.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['Recall'])\n",
        "\n",
        "#Train simple DNN on extracted features.\n",
        "history_1 = model_1.fit( #final_features, y,\n",
        "            X_train, y_train,\n",
        "            batch_size= batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_test, y_test),\n",
        "#             validation_split = 0.1,\n",
        "            callbacks=my_callback,\n",
        "            class_weight = class_weights_dict\n",
        "                       )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016633a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, recall = model_1.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"loss: \", loss)\n",
        "print(\"recall: \", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86603119",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc6c4ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_data = history_1.history\n",
        "\n",
        "loss_df_1 = pd.DataFrame(history_data)\n",
        "loss_df_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdcba6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_df_1.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25ca3d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_1.evaluate(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3ea95a",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_1.evaluate(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989f87cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming y_train and y_test are one-hot encoded, convert them to indices\n",
        "y_train_indices = np.argmax(y_train, axis=1)\n",
        "y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get predicted labels for both training and test datasets\n",
        "train_pred_prob = model_1.predict(X_train)\n",
        "test_pred_prob = model_1.predict(X_test)\n",
        "\n",
        "y_train_pred = np.argmax(train_pred_prob, axis=1)\n",
        "y_test_pred = np.argmax(test_pred_prob, axis=1)\n",
        "\n",
        "# Calculate confusion matrix and classification report for training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(confusion_matrix(y_train_indices, y_train_pred))\n",
        "print(classification_report(y_train_indices, y_train_pred))\n",
        "\n",
        "# Calculate confusion matrix and classification report for test dataset\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(confusion_matrix(y_test_indices, y_test_pred))\n",
        "print(classification_report(y_test_indices, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba0adfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming y_test is in multilabel-indicator format\n",
        "y_pred_proba = model_1.predict(X_test)\n",
        "\n",
        "# Calculate average precision and area under the ROC curve for each class\n",
        "average_precisions = []\n",
        "roc_aucs = []\n",
        "\n",
        "for i in range(n_classes):  # n_classes is the number of classes in your problem\n",
        "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "    average_precisions.append(average_precision_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "    roc_aucs.append(roc_auc_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "\n",
        "# Plot precision-recall curves\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(recall, precision, lw=2, label=f'Class {i + 1} (AP = {average_precisions[i]:.2f}, AUC = {roc_aucs[i]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Each Class')\n",
        "\n",
        "# Create a DataFrame for the legend information\n",
        "legend_data = {'Class': [f'Class {i + 1}' for i in range(n_classes)],\n",
        "               'Average Precision': average_precisions,\n",
        "               'AUC': roc_aucs}\n",
        "legend_df = pd.DataFrame(legend_data)\n",
        "plt.show()\n",
        "legend_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ceeb97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
        "# from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Assuming y_test_indices and y_test_pred are obtained as mentioned in your code\n",
        "# Convert to binary format\n",
        "y_test_binary = label_binarize(y_test_indices, classes=range(n_classes))\n",
        "y_pred_binary = label_binarize(y_test_pred, classes=range(n_classes))\n",
        "\n",
        "# Calculate precision, recall, and AP\n",
        "model1_precision = precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model1_recall = recall_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model1_AP = average_precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "print(f'Weighted-Averaged Precision: {model1_precision:.2f}')\n",
        "print(f'Weighted-Averaged Recall: {model1_recall:.2f}')\n",
        "print(f'Weighted-Averaged AP: {model1_AP:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7949302",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_features_1 = np.concatenate([vgg16_features,\n",
        "                                 resnet50_features,\n",
        "                                 mobilenet_v2_features,\n",
        "                                 inc_resnet_features,], axis=-1)\n",
        "print('Final feature maps shape', final_features_1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da10faf",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_value_counts = pd.Series(y.argmax(axis=1)).value_counts(normalize=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features_1, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
        "\n",
        "# y_train_indices = np.argmax(y_train, axis=1)\n",
        "# y_val_indices = np.argmax(y_val, axis=1)\n",
        "# y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# train_value_counts = pd.Series(y_train_indices).value_counts(normalize=True)\n",
        "# val_value_counts = pd.Series(y_val_indices).value_counts(normalize=True)\n",
        "# test_value_counts = pd.Series(y_test_indices).value_counts(normalize=True)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "# bar_width = 0.2\n",
        "# index = np.arange(len(original_value_counts))\n",
        "\n",
        "# bar1 = ax.barh(index, original_value_counts, bar_width, label='Main Data')\n",
        "# bar2 = ax.barh(index, train_value_counts, bar_width, label='Train Set', left=original_value_counts)\n",
        "# bar3 = ax.barh(index, val_value_counts, bar_width, label='Validation Set', left=original_value_counts + train_value_counts)\n",
        "# bar4 = ax.barh(index, test_value_counts, bar_width, label='Test Set', left=original_value_counts + train_value_counts + val_value_counts)\n",
        "\n",
        "# ax.set_xlabel('Percentages')\n",
        "# ax.set_title('Class Distribution')\n",
        "# ax.set_yticks(index)\n",
        "# ax.set_yticklabels(original_value_counts.index)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acfc6a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(np.argmax(y_train, axis=1))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "\n",
        "class_weights_dict = {class_num: weight for class_num, weight in zip(np.unique(y_train_encoded), class_weights)}\n",
        "\n",
        "print(\"Class Weights Dictionary:\")\n",
        "print(class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37357d86",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare call backs\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_loss', verbose=1,mode = 'min', patience=15, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bba4c31",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare DNN model\n",
        "model_2 = keras.models.Sequential([\n",
        "    InputLayer(X_train.shape[1:]),\n",
        "    Dropout(0.5),\n",
        "    Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['Recall'])\n",
        "\n",
        "#Train simple DNN on extracted features.\n",
        "history_2 = model_2.fit(X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=my_callback,\n",
        "            class_weight = class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e9a4aa5",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, recall = model_2.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"loss: \", loss)\n",
        "print(\"recall: \", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "103320ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0853413d",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_data = history_2.history\n",
        "\n",
        "loss_df_2 = pd.DataFrame(history_data)\n",
        "loss_df_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f52089f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_df_2.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442e3539",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming y_train and y_test are one-hot encoded, convert them to indices\n",
        "y_train_indices = np.argmax(y_train, axis=1)\n",
        "y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get predicted labels for both training and test datasets\n",
        "train_pred_prob = model_2.predict(X_train)\n",
        "test_pred_prob = model_2.predict(X_test)\n",
        "\n",
        "y_train_pred = np.argmax(train_pred_prob, axis=1)\n",
        "y_test_pred = np.argmax(test_pred_prob, axis=1)\n",
        "\n",
        "# Calculate confusion matrix and classification report for training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(confusion_matrix(y_train_indices, y_train_pred))\n",
        "print(classification_report(y_train_indices, y_train_pred))\n",
        "\n",
        "# Calculate confusion matrix and classification report for test dataset\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(confusion_matrix(y_test_indices, y_test_pred))\n",
        "print(classification_report(y_test_indices, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896b8c3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming y_test is in multilabel-indicator format\n",
        "y_pred_proba = model_2.predict(X_test)\n",
        "\n",
        "# Calculate average precision and area under the ROC curve for each class\n",
        "average_precisions = []\n",
        "roc_aucs = []\n",
        "\n",
        "for i in range(n_classes):  # n_classes is the number of classes in your problem\n",
        "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "    average_precisions.append(average_precision_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "    roc_aucs.append(roc_auc_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "\n",
        "# Plot precision-recall curves\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(recall, precision, lw=2, label=f'Class {i + 1} (AP = {average_precisions[i]:.2f}, AUC = {roc_aucs[i]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Each Class')\n",
        "\n",
        "# Create a DataFrame for the legend information\n",
        "legend_data = {'Class': [f'Class {i + 1}' for i in range(n_classes)],\n",
        "               'Average Precision': average_precisions,\n",
        "               'AUC': roc_aucs}\n",
        "legend_df = pd.DataFrame(legend_data)\n",
        "plt.show()\n",
        "legend_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3da7816",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming y_test_indices and y_test_pred are obtained as mentioned in your code\n",
        "# Convert to binary format\n",
        "y_test_binary = label_binarize(y_test_indices, classes=range(n_classes))\n",
        "y_pred_binary = label_binarize(y_test_pred, classes=range(n_classes))\n",
        "\n",
        "# Calculate precision, recall, and AP\n",
        "model2_precision = precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model2_recall = recall_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model2_AP = average_precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "print(f'Weighted-Averaged Precision: {model2_precision:.2f}')\n",
        "print(f'Weighted-Averaged Recall: {model2_recall:.2f}')\n",
        "print(f'Weighted-Averaged AP: {model2_AP:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df17d030",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_features_3 = np.concatenate([vgg16_features,\n",
        "                                 resnet50_features,\n",
        "                                 mobilenet_v2_features,\n",
        "                                 densenet_features], axis=-1)\n",
        "print('Final feature maps shape', final_features_3.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7cf199f",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_value_counts = pd.Series(y.argmax(axis=1)).value_counts(normalize=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features_3, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
        "\n",
        "# y_train_indices = np.argmax(y_train, axis=1)\n",
        "# y_val_indices = np.argmax(y_val, axis=1)\n",
        "# y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# train_value_counts = pd.Series(y_train_indices).value_counts(normalize=True)\n",
        "# val_value_counts = pd.Series(y_val_indices).value_counts(normalize=True)\n",
        "# test_value_counts = pd.Series(y_test_indices).value_counts(normalize=True)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "# bar_width = 0.2\n",
        "# index = np.arange(len(original_value_counts))\n",
        "\n",
        "# bar1 = ax.barh(index, original_value_counts, bar_width, label='Main Data')\n",
        "# bar2 = ax.barh(index, train_value_counts, bar_width, label='Train Set', left=original_value_counts)\n",
        "# bar3 = ax.barh(index, val_value_counts, bar_width, label='Validation Set', left=original_value_counts + train_value_counts)\n",
        "# bar4 = ax.barh(index, test_value_counts, bar_width, label='Test Set', left=original_value_counts + train_value_counts + val_value_counts)\n",
        "\n",
        "# ax.set_xlabel('Percentages')\n",
        "# ax.set_title('Class Distribution')\n",
        "# ax.set_yticks(index)\n",
        "# ax.set_yticklabels(original_value_counts.index)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b889a9e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(np.argmax(y_train, axis=1))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "\n",
        "class_weights_dict = {class_num: weight for class_num, weight in zip(np.unique(y_train_encoded), class_weights)}\n",
        "\n",
        "print(\"Class Weights Dictionary:\")\n",
        "print(class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbd2575",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare call backs\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_loss', verbose=1,mode = 'min', patience=15, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01739c9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare DNN model\n",
        "model_3 = keras.models.Sequential([\n",
        "    InputLayer(X_train.shape[1:]),\n",
        "    Dropout(0.5),\n",
        "    Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_3.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['Recall'])\n",
        "\n",
        "#Train simple DNN on extracted features.\n",
        "history_3 = model_3.fit(X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=my_callback,\n",
        "            class_weight = class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec9edc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, accuracy = model_3.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"loss: \", loss)\n",
        "print(\"accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3eb8edd",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2c408cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_data = history_3.history\n",
        "\n",
        "loss_df_3 = pd.DataFrame(history_data)\n",
        "loss_df_3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6025d04a",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_df_3.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a0ba5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming y_train and y_test are one-hot encoded, convert them to indices\n",
        "y_train_indices = np.argmax(y_train, axis=1)\n",
        "y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get predicted labels for both training and test datasets\n",
        "train_pred_prob = model_3.predict(X_train)\n",
        "test_pred_prob = model_3.predict(X_test)\n",
        "\n",
        "y_train_pred = np.argmax(train_pred_prob, axis=1)\n",
        "y_test_pred = np.argmax(test_pred_prob, axis=1)\n",
        "\n",
        "# Calculate confusion matrix and classification report for training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(confusion_matrix(y_train_indices, y_train_pred))\n",
        "print(classification_report(y_train_indices, y_train_pred))\n",
        "\n",
        "# Calculate confusion matrix and classification report for test dataset\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(confusion_matrix(y_test_indices, y_test_pred))\n",
        "print(classification_report(y_test_indices, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065fda14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming y_test is in multilabel-indicator format\n",
        "y_pred_proba = model_3.predict(X_test)\n",
        "\n",
        "# Calculate average precision and area under the ROC curve for each class\n",
        "average_precisions = []\n",
        "roc_aucs = []\n",
        "\n",
        "for i in range(n_classes):  # n_classes is the number of classes in your problem\n",
        "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "    average_precisions.append(average_precision_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "    roc_aucs.append(roc_auc_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "\n",
        "# Plot precision-recall curves\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(recall, precision, lw=2, label=f'Class {i + 1} (AP = {average_precisions[i]:.2f}, AUC = {roc_aucs[i]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Each Class')\n",
        "\n",
        "# Create a DataFrame for the legend information\n",
        "legend_data = {'Class': [f'Class {i + 1}' for i in range(n_classes)],\n",
        "               'Average Precision': average_precisions,\n",
        "               'AUC': roc_aucs}\n",
        "legend_df = pd.DataFrame(legend_data)\n",
        "plt.show()\n",
        "legend_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ac65ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
        "# from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Assuming y_test_indices and y_test_pred are obtained as mentioned in your code\n",
        "# Convert to binary format\n",
        "y_test_binary = label_binarize(y_test_indices, classes=range(n_classes))\n",
        "y_pred_binary = label_binarize(y_test_pred, classes=range(n_classes))\n",
        "\n",
        "# Calculate precision, recall, and AP\n",
        "model3_precision = precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model3_recall = recall_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model3_AP = average_precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "print(f'Weighted-Averaged Precision: {model3_precision:.2f}')\n",
        "print(f'Weighted-Averaged Recall: {model3_recall:.2f}')\n",
        "print(f'Weighted-Averaged AP: {model3_AP:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e172f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_features_4 = np.concatenate([inception_features,\n",
        "                                 resnet50_features,\n",
        "                                 nasnet_features,\n",
        "                                 densenet_features], axis=-1)\n",
        "print('Final feature maps shape', final_features_4.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1dc6ac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_value_counts = pd.Series(y.argmax(axis=1)).value_counts(normalize=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features_4, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
        "\n",
        "# y_train_indices = np.argmax(y_train, axis=1)\n",
        "# y_val_indices = np.argmax(y_val, axis=1)\n",
        "# y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# train_value_counts = pd.Series(y_train_indices).value_counts(normalize=True)\n",
        "# val_value_counts = pd.Series(y_val_indices).value_counts(normalize=True)\n",
        "# test_value_counts = pd.Series(y_test_indices).value_counts(normalize=True)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "# bar_width = 0.2\n",
        "# index = np.arange(len(original_value_counts))\n",
        "\n",
        "# bar1 = ax.barh(index, original_value_counts, bar_width, label='Main Data')\n",
        "# bar2 = ax.barh(index, train_value_counts, bar_width, label='Train Set', left=original_value_counts)\n",
        "# bar3 = ax.barh(index, val_value_counts, bar_width, label='Validation Set', left=original_value_counts + train_value_counts)\n",
        "# bar4 = ax.barh(index, test_value_counts, bar_width, label='Test Set', left=original_value_counts + train_value_counts + val_value_counts)\n",
        "\n",
        "# ax.set_xlabel('Percentages')\n",
        "# ax.set_title('Class Distribution')\n",
        "# ax.set_yticks(index)\n",
        "# ax.set_yticklabels(original_value_counts.index)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1491f7bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(np.argmax(y_train, axis=1))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "\n",
        "class_weights_dict = {class_num: weight for class_num, weight in zip(np.unique(y_train_encoded), class_weights)}\n",
        "\n",
        "print(\"Class Weights Dictionary:\")\n",
        "print(class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fb1aff",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare call backs\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_loss', verbose=1,mode = 'min', patience=15, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761a20f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare DNN model\n",
        "model_4 = keras.models.Sequential([\n",
        "    InputLayer(X_train.shape[1:]),\n",
        "    Dropout(0.5),\n",
        "    Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_4.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['Recall'])\n",
        "\n",
        "#Train simple DNN on extracted features.\n",
        "history_4 = model_4.fit(X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=my_callback,\n",
        "            class_weight = class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6bce5af",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, recall = model_4.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"loss: \", loss)\n",
        "print(\"recall: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77303bc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65557308",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_data = history_4.history\n",
        "\n",
        "loss_df_4 = pd.DataFrame(history_data)\n",
        "loss_df_4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40062562",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_df_4.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6556b9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming y_train and y_test are one-hot encoded, convert them to indices\n",
        "y_train_indices = np.argmax(y_train, axis=1)\n",
        "y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get predicted labels for both training and test datasets\n",
        "train_pred_prob = model_4.predict(X_train)\n",
        "test_pred_prob = model_4.predict(X_test)\n",
        "\n",
        "y_train_pred = np.argmax(train_pred_prob, axis=1)\n",
        "y_test_pred = np.argmax(test_pred_prob, axis=1)\n",
        "\n",
        "# Calculate confusion matrix and classification report for training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(confusion_matrix(y_train_indices, y_train_pred))\n",
        "print(classification_report(y_train_indices, y_train_pred))\n",
        "\n",
        "# Calculate confusion matrix and classification report for test dataset\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(confusion_matrix(y_test_indices, y_test_pred))\n",
        "print(classification_report(y_test_indices, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558b175f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming y_test is in multilabel-indicator format\n",
        "y_pred_proba = model_4.predict(X_test)\n",
        "\n",
        "# Calculate average precision and area under the ROC curve for each class\n",
        "average_precisions = []\n",
        "roc_aucs = []\n",
        "\n",
        "for i in range(n_classes):  # n_classes is the number of classes in your problem\n",
        "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "    average_precisions.append(average_precision_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "    roc_aucs.append(roc_auc_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "\n",
        "# Plot precision-recall curves\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(recall, precision, lw=2, label=f'Class {i + 1} (AP = {average_precisions[i]:.2f}, AUC = {roc_aucs[i]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Each Class')\n",
        "\n",
        "# Create a DataFrame for the legend information\n",
        "legend_data = {'Class': [f'Class {i + 1}' for i in range(n_classes)],\n",
        "               'Average Precision': average_precisions,\n",
        "               'AUC': roc_aucs}\n",
        "legend_df = pd.DataFrame(legend_data)\n",
        "plt.show()\n",
        "legend_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de0626df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
        "# from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Assuming y_test_indices and y_test_pred are obtained as mentioned in your code\n",
        "# Convert to binary format\n",
        "y_test_binary = label_binarize(y_test_indices, classes=range(n_classes))\n",
        "y_pred_binary = label_binarize(y_test_pred, classes=range(n_classes))\n",
        "\n",
        "# Calculate precision, recall, and AP\n",
        "model4_precision = precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model4_recall = recall_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model4_AP = average_precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "print(f'Weighted-Averaged Precision: {model4_precision:.2f}')\n",
        "print(f'Weighted-Averaged Recall: {model4_recall:.2f}')\n",
        "print(f'Weighted-Averaged AP: {model4_AP:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668ca8c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_features_5 = np.concatenate([mobilenet_v2_features,\n",
        "                                 resnet50_features,\n",
        "                                 vgg16_features,\n",
        "                                 densenet_features], axis=-1)\n",
        "print('Final feature maps shape', final_features_5.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be41850e",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_value_counts = pd.Series(y.argmax(axis=1)).value_counts(normalize=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features_5, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
        "\n",
        "# y_train_indices = np.argmax(y_train, axis=1)\n",
        "# y_val_indices = np.argmax(y_val, axis=1)\n",
        "# y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# train_value_counts = pd.Series(y_train_indices).value_counts(normalize=True)\n",
        "# val_value_counts = pd.Series(y_val_indices).value_counts(normalize=True)\n",
        "# test_value_counts = pd.Series(y_test_indices).value_counts(normalize=True)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "# bar_width = 0.2\n",
        "# index = np.arange(len(original_value_counts))\n",
        "\n",
        "# bar1 = ax.barh(index, original_value_counts, bar_width, label='Main Data')\n",
        "# bar2 = ax.barh(index, train_value_counts, bar_width, label='Train Set', left=original_value_counts)\n",
        "# bar3 = ax.barh(index, val_value_counts, bar_width, label='Validation Set', left=original_value_counts + train_value_counts)\n",
        "# bar4 = ax.barh(index, test_value_counts, bar_width, label='Test Set', left=original_value_counts + train_value_counts + val_value_counts)\n",
        "\n",
        "# ax.set_xlabel('Percentages')\n",
        "# ax.set_title('Class Distribution')\n",
        "# ax.set_yticks(index)\n",
        "# ax.set_yticklabels(original_value_counts.index)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6a4cd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(np.argmax(y_train, axis=1))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "\n",
        "class_weights_dict = {class_num: weight for class_num, weight in zip(np.unique(y_train_encoded), class_weights)}\n",
        "\n",
        "print(\"Class Weights Dictionary:\")\n",
        "print(class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8d00c91",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare call backs\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_loss', verbose=1,mode = 'min', patience=15, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641a5d89",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare DNN model\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model_5 = keras.models.Sequential([\n",
        "    InputLayer(X_train.shape[1:]),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.00005)\n",
        "model_5.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['Recall'])\n",
        "\n",
        "#Train simple DNN on extracted features.\n",
        "history_5 = model_5.fit(X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=my_callback,\n",
        "            class_weight = class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c25b937d",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, recall = model_5.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"loss: \", loss)\n",
        "print(\"recall: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c160b0f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f48ba7",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_data = history_5.history\n",
        "\n",
        "loss_df_5 = pd.DataFrame(history_data)\n",
        "loss_df_5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2933772",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_df_5.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b574b6da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming y_train and y_test are one-hot encoded, convert them to indices\n",
        "y_train_indices = np.argmax(y_train, axis=1)\n",
        "y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get predicted labels for both training and test datasets\n",
        "train_pred_prob = model_5.predict(X_train)\n",
        "test_pred_prob = model_5.predict(X_test)\n",
        "\n",
        "y_train_pred = np.argmax(train_pred_prob, axis=1)\n",
        "y_test_pred = np.argmax(test_pred_prob, axis=1)\n",
        "\n",
        "# Calculate confusion matrix and classification report for training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(confusion_matrix(y_train_indices, y_train_pred))\n",
        "print(classification_report(y_train_indices, y_train_pred))\n",
        "\n",
        "# Calculate confusion matrix and classification report for test dataset\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(confusion_matrix(y_test_indices, y_test_pred))\n",
        "print(classification_report(y_test_indices, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa515595",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming y_test is in multilabel-indicator format\n",
        "y_pred_proba = model_5.predict(X_test)\n",
        "\n",
        "# Calculate average precision and area under the ROC curve for each class\n",
        "average_precisions = []\n",
        "roc_aucs = []\n",
        "\n",
        "for i in range(n_classes):  # n_classes is the number of classes in your problem\n",
        "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "    average_precisions.append(average_precision_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "    roc_aucs.append(roc_auc_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "\n",
        "# Plot precision-recall curves\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(recall, precision, lw=2, label=f'Class {i + 1} (AP = {average_precisions[i]:.2f}, AUC = {roc_aucs[i]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Each Class')\n",
        "\n",
        "# Create a DataFrame for the legend information\n",
        "legend_data = {'Class': [f'Class {i + 1}' for i in range(n_classes)],\n",
        "               'Average Precision': average_precisions,\n",
        "               'AUC': roc_aucs}\n",
        "legend_df = pd.DataFrame(legend_data)\n",
        "plt.show()\n",
        "legend_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc17ef63",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Assuming y_test_indices and y_test_pred are obtained as mentioned in your code\n",
        "# Convert to binary format\n",
        "y_test_binary = label_binarize(y_test_indices, classes=range(n_classes))\n",
        "y_pred_binary = label_binarize(y_test_pred, classes=range(n_classes))\n",
        "\n",
        "# Calculate precision, recall, and AP\n",
        "model5_precision = precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model5_recall = recall_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model5_AP = average_precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "print(f'Weighted-Averaged Precision: {model5_precision:.2f}')\n",
        "print(f'Weighted-Averaged Recall: {model5_recall:.2f}')\n",
        "print(f'Weighted-Averaged AP: {model5_AP:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2332c0ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_features_6 = np.concatenate([inception_features,\n",
        "                                 xception_features,\n",
        "                                 nasnet_features,\n",
        "                                    ],axis=-1)\n",
        "print('Final feature maps shape', final_features_6.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34e8eed",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_value_counts = pd.Series(y.argmax(axis=1)).value_counts(normalize=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features_6, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.05, stratify=y_train, random_state=42)\n",
        "\n",
        "# y_train_indices = np.argmax(y_train, axis=1)\n",
        "# y_val_indices = np.argmax(y_val, axis=1)\n",
        "# y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# train_value_counts = pd.Series(y_train_indices).value_counts(normalize=True)\n",
        "# val_value_counts = pd.Series(y_val_indices).value_counts(normalize=True)\n",
        "# test_value_counts = pd.Series(y_test_indices).value_counts(normalize=True)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "# bar_width = 0.2\n",
        "# index = np.arange(len(original_value_counts))\n",
        "\n",
        "# bar1 = ax.barh(index, original_value_counts, bar_width, label='Main Data')\n",
        "# bar2 = ax.barh(index, train_value_counts, bar_width, label='Train Set', left=original_value_counts)\n",
        "# bar3 = ax.barh(index, val_value_counts, bar_width, label='Validation Set', left=original_value_counts + train_value_counts)\n",
        "# bar4 = ax.barh(index, test_value_counts, bar_width, label='Test Set', left=original_value_counts + train_value_counts + val_value_counts)\n",
        "\n",
        "# ax.set_xlabel('Percentages')\n",
        "# ax.set_title('Class Distribution')\n",
        "# ax.set_yticks(index)\n",
        "# ax.set_yticklabels(original_value_counts.index)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b6a7c92",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(np.argmax(y_train, axis=1))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "\n",
        "class_weights_dict = {class_num: weight for class_num, weight in zip(np.unique(y_train_encoded), class_weights)}\n",
        "\n",
        "print(\"Class Weights Dictionary:\")\n",
        "print(class_weights_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71db86aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "807843da",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prepare call backs\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_loss', verbose=1,mode = 'min', patience=15, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c0961ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras import regularizers\n",
        "#Prepare DNN model\n",
        "model_6 = keras.models.Sequential([\n",
        "    InputLayer(X_train.shape[1:]),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(n_classes, activation='softmax', kernel_regularizer=regularizers.l1(0.001)\n",
        "         )])\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "custom_optimizer = Adam(learning_rate=0.005)\n",
        "\n",
        "model_6.compile(optimizer=custom_optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['Recall'])\n",
        "\n",
        "#model_6.compile(optimizer='adam',\n",
        " #             loss='categorical_crossentropy',\n",
        "  #            metrics=['Recall'])\n",
        "\n",
        "#Train simple DNN on extracted features.\n",
        "history_6 = model_6.fit(X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=my_callback,\n",
        "            class_weight = class_weights_dict\n",
        "                       )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0381592f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "y_pred_proba = model_6.predict(X_test)\n",
        "\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "loss = log_loss(y_true, y_pred_proba)\n",
        "\n",
        "print(f\"Multi Class Log Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6932f4f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, accuracy = model_6.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"loss: \", loss)\n",
        "print(\"accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86921c65",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3665923",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_data = history_6.history\n",
        "\n",
        "loss_df_6 = pd.DataFrame(history_data)\n",
        "loss_df_6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75fe9940",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_df_6.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1dca23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming y_train and y_test are one-hot encoded, convert them to indices\n",
        "y_train_indices = np.argmax(y_train, axis=1)\n",
        "y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get predicted labels for both training and test datasets\n",
        "train_pred_prob = model_6.predict(X_train)\n",
        "test_pred_prob = model_6.predict(X_test)\n",
        "\n",
        "y_train_pred = np.argmax(train_pred_prob, axis=1)\n",
        "y_test_pred = np.argmax(test_pred_prob, axis=1)\n",
        "\n",
        "# Calculate confusion matrix and classification report for training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(confusion_matrix(y_train_indices, y_train_pred))\n",
        "print(classification_report(y_train_indices, y_train_pred))\n",
        "\n",
        "# Calculate confusion matrix and classification report for test dataset\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(confusion_matrix(y_test_indices, y_test_pred))\n",
        "print(classification_report(y_test_indices, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d28efdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_test is in multilabel-indicator format\n",
        "y_pred_proba = model_6.predict(X_test)\n",
        "\n",
        "# Calculate average precision and area under the ROC curve for each class\n",
        "average_precisions = []\n",
        "roc_aucs = []\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(n_classes):  # n_classes is the number of classes in your problem\n",
        "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "    average_precisions.append(average_precision_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "    roc_aucs.append(roc_auc_score(y_test[:, i], y_pred_proba[:, i]))\n",
        "\n",
        "    # Plot precision-recall curves for each class\n",
        "    plt.plot(recall, precision, lw=2, label=f'Class {i + 1} (AP = {average_precisions[i]:.2f}, AUC = {roc_aucs[i]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Each Class')\n",
        "plt.show()\n",
        "\n",
        "# Create a DataFrame for the legend information\n",
        "legend_data = {'Class': [f'Class {i + 1}' for i in range(n_classes)],\n",
        "               'Average Precision': average_precisions,\n",
        "               'AUC': roc_aucs}\n",
        "legend_df = pd.DataFrame(legend_data)\n",
        "legend_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d85f8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
        "# from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Assuming y_test_indices and y_test_pred are obtained as mentioned in your code\n",
        "# Convert to binary format\n",
        "y_test_binary = label_binarize(y_test_indices, classes=range(n_classes))\n",
        "y_pred_binary = label_binarize(y_test_pred, classes=range(n_classes))\n",
        "\n",
        "# Calculate precision, recall, and AP\n",
        "model6_precision = precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model6_recall = recall_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "model6_AP = average_precision_score(y_test_binary, y_pred_binary, average='weighted')\n",
        "\n",
        "print(f'Weighted-Averaged Precision: {model6_precision:.2f}')\n",
        "print(f'Weighted-Averaged Recall: {model6_recall:.2f}')\n",
        "print(f'Weighted-Averaged AP: {model6_AP:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30285113",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_names = [\"Model\"] + [f\"Model {i}\" for i in range(1, 7)]  # Change the range to include Model 6\n",
        "\n",
        "compare = pd.DataFrame({\n",
        "    \"Model\": model_names,\n",
        "    \"Precision\": [model_precision, model1_precision, model2_precision, model3_precision, model4_precision, model5_precision, model6_precision], \n",
        "    \"Recall\": [model_recall, model1_recall, model2_recall, model3_recall, model4_recall, model5_recall, model6_recall],  \n",
        "    \"AP\": [model_AP, model1_AP, model2_AP, model3_AP, model4_AP, model5_AP, model6_AP] \n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af2013fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_palette = \"Reds\" \n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "plt.subplot(311)\n",
        "compare_precision = compare.sort_values(by=\"Precision\", ascending=False)\n",
        "ax = sns.barplot(x=\"Precision\", y=\"Model\", data=compare_precision, palette=new_palette)\n",
        "ax.bar_label(ax.containers[0], fmt=\"%.3f\", fontsize=10)\n",
        "plt.title(\"Precision Comparison\")\n",
        "\n",
        "plt.subplot(312)\n",
        "compare_recall = compare.sort_values(by=\"Recall\", ascending=False)\n",
        "ax = sns.barplot(x=\"Recall\", y=\"Model\", data=compare_recall, palette=new_palette)\n",
        "ax.bar_label(ax.containers[0], fmt=\"%.3f\", fontsize=10)\n",
        "plt.title(\"Recall Comparison\")\n",
        "\n",
        "plt.subplot(313)\n",
        "compare_ap = compare.sort_values(by=\"AP\", ascending=False)\n",
        "ax = sns.barplot(x=\"AP\", y=\"Model\", data=compare_ap, palette=new_palette)\n",
        "ax.bar_label(ax.containers[0], fmt=\"%.3f\", fontsize=10)\n",
        "plt.title(\"Average Precision Comparison\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0262e59f",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 3, figsize=(15, 8))\n",
        "\n",
        "loss_df_1.plot(ax=axs[0, 0])\n",
        "axs[0, 0].set_title('Model 0')\n",
        "axs[0, 0].set_xlabel('Epochs')\n",
        "axs[0, 0].set_ylabel('Loss')\n",
        "\n",
        "loss_df_1.plot(ax=axs[0, 1])\n",
        "axs[0, 1].set_title('Model 1')\n",
        "axs[0, 1].set_xlabel('Epochs')\n",
        "axs[0, 1].set_ylabel('Loss')\n",
        "\n",
        "loss_df_2.plot(ax=axs[0, 2])\n",
        "axs[0, 2].set_title('Model 2')\n",
        "axs[0, 2].set_xlabel('Epochs')\n",
        "axs[0, 2].set_ylabel('Loss')\n",
        "\n",
        "loss_df_3.plot(ax=axs[1, 0])\n",
        "axs[1, 0].set_title('Model 3')\n",
        "axs[1, 0].set_xlabel('Epochs')\n",
        "axs[1, 0].set_ylabel('Loss')\n",
        "\n",
        "loss_df_4.plot(ax=axs[1, 1])\n",
        "axs[1, 1].set_title('Model 4')\n",
        "axs[1, 1].set_xlabel('Epochs')\n",
        "axs[1, 1].set_ylabel('Loss')\n",
        "\n",
        "loss_df_5.plot(ax=axs[1, 2])\n",
        "axs[1, 2].set_title('Model 5')\n",
        "axs[1, 2].set_xlabel('Epochs')\n",
        "axs[1, 2].set_ylabel('Loss')\n",
        "\n",
        "loss_df_6.plot(ax=axs[2, 0])\n",
        "axs[2, 0].set_title('Model 6')\n",
        "axs[2, 0].set_xlabel('Epochs')\n",
        "axs[2, 0].set_ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbf07b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def images_to_array2(data_dir, labels_dataframe, img_size = (224,224,3)):\n",
        "    '''\n",
        "    Do same as images_to_array but omit some unnecessary steps for test data.\n",
        "    '''\n",
        "    images_names = labels_dataframe['id']\n",
        "    data_size = len(images_names)\n",
        "    X = np.zeros([data_size, img_size[0], img_size[1], 3], dtype=np.uint8)\n",
        "    \n",
        "    for i in tqdm(range(data_size)):\n",
        "        image_name = images_names[i]\n",
        "        img_dir = os.path.join(data_dir, image_name+'.jpg')\n",
        "        img_pixels = tf.keras.preprocessing.image.load_img(img_dir, target_size=img_size)\n",
        "        X[i] = img_pixels\n",
        "        \n",
        "    print('Ouptut Data Size: ', X.shape)\n",
        "    return X\n",
        "\n",
        "test_data = images_to_array2(test_dir, sample_df, img_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678f1fa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Extract test data features.\n",
        "inception_features = get_features(InceptionV3, inception_preprocessor, img_size, test_data)\n",
        "xception_features = get_features(Xception, xception_preprocessor, img_size, test_data)\n",
        "nasnet_features = get_features(NASNetLarge, nasnet_preprocessor, img_size, test_data)\n",
        "# inc_resnet_features = get_features(InceptionResNetV2, inc_resnet_preprocessor, img_size, test_data)\n",
        "\n",
        "test_features = np.concatenate([inception_features,\n",
        "                                 xception_features,\n",
        "                                 nasnet_features,\n",
        "#                                  inc_resnet_features\n",
        "                                ],axis=-1)\n",
        "print('Final feature maps shape', test_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57440474",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Free up some space.\n",
        "del test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80599425",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Predict test labels given test data features.\n",
        "y_pred = model_6.predict(test_features, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9ddf9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create submission file\n",
        "for b in dog_breeds:\n",
        "    sample_df[b] = y_pred[:,class_to_num[b]]\n",
        "    \n",
        "sample_df.to_csv('submission.csv', index=None)\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}