{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing the required libraries\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from keras import backend, Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e75786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reading the training dataset and performing the EDA ( Exploratory Data Analysis ) in the upcoming cells\"\"\"\n",
    "\n",
    "path = '../input/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "dataset = pd.read_csv('../input/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Performing EDA ( Exploratory Data Analysis ) \"\"\"\n",
    "\n",
    "\n",
    "#Having a look at the data\n",
    "print(\"Number of rows in data = \", dataset.shape[0])\n",
    "print(\"Number of columns in data = \", dataset.shape[1])\n",
    "print(\"\\n\")\n",
    "print(\"---Sample data---\")\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06bbd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Performing EDA ( Exploratory Data Analysis ) \"\"\"\n",
    "\n",
    "#Visualising the Training data : Number of comments vs. Label\n",
    "labels = dataset.columns.values[2:]\n",
    "labels_count = dataset.iloc[:, 2:].sum().values\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize = (15, 8))\n",
    "fig = sns.barplot(labels, labels_count)\n",
    "plt.title(\"Comments vs. Label\", fontsize = 24)\n",
    "plt.ylabel('Number of comments', fontsize = 20)\n",
    "plt.xlabel('Label ', fontsize = 20)\n",
    "rects = fig.patches\n",
    "for rect, label in zip(rects, labels_count):\n",
    "    height = rect.get_height()\n",
    "    fig.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha = 'center', va = 'bottom', fontsize = 18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for Text Preprocessing\"\"\"\n",
    "\n",
    "#Removing non-english symbols, HTML tags, converting to lower-case, lemmatizing, and finally removing the stop-words \n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(X):\n",
    "    processed = []\n",
    "    for text in X:\n",
    "        text = text[0]\n",
    "        text = re.sub(r'[^\\w\\s]', '',text, re.UNICODE)\n",
    "        text = re.sub('\\n', ' ',text, re.UNICODE)\n",
    "        text = re.sub('<.*?>', '', text)\n",
    "        text = text.lower()\n",
    "        text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "        text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "        text = [word for word in text if not word in stop_words]\n",
    "        processed.append(text)\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Getting the X and Y and preprocessing them\"\"\"\n",
    "\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y_train = dataset.iloc[:, 2:8].values\n",
    "X_train = clean_text(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tokenization and Padding\"\"\"\n",
    "vocab_size = 10000\n",
    "maxlen = 250\n",
    "embed_dim = 20\n",
    "batch_size = 64\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenized_word_list = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"EarlyStopping and ModelCheckpoint\"\"\"\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 2)\n",
    "mc = ModelCheckpoint('model_best.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bb401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating TextCNN model for comment classification\"\"\"\n",
    "\n",
    "#Defining the Layers of the model\n",
    "input_X = Input(shape=(maxlen, ))\n",
    "embed = Embedding(vocab_size, embed_dim)(input_X)\n",
    "conv_1 = Conv1D(filters = 64, kernel_size = 3, activation = 'relu', padding = 'valid')(embed)\n",
    "out_1 = GlobalMaxPooling1D()(conv_1)\n",
    "conv_2 = Conv1D(filters = 64, kernel_size = 5, activation = 'relu', padding = 'valid')(embed)\n",
    "out_2 = GlobalMaxPooling1D()(conv_2)\n",
    "conc = concatenate([out_1, out_2])\n",
    "dense1 = Dense(32, activation = 'relu')(conc)\n",
    "out = Dense(6, activation = 'sigmoid', name = 'output_layer')(dense1)\n",
    "\n",
    "#Defining the model now\n",
    "model = Model(input_X, out)\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da17f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fitting the model\"\"\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_padded, y_train, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "model.fit(X_train, y_train, epochs = 10, batch_size = batch_size, verbose = 1, validation_data = [X_val, y_val], callbacks = [es, mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860de5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing the Test Data and making it ready to be passed to the Model\"\"\"\n",
    "\n",
    "dataset2 = pd.read_csv('../input/test.csv')\n",
    "X_test = dataset2.iloc[:, 1:2].values\n",
    "X_test = clean_text(X_test)\n",
    "tokenized_word_list = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing and creating the test results\"\"\"\n",
    "\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "model = load_model('model_best.h5')\n",
    "y_test = model.predict(X_test_padded, batch_size = 512, verbose = 1)\n",
    "sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "sample_submission[labels] = y_test\n",
    "sample_submission.to_csv('submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c48b36",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
