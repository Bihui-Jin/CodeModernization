{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e766b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import imageio as im\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input/\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images dataset\n",
    "def loadImagesData(glob_path):\n",
    "    images = []\n",
    "    names = []\n",
    "    for img_path in glob.glob(glob_path):\n",
    "        # load/resize images with cv2\n",
    "        names.append(os.path.basename(img_path))\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        images.append(img) # already 32x32\n",
    "    return (images,names)\n",
    "# map of training label to list of images\n",
    "trainData = {}\n",
    "namesData = {}\n",
    "for label in os.listdir('../input/train/'):\n",
    "    (images,names) = loadImagesData(f\"../input/train/{label}/*.jpg\")\n",
    "    print(f\"../input/train/{label}/*.jpg\")\n",
    "    trainData[label] = images\n",
    "    namesData[label] = names\n",
    "print(\"train labels:\", \",\".join(trainData.keys()))\n",
    "print(len(trainData['train']))\n",
    "# show some data\n",
    "plt.figure(figsize=(4,2))\n",
    "columns = 4\n",
    "for i in range(0,8):\n",
    "    plt.subplot(8 / columns + 1, columns, i + 1)\n",
    "    plt.imshow(trainData['train'][i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ef24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv('../input/train.csv')\n",
    "print(train_meta.shape)\n",
    "print(train_meta.has_cactus.value_counts())\n",
    "# lookup table of name to has_cactus\n",
    "lookupY = {}\n",
    "for i in range(0,len(train_meta)):\n",
    "    row = train_meta.iloc[i,:]\n",
    "    lookupY[row.id] = row.has_cactus\n",
    "train_meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f17ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build x/y dataset\n",
    "trainList = []\n",
    "maxCount = 4364 # number of has_cactus = 0\n",
    "counts = {'0':0,'1':0}\n",
    "for (i,image) in enumerate(trainData['train']):\n",
    "    label = lookupY[namesData['train'][i]]\n",
    "    counts[str(label)] = 1 + counts[str(label)]\n",
    "    if counts[str(label)] < maxCount:\n",
    "        trainList.append({\n",
    "            'label': label,\n",
    "            'data': image\n",
    "        })\n",
    "# shuffle dataset\n",
    "random.shuffle(trainList)\n",
    "# dataframe and display\n",
    "train_df = pd.DataFrame(trainList)\n",
    "gc.collect()\n",
    "print(train_df.shape)\n",
    "print(train_df.label.value_counts())\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0900c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode training data\n",
    "data_stack = np.stack(train_df['data'].values)\n",
    "dfloats = data_stack.astype(np.float)\n",
    "all_x = np.multiply(dfloats, 1.0 / 255.0) # np.array(train_df['data'].values, dtype=np.float) / 255.0\n",
    "print(all_x.shape)\n",
    "print(type(all_x))\n",
    "all_x[0,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63753606",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y = np.array(train_df.label).astype(np.float)\n",
    "all_y[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec918b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test/training data\n",
    "train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7)\n",
    "print(train_x.shape,test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y and rotation data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    rotation_range=60,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.2, # zoom images\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images\n",
    "datagen.fit(train_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "num_filters = 8\n",
    "input_shape = train_x.shape[1:]\n",
    "output_shape = 1\n",
    "# model\n",
    "m = Sequential()\n",
    "def tdsNet(m):\n",
    "    m.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    m.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dropout(0.5)) # increases val_acc from 0.89 to 0.92, acc from 0.8932 to 0.8987\n",
    "#     m.add(BatchNormalization()) # val_acc falling from 0.89 to 0.8322\n",
    "    m.add(Dense(units = output_shape, activation='sigmoid'))\n",
    "tdsNet(m)\n",
    "# compile adam with decay and use binary_crossentropy for single category dataset\n",
    "m.compile(optimizer = 'nadam',\n",
    "          loss = 'binary_crossentropy', \n",
    "          metrics = ['accuracy'])\n",
    "# show summary\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c672ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "batch_size = 32\n",
    "history = m.fit_generator(datagen.flow(train_x, train_y,\n",
    "                          batch_size=batch_size),\n",
    "                          steps_per_epoch= (train_x.shape[0] // batch_size),\n",
    "                          epochs = 4,\n",
    "                          validation_data=(test_x, test_y),\n",
    "                          workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b68127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "num_filters = 8\n",
    "input_shape = train_x.shape[1:]\n",
    "output_shape = 1\n",
    "# model\n",
    "m = Sequential()\n",
    "\n",
    "def cnnNet(m):\n",
    "    m.add(Conv2D(30, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    m.add(MaxPooling2D(2,2))\n",
    "    m.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
    "    m.add(MaxPooling2D(2,2))\n",
    "    # my update\n",
    "    '''\n",
    "    m.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
    "    m.add(MaxPooling2D(2,2))\n",
    "    '''\n",
    "    # my update\n",
    "    m.add(Dense(7, activation='relu')) # <7 stops working, but higher values do nothing\n",
    "    m.add(Flatten())\n",
    "#     m.add(Dropout(0.8)) # makes no sense: make acc and val_acc lower # increases val_acc from 0.9404 to .., acc from 0.9226 to ..\n",
    "    m.add(Dense(units = output_shape, activation='sigmoid')) #\n",
    "\n",
    "'''\n",
    "# LeNet\n",
    "def cnnNet(m):\n",
    "    m.add(Conv2D(20, 5, padding='same', input_shape=input_shape)) # size: 5\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    m.add(Conv2D(50, 5, padding='same')) # size: 5\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(500)) #\n",
    "    m.add(Activation('relu'))\n",
    "    \n",
    "    m.add(Dense(units = output_shape))\n",
    "    m.add(Activation(\"sigmoid\")) # softmax\n",
    "'''\n",
    "    \n",
    "cnnNet(m)\n",
    "# compile adam with decay and use binary_crossentropy for single category dataset\n",
    "m.compile(optimizer = 'nadam',\n",
    "          loss = 'binary_crossentropy', \n",
    "          metrics = ['accuracy'])\n",
    "# show summary\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train model\n",
    "batch_size = 32\n",
    "history = m.fit_generator(datagen.flow(train_x, train_y,\n",
    "                          batch_size=batch_size),\n",
    "                          steps_per_epoch= (train_x.shape[0] // batch_size),\n",
    "                          epochs = 15, # 4,\n",
    "                          validation_data=(test_x, test_y),\n",
    "                          workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeadb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete x/y dataset\n",
    "trainList = []\n",
    "for (i,image) in enumerate(trainData['train']):\n",
    "    label = lookupY[namesData['train'][i]]\n",
    "    trainList.append({\n",
    "        'label': label,\n",
    "        'data': image\n",
    "    })\n",
    "# shuffle dataset\n",
    "random.shuffle(trainList)\n",
    "# dataframe and display\n",
    "train_df = pd.DataFrame(trainList)\n",
    "gc.collect()\n",
    "# encode training data\n",
    "data_stack = np.stack(train_df['data'].values)\n",
    "dfloats = data_stack.astype(np.float)\n",
    "all_x = np.multiply(dfloats, 1.0 / 255.0)\n",
    "all_x.shape\n",
    "all_y = np.array(train_df.label).astype(np.float)\n",
    "# split test/training data\n",
    "train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7)\n",
    "print(train_x.shape,test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40edf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training model\n",
    "batch_size = 64\n",
    "history = m.fit_generator(datagen.flow(train_x, train_y,\n",
    "                          batch_size=batch_size),\n",
    "                          steps_per_epoch= (train_x.shape[0] // batch_size),\n",
    "                          epochs = 15, # 4,\n",
    "                          validation_data=(test_x, test_y),\n",
    "                          workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sample submission format\n",
    "pd.read_csv('../input/sample_submission.csv').head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output predicted submission csv\n",
    "(test_images, test_names) = loadImagesData(f\"../input/test/test/*.jpg\")\n",
    "data_stack = np.stack(test_images)\n",
    "dfloats = data_stack.astype(np.float32)\n",
    "unknown_x = np.multiply(dfloats, 1.0 / 255.0)\n",
    "# predict\n",
    "predicted = np.ravel(m.predict(unknown_x))\n",
    "submission_df = pd.DataFrame({'id':test_names,'has_cactus':predicted})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "len(submission_df)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
