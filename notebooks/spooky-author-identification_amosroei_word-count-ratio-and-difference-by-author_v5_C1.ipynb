{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import islice\n",
    "import textwrap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "wrapper = textwrap.TextWrapper(initial_indent='', width=70,\n",
    "                               subsequent_indent=' '*3)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "\n",
    "text_column = 'text'\n",
    "label = 'author'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d51ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stemmer and lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# define LemmaCountVectorizer which will find all unique word and their occurrences\n",
    "porter_stemmer = PorterStemmer()\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (porter_stemmer.stem(lemm.lemmatize(w)) for w in analyzer(doc))\n",
    "\n",
    "# Seperate text by author\n",
    "eap_text = list(train_df[train_df['author'] == 'EAP'][text_column].values)\n",
    "hpl_text = list(train_df[train_df['author'] == 'HPL'][text_column].values)\n",
    "mws_text = list(train_df[train_df['author'] == 'MWS'][text_column].values)\n",
    "\n",
    "author_text_dict = dict(zip([0,1,2], [eap_text,hpl_text, mws_text]))\n",
    "\n",
    "# apply LemmaCountVectorizer to the full text\n",
    "full_text = eap_text + mws_text + hpl_text\n",
    "\n",
    "full_tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                       min_df=2,\n",
    "                                       stop_words='english',\n",
    "                                       decode_error='ignore')\n",
    "full_tf = full_tf_vectorizer.fit_transform(full_text)\n",
    "full_feature_names = full_tf_vectorizer.get_feature_names()\n",
    "# full_count_vec = np.asarray(full_tf.sum(axis=0)).ravel()\n",
    "# full_zipped = list(zip(full_feature_names, full_count_vec))\n",
    "\n",
    "# create dataframe to store the word frequency for each author (initialized with zeros):\n",
    "# rows - represents the authors\n",
    "# columns represents each unique word (after stemming and lemmatizing)\n",
    "# so each cell in our new dataframe, represents how many occurrences each author has for each word in all of his lines\n",
    "author_word_freq_df = pd.DataFrame(0.0, index=[0,1,2], columns=full_feature_names)\n",
    "\n",
    "# dictionary contains each word count for each author\n",
    "author_wordcount_dict = {}\n",
    "\n",
    "for author, text in author_text_dict.items():\n",
    "  tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                       min_df=2,\n",
    "                                       stop_words='english',\n",
    "                                       decode_error='ignore')\n",
    "  tf = tf_vectorizer.fit_transform(text)\n",
    "  feature_names = tf_vectorizer.get_feature_names()\n",
    "  count_vec = np.asarray(tf.sum(axis=0)).ravel()\n",
    "  zipped = list(zip(feature_names, count_vec))\n",
    "  author_wordcount_dict[author] = zipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the word frequency dataframe by each author word count\n",
    "for author, zipped in author_wordcount_dict.items():\n",
    "  for word, count in zipped:\n",
    "    author_word_freq_df[word.lower()][author] = count\n",
    "\n",
    "# transpose the dataframe, now the rows contains the unique words, columns contains authors\n",
    "transposed_freq_df = author_word_freq_df.T\n",
    "\n",
    "# Create new columns:\n",
    "# 1. 0_count,1_count, 2_count - represeting the word count difference between the authors\n",
    "# 2. 0_ratio,1_ratio, 2_ratio - represeting the word count ratio between the authors\n",
    "\n",
    "transposed_freq_df['0_count'] = transposed_freq_df[0] - transposed_freq_df[1] - transposed_freq_df[2]\n",
    "transposed_freq_df['1_count'] = transposed_freq_df[1] - transposed_freq_df[0] - transposed_freq_df[2]\n",
    "transposed_freq_df['2_count'] = transposed_freq_df[2] - transposed_freq_df[0] - transposed_freq_df[1]\n",
    "\n",
    "# epsilon is used to prevent division by zero, when a certain word is used by only one author\n",
    "epsilon = 1 \n",
    "transposed_freq_df['0_ratio'] = (transposed_freq_df[0] + epsilon) /(transposed_freq_df[1] + transposed_freq_df[2] + epsilon)\n",
    "transposed_freq_df['1_ratio'] = (transposed_freq_df[1] + epsilon) /(transposed_freq_df[0] + transposed_freq_df[2] + epsilon)\n",
    "transposed_freq_df['2_ratio'] = (transposed_freq_df[2] + epsilon) /(transposed_freq_df[0] + transposed_freq_df[1] + epsilon)\n",
    "\n",
    "transposed_freq_df.sort_values(by='0_ratio', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_count_score(text, author):\n",
    "  word_list = word_tokenize(text)\n",
    "  score = 0\n",
    "    \n",
    "  for word in word_list:\n",
    "    lemm_word = porter_stemmer.stem(lemm.lemmatize(word))    \n",
    "    \n",
    "    if lemm_word in transposed_freq_df.index:\n",
    "      score = score + transposed_freq_df[str(author)+'_count'][lemm_word]\n",
    "    \n",
    "  score = score / len(word_list)\n",
    "  return score\n",
    "\n",
    "def calc_ratio_score(text, author):\n",
    "  word_list = word_tokenize(text)\n",
    "  score = 1\n",
    "    \n",
    "  for word in word_list:\n",
    "    lemm_word = porter_stemmer.stem(lemm.lemmatize(word))    \n",
    "    \n",
    "    if lemm_word in transposed_freq_df.index:\n",
    "      \n",
    "      score = score * transposed_freq_df[str(author)+'_ratio'][lemm_word]\n",
    "    \n",
    "  return score / len(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['eap_freq_count_score'] = train_df[text_column].apply(lambda row: calc_count_score(row, 0))\n",
    "train_df['hpl_freq_count_score'] = train_df[text_column].apply(lambda row: calc_count_score(row, 1))\n",
    "train_df['mws_freq_count_score'] = train_df[text_column].apply(lambda row: calc_count_score(row, 2))\n",
    "\n",
    "train_df['eap_freq_ratio_score'] = train_df[text_column].apply(lambda row: calc_ratio_score(row, 0))\n",
    "train_df['hpl_freq_ratio_score'] = train_df[text_column].apply(lambda row: calc_ratio_score(row, 1))\n",
    "train_df['mws_freq_ratio_score'] = train_df[text_column].apply(lambda row: calc_ratio_score(row, 2))\n",
    "\n",
    "test_df['eap_freq_count_score'] = test_df[text_column].apply(lambda row: calc_count_score(row, 0))\n",
    "test_df['hpl_freq_count_score'] = test_df[text_column].apply(lambda row: calc_count_score(row, 1))\n",
    "test_df['mws_freq_count_score'] = test_df[text_column].apply(lambda row: calc_count_score(row, 2))\n",
    "\n",
    "test_df['eap_freq_ratio_score'] = test_df[text_column].apply(lambda row: calc_ratio_score(row, 0))\n",
    "test_df['hpl_freq_ratio_score'] = test_df[text_column].apply(lambda row: calc_ratio_score(row, 1))\n",
    "test_df['mws_freq_ratio_score'] = test_df[text_column].apply(lambda row: calc_ratio_score(row, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "author_mapping_dict = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "cols_to_drop = ['id', 'text']\n",
    "X_train = train_df.drop(cols_to_drop+['author'], axis=1)\n",
    "X_test = test_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "y_train = train_df['author'].map(author_mapping_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softprob',\n",
    "                            colsample_bytree = 0.3,\n",
    "                            learning_rate = 0.1,\n",
    "                            max_depth = 3, \n",
    "                            alpha = 10,\n",
    "                            n_estimators = 10, num_round=2000)\n",
    "xgb_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(y_pred)\n",
    "out_df.columns = ['EAP', 'HPL', 'MWS']\n",
    "out_df.insert(0, 'id', test_id)\n",
    "out_df.to_csv(\"sub_fe.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
