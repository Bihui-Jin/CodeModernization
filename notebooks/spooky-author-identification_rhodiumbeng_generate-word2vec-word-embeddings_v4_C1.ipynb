{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41126202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a40a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "print(train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e503ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove everything that isn't word or space\n",
    "    text = re.sub(r'\\_', '', text)      # remove underscore\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69baf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean train_df['text']\n",
    "train_df['text'] = train_df['text'].map(lambda x: clean_text(x))\n",
    "train_df['text'] = train_df['text'].map(lambda x: x.strip().split())\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e191fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['text'].map(lambda x: clean_text(x))\n",
    "test_df['text'] = test_df['text'].map(lambda x: x.strip().split())\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  \n",
    "# iterate through each row in train_df \n",
    "for i in range(len(train_df)):\n",
    "    data.append(train_df['text'][i])\n",
    "for j in range(len(test_df)):\n",
    "    data.append(test_df['text'][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb63c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "embedding = gensim.models.Word2Vec(data, size = 50, window = 5, min_count = 1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.train(data,total_examples=len(data),epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2de5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert author labels into one-hot encodings\n",
    "train_df['author'] = pd.Categorical(train_df['author'])\n",
    "df_Dummies = pd.get_dummies(train_df['author'], prefix='author')\n",
    "train_df = pd.concat([train_df, df_Dummies], axis=1)\n",
    "# Check the conversion\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeaafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text']\n",
    "Y = train_df[['author_EAP', 'author_HPL', 'author_MWS']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, X[0])\n",
    "print(Y.shape, Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df['text']\n",
    "print(X_test.shape, X_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_avg(text):\n",
    "    \"\"\"Given a list of words, extract the respective GloVe representations\n",
    "    and average the values into a single vector encoding the text meaning.\"\"\"\n",
    "    # initialize the average word vector\n",
    "    avg = np.zeros((50,))\n",
    "    # average the word vector by looping over the words in text\n",
    "    for w in text:\n",
    "        avg += embedding[w]\n",
    "    avg = avg/len(text)\n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_avg = np.zeros((X.shape[0], 50)) # initialize X_avg\n",
    "for i in range(X.shape[0]):\n",
    "    X_avg[i] = text_to_avg(X[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe37d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_avg.shape)\n",
    "print(X_avg[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_avg = np.zeros((X_test.shape[0], 50)) # initialize X_test_avg\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_avg[i] = text_to_avg(X_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_avg.shape)\n",
    "print(X_test_avg[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X_avg, Y, test_size=0.2, random_state=123)\n",
    "print(X_train.shape, Y_train.shape, X_dev.shape, Y_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(50,)))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate the model for 30 epochs\n",
    "history = model.fit(X_train, Y_train, epochs=30, batch_size=None, validation_data=(X_dev, Y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and visualise the training and validation losses\n",
    "loss = history.history['loss']\n",
    "dev_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='training loss')\n",
    "plt.plot(epochs, dev_loss, 'b', label='validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d84222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(50,)))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f62a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for 10 epochs\n",
    "model.fit(X_avg, Y, epochs=10, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda93b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "preds = model.predict(X_test_avg)\n",
    "print(preds.shape)\n",
    "print(preds[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the predicted labels to be the one with the highest probability\n",
    "pred_labels = []\n",
    "for i in range(len(X_test_avg)):\n",
    "    pred_label = np.argmax(preds[i])\n",
    "    pred_labels.append(pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_labels[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(preds, columns=['EAP','HPL','MWS'])\n",
    "result.insert(0, 'id', test_df['id'])\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file in csv format\n",
    "result.to_csv('submission.csv', index=False, float_format='%.20f')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
