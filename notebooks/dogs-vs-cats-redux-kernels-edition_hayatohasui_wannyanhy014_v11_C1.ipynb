{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362335bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfea6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee120cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile # ZIPアーカイブを作成するためのクラス\n",
    "\n",
    "# train.zipの解凍\n",
    "with zipfile.ZipFile('/kaggle/input/dogs-vs-cats-redux-kernels-edition/train.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/kaggle/working/train')\n",
    "\n",
    "# test.zipの解凍\n",
    "with zipfile.ZipFile('/kaggle/input/dogs-vs-cats-redux-kernels-edition/test.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/kaggle/working/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5883ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # みんな大好き numpy（Pythonでの機械学習の計算をより速く、効率的に行えるようにする拡張モジュール）\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a10a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータのディレクトリのパスだよ\n",
    "train_dir = '/kaggle/working/train/train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31255974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testデータのディレクトリのパスだよ\n",
    "test_dir = '/kaggle/working/test/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像のサイズ統一\n",
    "IMG_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa77bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを読み込む用の関数\n",
    "def load_data(data_dir,sample_size=1000):\n",
    "    images = [] # 写真を入れる用のリストちゃん\n",
    "    labels = [] # その写真のラベル（わんこかにゃんこか）を入れる用のリストちゃん\n",
    "    files = os.listdir(data_dir)[:sample_size] # listdirはファイルの一覧を得ることができるやつ\n",
    "#     print(f'（発見） {len(files)} files in {data_dir}')  # デバッグ文（見つけたディレクトリを表示）\n",
    "    for file in files: # for文でくるくる回してく〜\n",
    "        img_path = os.path.join(data_dir, file) # ディレクトリのパスとファイル名（写真名）をくっつけて（join）、ファイル（写真）のパスを作る。それをimg_pathに入れる\n",
    "#         print(f'（ファイル読み込み） {img_path}')  # デバッグ文（読み込んだファイルを表示）\n",
    "        img = cv2.imread(img_path) # 写真を取得〜\n",
    "        if img is not None: # もし、写真がNoneじゃなかったら（つまり、写真がきちんとあったら）\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) # その写真のサイズとかを調整〜\n",
    "            images.append(img) # リストの中に一枚ずつ入れてく〜\n",
    "            label = 1 if 'dog' in file else 0 # 写真がもし、わんこなら 1, にゃんこなら 0とラベル付して\n",
    "            labels.append(label) # そのラベルをリストに入れてく〜\n",
    "        else: #　それ以外（つまり、写真がきちんと読み込めてなかったら）\n",
    "            print(f'（エラー）ファイル読み込めてねーじゃんか！！ {img_path}')  # デバッグ文（読み込めてないとエラーを吐く）\n",
    "    return np.array(images) / 255.0, np.array(labels) # かえりち（写真とラベルのリストたち）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータとtestデータを読み込んでる\n",
    "X_train, y_train = load_data(train_dir,sample_size=1000)\n",
    "X_test, y_test = load_data(test_dir, sample_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'X_train shape: {X_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066433f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'y_train shape: {y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bf654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_test shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'y_test shape: {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bee0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ拡張しとこ〜\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59648059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを作る\n",
    "def create_model(neuron):\n",
    "    # ライブラリとか関数とかをインポートする\n",
    "    Dense = keras.layers.Dense\n",
    "    Conv2D = keras.layers.Conv2D\n",
    "    MaxPooling2D = keras.layers.MaxPooling2D\n",
    "    Flatten = keras.layers.Flatten\n",
    "    Dropout = keras.layers.Dropout\n",
    "    \n",
    "    # モデルを定義する\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # ここから畳み込みの醍醐味。レイヤー追加してく〜\n",
    "    \n",
    "    # まず、32このフィルターと3x3のカーネルサイズを持ってる畳み込みそう。活性化関数はRelu。あと画像の形状の指定。今回は3だからカラー（あか、あお、緑）だよ\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "    # 2x2のプーリング層。空間サイズを半分に\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # 次、64このフィルターと3x3のカーネルサイズを持ってる畳み込みそう。活性化関数はRelu。\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    # 2x2のプーリング層。空間サイズを半分に\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # 次、128このフィルターと3x3のカーネルサイズを持ってる畳み込みそう。活性化関数はRelu。\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    # 2x2のプーリング層。空間サイズを半分に\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # 全部の結合そうを追加　　　　　　　　\n",
    "    # Flatten(): 3次元の特徴マップを1次元のベクトルに変換\n",
    "    model.add(Flatten())\n",
    "    # neuron数のユニットを持つ全結合層。活性化関数はRelu\n",
    "    model.add(Dense(neuron, activation='relu'))\n",
    "    # ドロップアウト正則化。半分のニューロンをランダムに無効。\n",
    "    model.add(Dropout(0.5))\n",
    "    # 出力層。シグモイド\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # モデルをコンパイルする\n",
    "    # バイナリクロスエントロピー損失関数。Adam。モデルの評価は精度で\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # モデルを返す\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0eb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    model.save(filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e93c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_model(filename):\n",
    "    return load_model(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習し、テストデータで評価し、スコアを表示する関数\n",
    "def fit_epoch(neuron, batch, epochs, initial_epoch=0, model_filename='model.h5'):\n",
    "    if initial_epoch == 0: # もし、モデルを作るのが初めてなら\n",
    "        # さっき作ったモデル作る関数を実行（ニューロン数送る）\n",
    "        model = create_model(neuron)\n",
    "    else: # もうすでに作ったことがあるなら\n",
    "        model = load_existing_model(model_filename) #モデルを読み込む\n",
    "        \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # モデルを学習する\n",
    "    hist = model.fit(datagen.flow(X_train, y_train, batch_size=batch),\n",
    "                     steps_per_epoch=len(X_train) // batch,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     epochs=epochs,\n",
    "                     initial_epoch=initial_epoch,\n",
    "                     verbose=1)\n",
    "    \n",
    "    # モデルを評価する。そしてそれをprint\n",
    "    score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('正解率=', score[1], 'loss=', score[0])\n",
    "    \n",
    "    # モデルを保存\n",
    "    save_model(model, model_filename)\n",
    "    \n",
    "    # 学習結果を可視化、グラフ化するよ。ほら、文字全部見るのってだるいじゃん\n",
    "    \n",
    "    # trainデータの精度の流れをグラフに\n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    # testデータの精度の流れをグラフに\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "    # グラフのタイトル。あくちゅありー\n",
    "    plt.title('Accuracy')\n",
    "    # 凡例\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # グラフの表示\n",
    "    plt.show()\n",
    "\n",
    "    # trainデータの損失値の流れをグラフに\n",
    "    plt.plot(hist.history['loss'])\n",
    "    # testデータの損失値の流れをグラフに\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    # グラフのタイトル。\n",
    "    plt.title('Loss')\n",
    "    # 凡例\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # グラフの表示\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 30  # 全体のエポック数〜\n",
    "neuron = 512  # ニューロンの数〜\n",
    "batch = 8  # バッチサイズ〜\n",
    "model_filename = 'model.h5'  # モデルのファイル名（モデルの拡張子は.h5だよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1回目の学習\n",
    "fit_epoch(neuron=neuron, batch=batch, epochs=10, initial_epoch=0, model_filename=model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b3713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2回目の学習\n",
    "fit_epoch(neuron=neuron, batch=batch, epochs=20, initial_epoch=10, model_filename=model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3回目の学習\n",
    "fit_epoch(neuron=neuron, batch=batch, epochs=30, initial_epoch=20, model_filename=model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 定数の設定\n",
    "IMG_SIZE = 64  # 画像のサイズ\n",
    "test_dir = '/kaggle/working/test/test'  # テストデータのディレクトリ\n",
    "model_filename = 'model.h5'  # 学習したモデルのファイル名\n",
    "output_csv = '/kaggle/working/submission.csv'  # 提出するCSVファイルのパス\n",
    "\n",
    "# 画像データの読み込み関数\n",
    "def load_test_data(data_dir):\n",
    "    images = []\n",
    "    filenames = os.listdir(data_dir)\n",
    "    for file in filenames:\n",
    "        img_path = os.path.join(data_dir, file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            images.append(img)\n",
    "        else:\n",
    "            print(f'Error reading {img_path}')\n",
    "    return np.array(images) / 255.0, filenames\n",
    "\n",
    "# テストデータの読み込み\n",
    "X_test, test_filenames = load_test_data(test_dir)\n",
    "\n",
    "# モデルの読み込み\n",
    "model = load_model(model_filename)\n",
    "\n",
    "# 予測の実行\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 予測結果を0または1に変換\n",
    "predictions = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# CSVファイルの作成\n",
    "output_df = pd.DataFrame({\n",
    "    'id': [os.path.splitext(f)[0] for f in test_filenames],  # ファイル名から拡張子を除去してIDにする\n",
    "    'label': predictions\n",
    "})\n",
    "output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f'CSVファイル {output_csv} を作成しました')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
