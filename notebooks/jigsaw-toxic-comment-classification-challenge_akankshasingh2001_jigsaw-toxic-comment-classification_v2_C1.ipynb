{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7e5c7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f206fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8af92e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458d2fbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\n",
        "test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd1ba0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367f76d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train.isnull().sum())\n",
        "print(test.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd43d5dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = train[\"comment_text\"]\n",
        "\n",
        "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
        "\n",
        "x_test = test[\"comment_text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9c4cc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "max_feature = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_feature)\n",
        "tokenizer.fit_on_texts(list(x_train))\n",
        "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
        "tokenized_test = tokenizer.texts_to_sequences(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52db5305",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_train[:1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e20e90",
      "metadata": {},
      "outputs": [],
      "source": [
        "maxlen = 50\n",
        "x_train = pad_sequences(tokenized_train, maxlen = maxlen)\n",
        "x_test = pad_sequences(tokenized_test, maxlen = maxlen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b066ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "input = Input(shape = (maxlen,))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37d719cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "x = Embedding(max_feature, embed_size)(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2ca0cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = LSTM(60, return_sequences = True, name = \"lstm_layer\")(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b4bb58",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = GlobalMaxPool1D()(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76001824",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = Dropout(0.1)(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7589887",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = Dense(50, activation = \"relu\")(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51cf5bb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Once again we have implemented dropout to prevent overfitting\n",
        "\n",
        "x = Dropout(0.1)(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23463b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = Dense(6, activation = \"sigmoid\")(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb381e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model(inputs = input, outputs = x)\n",
        "model.compile(loss = \"binary_crossentropy\",\n",
        "             optimizer = \"adam\",\n",
        "             metrics = [\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e86e3ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 2\n",
        "model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = 0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8768bce6",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(x_test,batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd44d408",
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n",
        "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304b032c",
      "metadata": {},
      "outputs": [],
      "source": [
        " \n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}