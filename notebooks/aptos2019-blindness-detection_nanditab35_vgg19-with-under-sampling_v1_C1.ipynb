{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74242ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d38fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import scipy.io as sio\n",
    "#import cv2\n",
    "#import imutils\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow.image\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"/kaggle/input\"))\n",
    "\n",
    "path = '/kaggle/input/'\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_lbl = pd.read_csv(path + 'train.csv')\n",
    "df_test_lbl = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "m_tr = np.shape(df_train_lbl)[0]\n",
    "m_te = np.shape(df_test_lbl)[0]\n",
    "\n",
    "print(m_tr)\n",
    "\n",
    "no_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==0])[0])/m_tr)\n",
    "print(no_dr_ratio\n",
    "\n",
    "mild_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==1])[0])/m_tr)\n",
    "print(mild_dr_ratio)\n",
    "\n",
    "moderate_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==2])[0])/m_tr)\n",
    "print(moderate_dr_ratio)\n",
    "\n",
    "severe_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==3])[0])/m_tr)\n",
    "print(severe_dr_ratio)\n",
    "\n",
    "proliferative_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==4])[0])/m_tr)\n",
    "print(proliferative_dr_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2953df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under-sampling of dataset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "train_path = path + 'train_images/'\n",
    "test_path = path + 'test_images/'\n",
    "\n",
    "all_images = glob.glob(train_path + '*.png')\n",
    "\n",
    "print(np.shape(df_train_lbl))\n",
    "sampled_train_lbl = pd.DataFrame(columns = df_train_lbl.columns)\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "sampled_train_lbl, y_resampled = rus.fit_resample(df_train_lbl, df_train_lbl['diagnosis'])\n",
    "\n",
    "print(sorted(Counter(y_resampled).items()))\n",
    "print(np.shape(sampled_train_lbl))\n",
    "\n",
    "sampled_m_tr = np.shape(sampled_train_lbl)[0]\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa01f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resize_tf(img_path, image_dim):\n",
    "    filename = tf.placeholder(tf.string, name=\"inputFile\")\n",
    "    fileContent = tf.read_file(filename, name=\"loadFile\")\n",
    "    image = tf.image.decode_png(fileContent, name=\"decodePng\")\n",
    "    \n",
    "    resize_nearest_neighbor = tf.image.resize_images(image, size=[image_dim,image_dim], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    feed_dict={filename: img_path}\n",
    "    with sess.as_default():\n",
    "        actual_resize_nearest_neighbor = resize_nearest_neighbor.eval(feed_dict)\n",
    "        #plt.imshow(actual_resize_nearest_neighbor)\n",
    "    return actual_resize_nearest_neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b33fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = image_resize_tf(\"../input/train_images/875d2ffcbf47.png\", 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the data\n",
    "from random import shuffle\n",
    "\n",
    "idx_arr = [i for i in range(0,sampled_m_tr)]\n",
    "shuffle(idx_arr)\n",
    "m_train_validate = int(sampled_m_tr*0.7)\n",
    "m_validate = sampled_m_tr - m_train_validate\n",
    "idx_train = idx_arr[:m_train_validate]\n",
    "idx_validate = idx_arr[m_train_validate:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27351b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing training images\n",
    "img_arr_train = np.ndarray(shape=(m_train_validate, 224, 224, 3))\n",
    "lbl_train = np.ndarray(shape=(m_train_validate, 5))\n",
    "#one_hot_targets = np.eye(nb_classes)[targets]\n",
    "idx = 0\n",
    "k = 0\n",
    "for idx in range(0,np.shape(sampled_train_lbl)[0]):\n",
    "    if idx in idx_train:\n",
    "        name = sampled_train_lbl[idx][0] + '.png'\n",
    "        lbl_train[k,:] = np.eye(5)[int(sampled_train_lbl[idx][1])].T\n",
    "        img = image_resize_tf(train_path + name, 224)\n",
    "        #print(img)\n",
    "        img_arr_train[k,:,:,:] = img\n",
    "        k += 1\n",
    "print(np.shape(img_arr_train))\n",
    "print(np.shape(lbl_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing validating images\n",
    "img_arr_validate = np.ndarray(shape=(m_validate, 224, 224, 3))\n",
    "lbl_validate = np.ndarray(shape=(m_validate, 5))\n",
    "idx = 0\n",
    "k = 0\n",
    "for idx in range(0,np.shape(sampled_train_lbl)[0]):\n",
    "    if idx in idx_validate:\n",
    "        name = sampled_train_lbl[idx][0] + '.png'\n",
    "        lbl_validate[k,:] = np.eye(5)[int(sampled_train_lbl[idx][1])].T\n",
    "        img = image_resize_tf(train_path + name, 224)\n",
    "        #print(img)\n",
    "        img_arr_validate[k,:,:,:] = img\n",
    "        k += 1\n",
    "\n",
    "print(np.shape(img_arr_validate))\n",
    "print(np.shape(lbl_validate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ec82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg19 code\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "#Instantiate an empty model\n",
    "model = Sequential([\n",
    "Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'),\n",
    "Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "Conv2D(128, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Flatten(),\n",
    "Dense(4096, activation='relu'),\n",
    "Dense(4096, activation='relu'),\n",
    "#Dense(1000, activation='relu'),\n",
    "Dense(5, activation='softmax'),    \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=[\"accuracy\"])\n",
    "sgd = SGD(lr=0.0001, momentum=0.9)\n",
    "#adm = Adam()\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=sgd, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f37ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_arr_train = img_arr_train/255\n",
    "#img_arr_validate = img_arr_validate/255\n",
    "\n",
    "# contering the the image array for training\n",
    "\n",
    "k = 0\n",
    "for k in range(0,np.size(img_arr_train,1)):\n",
    "    img_train = img_arr_train[k,:,:,:]\n",
    "    img_train_scaled = np.asarray(img_train)\n",
    "    mean1, std1 = img_train_scaled.mean(), img_train_scaled.std()\n",
    "    img_train_scaled = (img_train_scaled - mean1)/std1\n",
    "    img_arr_train[k,:,:,:] = img_train_scaled\n",
    "    \n",
    "print(np.shape(img_arr_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e218d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contering the the image array for validating\n",
    "\n",
    "k = 0\n",
    "for k in range(0,np.size(img_arr_validate,1)):\n",
    "    img_validate = img_arr_validate[k,:,:,:]\n",
    "    img_validate_scaled = np.asarray(img_validate)\n",
    "    mean1, std1 = img_validate_scaled.mean(), img_validate_scaled.std()\n",
    "    img_validate_scaled = (img_validate_scaled - mean1)/std1\n",
    "    img_arr_validate[k,:,:,:] = img_validate_scaled\n",
    "    \n",
    "print(np.shape(img_arr_validate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lbls = np.array(sampled_train_lbl['diagnosis']).reshape(sampled_m_tr,1)\n",
    "history = model.fit(x=img_arr_train,y=lbl_train,validation_data=(img_arr_validate, lbl_validate),batch_size=64,epochs=200,verbose=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_arr_train\n",
    "del img_arr_train\n",
    "del img_arr_validate\n",
    "del df_train_lbl\n",
    "del df_test_lbl\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_images = glob.glob(test_path + '*.png')\n",
    "df_sample_sub = pd.read_csv(path + 'sample_submission.csv')\n",
    "m_test = np.shape(df_sample_sub)[0]\n",
    "test_images = np.ndarray(shape=(m_test, 224, 224, 3))\n",
    "k = 0\n",
    "\n",
    "for row in df_sample_sub.iterrows():\n",
    "    name = row[1]['id_code'] + '.png'\n",
    "    img = image_resize_tf(test_path + name, 224)\n",
    "    img_test_scaled = np.asarray(img)\n",
    "    mean1, std1 = img_test_scaled.mean(), img_test_scaled.std()\n",
    "    img_test_scaled = (img_test_scaled - mean1)/std1\n",
    "    test_images[k,:,:,:] = img_test_scaled\n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad385030",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict_proba(test_images)\n",
    "y_test = np.argmax(scores,axis=1)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cba174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_sub['diagnosis'] = y_test\n",
    "df_sample_sub['diagnosis'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/kaggle/working/\")\n",
    "df_sample_sub.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
