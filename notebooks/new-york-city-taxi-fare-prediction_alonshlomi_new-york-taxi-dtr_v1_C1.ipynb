{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f834ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee238e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from matplotlib.pyplot import subplots\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss, mean_squared_error)\n",
    "from sklearn.ensemble import \\\n",
    "     (RandomForestRegressor as RF,\n",
    "      GradientBoostingRegressor as GBR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data\n",
    "train =  pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 100_000)\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea706ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all missing values from train data\n",
    "train.dropna(inplace=True)\n",
    "\n",
    "# drop all rows with pickup_latitude < -90 or pickup_latitude > 90\n",
    "train = train.drop(((train[train['pickup_latitude']<-90])).index, axis=0)\n",
    "train = train.drop(((train[train['pickup_latitude']>90])).index, axis=0)\n",
    "\n",
    "# drop all rows with pickup_longitude < -180 or pickup_longitude > 180\n",
    "train = train.drop(((train[train['pickup_longitude']<-180])).index, axis=0)\n",
    "train = train.drop(((train[train['pickup_longitude']>180])).index, axis=0)\n",
    "\n",
    "# drop all rows with dropoff_latitude < -90 or dropoff_latitude > 90\n",
    "train = train.drop(((train[train['dropoff_latitude']<-90])).index, axis=0)\n",
    "train = train.drop(((train[train['dropoff_latitude']>90])).index, axis=0)\n",
    "\n",
    "# drop all rows with dropoff_longitude < -180 or dropoff_longitude > 180\n",
    "train = train.drop(((train[train['dropoff_longitude']<-180])).index, axis=0)\n",
    "train = train.drop(((train[train['dropoff_longitude']>180])).index, axis=0)\n",
    "\n",
    "# drop all rows with fare_amount < 0.5 and fare_amount > 100\n",
    "train = train.drop(((train[train['fare_amount']>100])).index, axis=0)\n",
    "train = train.drop(((train[train['fare_amount']<=0.5])).index, axis=0)\n",
    "\n",
    "# drop all rows with passenger_count < 10 and passenger_count = 0\n",
    "train = train.drop(((train[train['passenger_count']==0])).index, axis=0)\n",
    "train = train.drop(train[train['passenger_count']>10].index, axis = 0)\n",
    "\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate the distance:\n",
    "# Haversine distance calculation\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth specified in radians.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in km\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for the training set\n",
    "# Calculate distance using Haversine formula\n",
    "train['distance'] = haversine_distance(train['pickup_latitude'], train['pickup_longitude'], train['dropoff_latitude'], train['dropoff_longitude'])\n",
    "# Convert pickup datetime to datetime object\n",
    "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\n",
    "# Extract features from pickup datetime\n",
    "train['day_of_week'] = train['pickup_datetime'].dt.dayofweek\n",
    "train['month'] = train['pickup_datetime'].dt.month\n",
    "train['hour'] = train['pickup_datetime'].dt.hour\n",
    "\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe584cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with distance > 28 and distance = 0\n",
    "train = train.drop(((train[train['distance']>28])).index, axis=0)\n",
    "train = train.drop(((train[train['distance']==0])).index, axis=0)\n",
    "\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d79d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = train[['passenger_count','distance', 'day_of_week', 'month', 'hour']]\n",
    "y = train['fare_amount']\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3780eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DTR(max_depth=3) # create a decision tree\n",
    "reg.fit(X_train, y_train) # fit the tree to training data\n",
    "ax = subplots(figsize=(12,12))[1]\n",
    "feature_names = list(X.columns) #  extracts the names of the features from X\n",
    "plot_tree(reg, feature_names=feature_names, ax=ax);# plot the tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190394de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = reg.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(\"Validation RMSE (without CV):\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcefc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the cost-complexity pruning path for the decision tree \n",
    "#calculates a sequence of best subtrees by pruning the decision tree based\n",
    "#on different values of the complexity parameter alpha (ccp_alpha). \n",
    "#function cost_complexity_pruning_path() returns the effective alphas of subtrees during pruning. \n",
    "ccp_path = reg.cost_complexity_pruning_path(X_train, y_train) \n",
    "# Cross-validated grid search\n",
    "# cross-validated grid search for the best value of ccp_alpha\n",
    "kfold = skm.KFold(5, shuffle=True, random_state=10)\n",
    "grid = skm.GridSearchCV(reg, {'ccp_alpha': ccp_path.ccp_alphas}, refit=True, cv=kfold,\n",
    "                            scoring='neg_mean_squared_error')\n",
    "G = grid.fit(X_train, y_train)\n",
    "\n",
    "best_ = grid.best_estimator_\n",
    "#np.mean((y_test - best_.predict(X_test))**2)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, best_.predict(X_val)))\n",
    "print(\"Validation RMSE (with CV):\", rmse_val)\n",
    "print(\"The best value for max_depth using cv\", best_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cccb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once satisfied with the model's performance, train the model on the entire original training set\n",
    "best_.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv')\n",
    "test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19eb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for the training set\n",
    "test['distance'] = haversine_distance(test['pickup_latitude'], test['pickup_longitude'], test['dropoff_latitude'], test['dropoff_longitude'])\n",
    "test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\n",
    "test['day_of_week'] = test['pickup_datetime'].dt.dayofweek\n",
    "test['month'] = test['pickup_datetime'].dt.month\n",
    "test['hour'] = test['pickup_datetime'].dt.hour\n",
    "\n",
    "# Define features for the test set\n",
    "X_test = test[['passenger_count','distance', 'day_of_week', 'month', 'hour']]\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = best_.predict(X_test)\n",
    "# Add the predictions to the test dataframe\n",
    "test['fare_amount'] = y_pred_test\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {'key': test['key'], 'fare_amount': test['fare_amount']},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "\n",
    "# save the predictions to a CSV file\n",
    "submission.to_csv('submission.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
