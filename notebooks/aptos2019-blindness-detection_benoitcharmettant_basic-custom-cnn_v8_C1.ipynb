{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "DATA_PATH = \"../input/\"\n",
    "TRAIN_PATH = DATA_PATH + 'train_images/'\n",
    "\n",
    "print(os.listdir(DATA_PATH))\n",
    "\n",
    "from glob import glob \n",
    "from skimage.io import imread\n",
    "import gc\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tile_dir = DATA_PATH + 'train_images/'\n",
    "\n",
    "df = pd.DataFrame({'path': glob(base_tile_dir +'/*.png')})\n",
    "\n",
    "df['id'] = df.path.map(lambda x: x.split('/')[3].split(\".\")[0])\n",
    "\n",
    "labels = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "labels = labels.rename(index=str, columns={'id_code':'id', 'diagnosis':'label'})\n",
    "\n",
    "df_data = df.merge(labels, on = \"id\")\n",
    "\n",
    "df_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c50881",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 150 # load 80k negative examples\n",
    "\n",
    "# take a random sample of class 0 with size equal to num samples in class 1\n",
    "df_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE*3, random_state = 101)\n",
    "# filter out class 1\n",
    "df_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE*2, random_state = 101)\n",
    "# filter out class 2\n",
    "df_2 = df_data[df_data['label'] == 2].sample(SAMPLE_SIZE*2, random_state = 101)\n",
    "# filter out class 3\n",
    "df_3 = df_data[df_data['label'] == 3].sample(SAMPLE_SIZE, random_state = 101)\n",
    "# filter out class 4\n",
    "df_4 = df_data[df_data['label'] == 4].sample(200, random_state = 101)\n",
    "\n",
    "# concat the dataframes\n",
    "df_data = shuffle(pd.concat([df_0, df_1, df_2, df_3, df_4], axis=0).reset_index(drop=True))\n",
    "\n",
    "print(df_data.head())\n",
    "print(df_data.label.value_counts())\n",
    "\n",
    "# train_test_split # stratify=y creates a balanced validation set.\n",
    "y = df_data['label']\n",
    "df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n",
    "\n",
    "# Create directories\n",
    "train_path = 'base_dir/train'\n",
    "valid_path = 'base_dir/valid'\n",
    "test_path = '../input/test'\n",
    "for fold in [train_path, valid_path]:\n",
    "    for subf in [\"0\", \"1\",\"2\",\"3\",\"4\"]:\n",
    "        os.makedirs(os.path.join(fold, subf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the id as the index in df_data\n",
    "df_data.set_index('id', inplace=True)\n",
    "df_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 96\n",
    "\n",
    "for image in tqdm(df_train['id'].values):\n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image + '.png'\n",
    "    label = str(df_data.loc[image,'label']) # get the label for a certain image\n",
    "    src = os.path.join(TRAIN_PATH, fname)\n",
    "    dst = os.path.join(train_path, label, fname)\n",
    "    \n",
    "    pil_im = Image.open(src)\n",
    "    resized_image = pil_im.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    resized_image.save(dst)\n",
    "\n",
    "for image in tqdm(df_val['id'].values):\n",
    "    fname = image + '.png'\n",
    "    label = str(df_data.loc[image,'label']) # get the label for a certain image\n",
    "    src = os.path.join(TRAIN_PATH, fname)\n",
    "    dst = os.path.join(valid_path, label, fname)\n",
    "    \n",
    "    pil_im = Image.open(src)\n",
    "    resized_image = pil_im.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    resized_image.save(dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(5, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(Adam(0.01), loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Done !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19affef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "reducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=13,\n",
    "                   callbacks=[reducel, earlystopper])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96084fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make a prediction\n",
    "y_pred_keras = model.predict_generator(test_gen, steps=len(df_val), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073360",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_dir = '../input/test_images/'\n",
    "\n",
    "test_files = glob(os.path.join(base_test_dir,'*.png'))\n",
    "\n",
    "os.makedirs('valid/')\n",
    "\n",
    "for image in tqdm(test_files):\n",
    "    fname = image.split('/')[-1]\n",
    "    \n",
    "    src = os.path.join(base_test_dir, fname)\n",
    "    dst = os.path.join(\"valid/\",fname)\n",
    "    \n",
    "    pil_im = Image.open(src)\n",
    "    resized_image = pil_im.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    resized_image.save(dst)\n",
    "    \n",
    "test_files = glob(os.path.join('valid','*.png'))\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "file_batch = 20\n",
    "max_idx = len(test_files)\n",
    "for idx in range(0, max_idx, file_batch):\n",
    "    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n",
    "    test_df['id_code'] = test_df.path.map(lambda x: x.split('/')[1].split(\".\")[0])\n",
    "    test_df['image'] = test_df['path'].map(imread)\n",
    "    K_test = np.stack(test_df[\"image\"].values)\n",
    "    K_test = (K_test - K_test.mean()) / K_test.std()\n",
    "    predictions = model.predict(K_test)\n",
    "    \n",
    "    pred = []\n",
    "    \n",
    "    for l in predictions:\n",
    "        pred.append(np.argmax(l))\n",
    "    \n",
    "    \n",
    "    test_df['diagnosis'] = pred\n",
    "    submission = pd.concat([submission, test_df[[\"id_code\", \"diagnosis\"]]])\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7699daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission\n",
    "# Delete the test_dir directory we created to prevent a Kaggle error.\n",
    "# Kaggle allows a max of 500 files to be saved.\n",
    "\n",
    "shutil.rmtree(train_path)\n",
    "shutil.rmtree(valid_path)\n",
    "shutil.rmtree('valid/')\n",
    "submission.to_csv(\"submission.csv\", index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"submission.csv\")\n",
    "df[\"diagnosis\"].value_counts()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
