{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7dbed2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af67eeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269156ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78a46ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Display all rows\n",
        "pd.set_option('display.max_rows',None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ec0956",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data=pd.read_csv('/kaggle/input/leaf-classification/train.csv.zip')\n",
        "test_data=pd.read_csv('/kaggle/input/leaf-classification/test.csv.zip')\n",
        "train_data.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94befb3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#shape\n",
        "print(f'data contains {train_data.shape[0]} rows and {train_data.shape[1]} columns \\n')\n",
        "#missing data\n",
        "print(f'missing data per column is \\n {train_data.isna().sum()}')\n",
        "#duplicate\n",
        "duplicated_data=train_data.duplicated()\n",
        "#print(f'Number of duplicated rows = {len(duplicated_data[duplicated_data[1]==True])}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6da088e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "#using GPU for faster training\n",
        "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "use_cuda=torch.cuda.is_available()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c9e83da",
      "metadata": {},
      "outputs": [],
      "source": [
        "#split into X and y to split to train and test\n",
        "X=train_data.loc[0:,train_data.columns!='species']\n",
        "X=X.drop(\"id\",axis=1)\n",
        "y=LabelEncoder().fit_transform(train_data.loc[0:,train_data.columns=='species'])\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
        "\n",
        "#to (train) tensors\n",
        "X_train_tensor=torch.tensor(X_train.values)\n",
        "y_train_tensor=torch.tensor(y_train)\n",
        "train_tensor=TensorDataset(X_train_tensor,y_train_tensor)\n",
        "\n",
        "#to (test) tensors\n",
        "X_test_tensor=torch.tensor(X_test.values)\n",
        "y_test_tensor=torch.tensor(y_test)\n",
        "test_tensor=TensorDataset(X_test_tensor,y_test_tensor)\n",
        "\n",
        "#train and test and kaggle test\n",
        "batch_size=64\n",
        "train_dataloader=DataLoader(train_tensor,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader=DataLoader(test_tensor,batch_size=batch_size,shuffle=True)\n",
        "test_kaggle_dataloader=DataLoader(test_data,batch_size=batch_size,shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4197939a",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tensor,y_train_tensor=X_train_tensor.to(device),y_train_tensor.to(device)\n",
        "X_test_tensor,y_test_tensor=X_test_tensor.to(device),y_test_tensor.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32448b6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "net=nn.Sequential(nn.Linear(192,5),\n",
        "                 nn.ReLU(),\n",
        "                 nn.Linear(5,9),\n",
        "                 nn.ReLU(),\n",
        "                 nn.Linear(9,99),\n",
        "                 nn.Softmax())\n",
        "net=net.double()\n",
        "net.to(device)\n",
        "#net=net.cuda() if use_cuda else net\n",
        "#train_dataloader=train_dataloader.cuda() if use_cuda  else train_dataloader\n",
        "#test_dataloader=test_dataloader.cuda() if use_cuda  else test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61438057",
      "metadata": {},
      "outputs": [],
      "source": [
        "#optimizer and loss\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "learning_rate=0.0001\n",
        "momentum=0.9\n",
        "optimizer=optim.SGD(net.parameters(),lr=learning_rate,momentum=momentum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4610c16b",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_losses=[]\n",
        "test_losses=[]\n",
        "\n",
        "epochs=10\n",
        "for epoch in range(epochs):\n",
        "    #Training\n",
        "    net.train()\n",
        "    train_loss=0.0\n",
        "    for features,target in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        features=features.to(device).double()\n",
        "        target=target.to(device)\n",
        "        outputs=net(features.double())\n",
        "        loss=criterion(outputs,target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss+=loss.item()*features.size(0)\n",
        "    train_loss/=len(train_dataloader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "        #_,pred=torch.max(outputs,1)\n",
        "    net.eval()\n",
        "    test_loss=0.0\n",
        "    correct=0\n",
        "    total=0\n",
        "    with torch.no_grad():\n",
        "        for features_t,target_t in test_dataloader:\n",
        "            features_t=features_t.to(device).double()\n",
        "            target_t=target_t.to(device)\n",
        "            outputs_t=net(features_t)\n",
        "            loss_t=criterion(outputs_t,target_t)\n",
        "            test_loss+=loss_t.item()*features.size(0)\n",
        "            _,pred_t=torch.max(outputs_t,1)\n",
        "            total+=target.size(0)\n",
        "            correct+=(pred_t==target_t).sum().item()\n",
        "    test_loss/=len(test_dataloader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {(100 * correct / total):.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3735e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "index=test_data['id']\n",
        "test=torch.tensor(test_data.drop('id',axis=1).values)\n",
        "test=test.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270ac2c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = ['Acer_Capillipes', 'Acer_Circinatum', 'Acer_Mono', 'Acer_Opalus', 'Acer_Palmatum', 'Acer_Pictum', 'Acer_Platanoids', 'Acer_Rubrum', 'Acer_Rufinerve', 'Acer_Saccharinum', 'Alnus_Cordata', 'Alnus_Maximowiczii', 'Alnus_Rubra', 'Alnus_Sieboldiana', 'Alnus_Viridis', 'Arundinaria_Simonii', 'Betula_Austrosinensis', 'Betula_Pendula', 'Callicarpa_Bodinieri', 'Castanea_Sativa', 'Celtis_Koraiensis', 'Cercis_Siliquastrum', 'Cornus_Chinensis', 'Cornus_Controversa', 'Cornus_Macrophylla', 'Cotinus_Coggygria', 'Crataegus_Monogyna', 'Cytisus_Battandieri', 'Eucalyptus_Glaucescens', 'Eucalyptus_Neglecta', 'Eucalyptus_Urnigera', 'Fagus_Sylvatica', 'Ginkgo_Biloba', 'Ilex_Aquifolium', 'Ilex_Cornuta', 'Liquidambar_Styraciflua', 'Liriodendron_Tulipifera', 'Lithocarpus_Cleistocarpus', 'Lithocarpus_Edulis', 'Magnolia_Heptapeta', 'Magnolia_Salicifolia', 'Morus_Nigra', 'Olea_Europaea', 'Phildelphus', 'Populus_Adenopoda', 'Populus_Grandidentata', 'Populus_Nigra', 'Prunus_Avium', 'Prunus_X_Shmittii', 'Pterocarya_Stenoptera', 'Quercus_Afares', 'Quercus_Agrifolia', 'Quercus_Alnifolia', 'Quercus_Brantii', 'Quercus_Canariensis', 'Quercus_Castaneifolia', 'Quercus_Cerris', 'Quercus_Chrysolepis', 'Quercus_Coccifera', 'Quercus_Coccinea', 'Quercus_Crassifolia', 'Quercus_Crassipes', 'Quercus_Dolicholepis', 'Quercus_Ellipsoidalis', 'Quercus_Greggii', 'Quercus_Hartwissiana', 'Quercus_Ilex', 'Quercus_Imbricaria', 'Quercus_Infectoria_sub', 'Quercus_Kewensis', 'Quercus_Nigra', 'Quercus_Palustris', 'Quercus_Phellos', 'Quercus_Phillyraeoides', 'Quercus_Pontica', 'Quercus_Pubescens', 'Quercus_Pyrenaica', 'Quercus_Rhysophylla', 'Quercus_Rubra', 'Quercus_Semecarpifolia', 'Quercus_Shumardii', 'Quercus_Suber', 'Quercus_Texana', 'Quercus_Trojana', 'Quercus_Variabilis', 'Quercus_Vulcanica', 'Quercus_x_Hispanica', 'Quercus_x_Turneri', 'Rhododendron_x_Russellianum', 'Salix_Fragilis', 'Salix_Intergra', 'Sorbus_Aria', 'Tilia_Oliveri', 'Tilia_Platyphyllos', 'Tilia_Tomentosa', 'Ulmus_Bergmanniana', 'Viburnum_Tinus', 'Viburnum_x_Rhytidophylloides', 'Zelkova_Serrata']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86e02d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "output=net(test)\n",
        "output=output.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ae7a56",
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(output.detach().cpu().numpy(), columns=classes)\n",
        "submission.insert(0, 'id', index)\n",
        "submission.to_csv('submission.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26aa96d5",
      "metadata": {},
      "outputs": [],
      "source": [
        " \n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}