{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b331f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b05c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37684140",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /kaggle/input/denoising-dirty-documents/train.zip -d /content/denoising_data\n",
    "!unzip /kaggle/input/denoising-dirty-documents/test.zip -d /content/denoising_data\n",
    "!unzip /kaggle/input/denoising-dirty-documents/train_cleaned.zip -d /content/denoising_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7896638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 디렉토리\n",
    "train_dir = \"/content/denoising_data/train\"\n",
    "train_cleaned_dir = \"/content/denoising_data/train_cleaned\"\n",
    "test_dir = \"/content/denoising_data/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더에서 이미지 가져오기 함수\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".png\"):   # PNG파일 로드\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = cv2.imread(img_path)  # OpenCV로 이미지 읽기(BGR형식)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR -> RGB\n",
    "            images.append(img)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ff661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 불러오기\n",
    "train_images = load_images_from_folder(train_dir)\n",
    "train_cleaned_images = load_images_from_folder(train_cleaned_dir)\n",
    "test_images = load_images_from_folder(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacccc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "print(f\"train_images: {len(train_images)}\")\n",
    "print(f\"train_cleaned_images: {len(train_cleaned_images)}\")\n",
    "print(f\"test_images: {len(test_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efaaa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c696ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터셋의 이미지 크기 추출\n",
    "train_sizes = [img.shape[:2] for img in train_images]\n",
    "train_cleaned_sizes = [img.shape[:2] for img in train_cleaned_images]\n",
    "test_sizes = [img.shape[:2] for img in test_images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유니크 이미지 크기\n",
    "unique_train_sizes = np.unique(train_sizes, axis=0)\n",
    "unique_train_cleaned_sizes = np.unique(train_cleaned_sizes, axis=0)\n",
    "unique_test_sizes = np.unique(test_sizes, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "print(f\"train_images:\\n {unique_train_sizes}\")\n",
    "print(f\"train_cleaned_images:\\n {unique_train_cleaned_sizes}\")\n",
    "print(f\"test_images:\\n {unique_test_sizes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041918a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 258 -> 420 변환\n",
    "class PadToSize:\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size  # (높이, 너비)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # 텐서 크기 가져오기 (C, H, W 형식)\n",
    "        _, height, width = img.shape\n",
    "        target_height, target_width = self.target_size\n",
    "\n",
    "        # 패딩 계산\n",
    "        pad_top = (target_height - height) // 2\n",
    "        pad_bottom = target_height - height - pad_top\n",
    "        pad_left = (target_width - width) // 2\n",
    "        pad_right = target_width - width - pad_left\n",
    "\n",
    "        # 패딩 적용\n",
    "        return TF.pad(img, [pad_left, pad_top, pad_right, pad_bottom], fill=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grayscale:\n",
    "    def __call__(self, img):\n",
    "        # Tensor -> PIL 이미지로 변환\n",
    "        pil_img = TF.to_pil_image(img) if isinstance(img, torch.Tensor) else img\n",
    "\n",
    "        # 그레이스케일 변환\n",
    "        grayscale_img = pil_img.convert(\"L\")  # RGB -> Grayscale\n",
    "\n",
    "        # PIL -> Tensor 변환\n",
    "        return TF.to_tensor(grayscale_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed24e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 트랜스폼 설정\n",
    "train_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        PadToSize((420, 540)),                                                  # 크기 통일\n",
    "        v2.RandomApply([v2.GaussianBlur(kernel_size=3)], p=0.4),                # 가우시안 블러 (40% 확률 적용)\n",
    "        v2.RandomApply([v2.ColorJitter(brightness=0.3, contrast=0.3)], p=0.5),  # 밝기 & 대비 조절 (50% 확률 적용)\n",
    "        Grayscale(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 검증용 트랜스폼 설정 \n",
    "val_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        PadToSize((420, 540)),                                                  # 크기 통일\n",
    "        Grayscale(),  \n",
    "        v2.ToDtype(torch.float32, scale=True), \n",
    "    ]\n",
    ")\n",
    "\n",
    "# 시험용 트랜스폼 설정\n",
    "test_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        PadToSize((420, 540)),                                                  # 크기 통일\n",
    "        Grayscale(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Image Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(\n",
    "            [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.png')]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # RGB로 열기\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일 리스트 로드\n",
    "train_files = sorted(\n",
    "    [os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.endswith('.png')]\n",
    ")\n",
    "cleaned_files = sorted(\n",
    "    [os.path.join(train_cleaned_dir, f) for f in os.listdir(train_cleaned_dir) if f.endswith('.png')]\n",
    ")\n",
    "\n",
    "# 80:20 비율로 train / val 데이터셋 분할\n",
    "train_files, val_files, cleaned_train, cleaned_val = train_test_split(\n",
    "    train_files, cleaned_files, test_size=2/9, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef48455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 클래스 (파일 리스트만 넣어서 데이터셋 만들기)\n",
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, train_files, cleaned_files, transform=None):\n",
    "        self.train_files = train_files\n",
    "        self.cleaned_files = cleaned_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        train_img = Image.open(self.train_files[idx]).convert('RGB')\n",
    "        cleaned_img = Image.open(self.cleaned_files[idx]).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            train_img = self.transform(train_img)\n",
    "            cleaned_img = self.transform(cleaned_img)\n",
    "\n",
    "        return train_img, cleaned_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "train_dataset = PairedImageDataset(train_files, cleaned_train, train_transforms)\n",
    "val_dataset = PairedImageDataset(val_files, cleaned_val, val_transforms)\n",
    "test_dataset = ImageDataset(test_dir, test_transforms)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # 훈련 데이터 셔플 O\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)     # 검증 데이터 셔플 X\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)   # 테스트 데이터 셔플 X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a77f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_paired_dataset(paired_loader, num_images=5):\n",
    "    # paired_loader에서 데이터 하나 가져오기\n",
    "    for train_images, cleaned_images in paired_loader:\n",
    "        # 시각화\n",
    "        fig, axes = plt.subplots(num_images, 2, figsize=(8, num_images * 3))  # num_images 행, 2열 (위: train, 아래: cleaned)\n",
    "        for i in range(num_images):\n",
    "            # 첫 번째 열: 원본 이미지 (train)\n",
    "            axes[i, 0].imshow(train_images[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')  # [C, H, W] -> [H, W, C]\n",
    "            axes[i, 0].set_title(f\"Original {i+1}\")\n",
    "            axes[i, 0].axis('off')  # 축 비활성화\n",
    "\n",
    "            # 두 번째 열: 정제된 이미지 (cleaned)\n",
    "            axes[i, 1].imshow(cleaned_images[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')  # [C, H, W] -> [H, W, C]\n",
    "            axes[i, 1].set_title(f\"Cleaned {i+1}\")\n",
    "            axes[i, 1].axis('off')  # 축 비활성화\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break  # 한 번만 실행하도록 break\n",
    "\n",
    "# PairedImageDataset에서 이미지 위아래 시각화\n",
    "visualize_paired_dataset(train_loader, num_images=3)\n",
    "visualize_paired_dataset(val_loader, num_images=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        # Depthwise + Pointwise\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, groups=1, bias=False),\n",
    "            nn.Conv2d(1, 8, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # (8, 210, 270)\n",
    "        )\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1, groups=1, bias=False),\n",
    "            nn.Conv2d(8, 16, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # (16, 105, 135)\n",
    "        )\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, groups=1, bias=False),\n",
    "            nn.Conv2d(16, 32, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # (32, 52, 67)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=1, stride=1, bias=False),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1, output_padding=1, groups=16, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4)\n",
    "        )\n",
    "\n",
    "        # (32 → 16)\n",
    "        self.conv3 = nn.Conv2d(32, 16, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=1, stride=1, bias=False),\n",
    "            nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2, padding=1, output_padding=1, groups=8, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4)\n",
    "        )\n",
    "\n",
    "        # (16 → 8)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=1, stride=1, bias=False),\n",
    "            nn.ConvTranspose2d(1, 1, kernel_size=3, stride=2, padding=1, output_padding=1, groups=1, bias=False),\n",
    "            nn.Sigmoid()  # 픽셀 값을 0~1로 정규화\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding with Skip Connections\n",
    "        enc1_out = self.enc1(x)  # (8, 210, 270)\n",
    "        enc2_out = self.enc2(enc1_out)  # (16, 105, 135)\n",
    "        enc3_out = self.enc3(enc2_out)  # (32, 52, 67)\n",
    "\n",
    "        # Decoding with Skip Connections\n",
    "        dec3_out = self.dec3(enc3_out)  # (16, 104, 134)\n",
    "        dec3_out = F.interpolate(dec3_out, size=(105, 135), mode='bilinear', align_corners=False)  # 크기 보정\n",
    "        dec3_out = torch.cat([dec3_out, enc2_out], dim=1)  # Skip Connection\n",
    "        dec3_out = self.conv3(dec3_out)  # (32 → 16)\n",
    "\n",
    "        dec2_out = self.dec2(dec3_out)  # (8, 208, 268)\n",
    "        dec2_out = F.interpolate(dec2_out, size=(210, 270), mode='bilinear', align_corners=False)  # 크기 보정\n",
    "        dec2_out = torch.cat([dec2_out, enc1_out], dim=1)  # Skip Connection\n",
    "        dec2_out = self.conv2(dec2_out)  # (16 → 8)\n",
    "\n",
    "        dec1_out = self.dec1(dec2_out)  # (1, 420, 540)\n",
    "\n",
    "        # resizing\n",
    "        resized = F.interpolate(dec1_out, size=(420, 540), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Residual network with alpha\n",
    "        outputs = torch.clamp(resized, min=0.001, max=0.999)\n",
    "        return outputs\n",
    "\n",
    "model = DenoisingAutoencoder().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(16, 1, 420, 540), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return torch.sqrt(F.mse_loss(pred, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, lambda_rmse=0.8, lambda_l1=0.2):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.lambda_rmse = lambda_rmse\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.rmse_loss = RMSELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.lambda_rmse * self.rmse_loss(pred, target) + self.lambda_l1 * self.l1_loss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "criterion = HybridLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-3)     # 스케쥴러가 적용될 것을 감안해 0.01부터 시작\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정\n",
    "epochs = 1000  # 최대 학습 에포크 수\n",
    "patience = 5  # 몇 번 연속 val_loss가 증가하면 멈출지\n",
    "early_stop_counter = 0  # Early Stopping 카운터\n",
    "best_val_loss = float('inf')  # 초기 Best Loss를 매우 큰 값으로 설정\n",
    "best_model_state = None  # Best 모델의 가중치를 저장할 변수\n",
    "\n",
    "epoch = 0\n",
    "while epoch < epochs:\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_rmse_loss = 0.0\n",
    "    for train_images, train_cleaned_images in train_loader:\n",
    "        train_images, train_cleaned_images = train_images.to(device), train_cleaned_images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_images)\n",
    "        loss = criterion(outputs, train_cleaned_images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_rmse_loss += RMSELoss()(outputs, train_cleaned_images).item()  # RMSE 계산\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_rmse_loss /= len(train_loader)  # 평균 RMSE 계산\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, RMSE Score : {train_rmse_loss:.4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_rmse_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_cleaned_images in val_loader:\n",
    "            val_images, val_cleaned_images = val_images.to(device), val_cleaned_images.to(device)\n",
    "            outputs = model(val_images)\n",
    "            loss = criterion(outputs, val_cleaned_images)\n",
    "            val_loss += loss.item()\n",
    "            val_rmse_loss += RMSELoss()(outputs, val_cleaned_images).item()  # RMSE 계산\n",
    "\n",
    "    val_loss /= len(val_loader)  # 평균 Loss 계산\n",
    "    val_rmse_loss /= len(val_loader)  # 평균 RMSE 계산\n",
    "\n",
    "    # 스케쥴러 적용\n",
    "    prev_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 러닝 레이트 변경시\n",
    "    if current_lr != prev_lr:\n",
    "        print(f\"Learning Rate updated: {current_lr:.6f}\\n\")\n",
    "\n",
    "    # Best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"New best validation loss: {val_loss:.4f} (Previous: {best_val_loss:.4f}), RMSE Score : {val_rmse_loss:.4f}\")\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"Validation loss increased! Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping triggered! after {epoch+1} epochs\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Best model loaded with val_loss = {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# Best model saved\n",
    "if best_model_state is not None:  # Best 모델 저장\n",
    "    torch.save(best_model_state, \"best_model.pth\")\n",
    "    print(f\"Best model saved with val_loss = {best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"No best model was saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff187c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model 불러오기\n",
    "best_model_path = \"best_model.pth\"  # 저장된 모델 경로\n",
    "model.load_state_dict(torch.load(best_model_path))  # Best Model 불러오기\n",
    "\n",
    "# 테스트\n",
    "model.eval()  # 평가 모드\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device) # 원본 이미지\n",
    "        outputs = model(images) # 출력\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images_and_outputs(images, outputs):\n",
    "    \"\"\"\n",
    "    이미지와 출력 이미지를 열로 구분하여 시각화.\n",
    "    :param images: 원본 이미지 텐서\n",
    "    :param outputs: 모델 출력 텐서\n",
    "    \"\"\"\n",
    "    num_images = images.size(0)  # 전체 이미지 개수\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(10, num_images * 3))  # num_images 행, 2열\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # 첫 번째 열: 원본 이미지\n",
    "        axes[i, 0].imshow(images[i].cpu().numpy().squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title(f\"Original {i + 1}\", fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # 두 번째 열: 출력 이미지\n",
    "        axes[i, 1].imshow(outputs[i].cpu().detach().numpy().squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title(f\"Output {i + 1}\", fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_images_and_outputs(images, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18038ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/content/denoising_data/test\"\n",
    "test_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model 불러오기\n",
    "best_model_path = \"best_model.pth\"  # 저장된 모델 경로\n",
    "model.load_state_dict(torch.load(best_model_path))  # Best Model 불러오기\n",
    "\n",
    "model.eval()\n",
    "all_outputs = []  # 예측 결과를 저장할 리스트\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch)  # outputs의 shape: (batch_size, 1, 420, 540)\n",
    "        all_outputs.append(outputs.cpu())\n",
    "\n",
    "# 모든 배치의 결과를 하나의 텐서로 결합\n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(\"all_outputs shape:\", all_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f440071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF  # 원래 패딩 적용에 사용한 함수\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 전제 조건:\n",
    "# - test_dir: 테스트 이미지가 저장된 폴더 경로 (이미 정의되어 있음)\n",
    "# - all_outputs: 모델 예측 결과가 순서대로 저장된 리스트 또는 텐서\n",
    "#                각 예측 결과는 (1, 420, 540) 크기의 텐서라고 가정\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# 학습 시 사용한 패딩 대상 크기 (PadToSize 클래스에서 사용한 target_size)\n",
    "target_size = (420, 540)  # (높이, 너비)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 원래 PadToSize 클래스에서 사용한 패딩 계산 로직\n",
    "# 원본 이미지에 대해 좌우 및 상하 패딩을 어떻게 적용했는지 계산하는 함수입니다.\n",
    "# ---------------------------------------------------------------------------------\n",
    "def compute_padding(orig_size, target_size):\n",
    "    \"\"\"\n",
    "    원본 이미지 크기(orig_size)를 target_size로 패딩했을 때 추가된 패딩 값을 계산합니다.\n",
    "    :param orig_size: (높이, 너비) 튜플 (원본 이미지 크기)\n",
    "    :param target_size: (높이, 너비) 튜플 (패딩이 적용된 크기)\n",
    "    :return: (pad_top, pad_bottom, pad_left, pad_right)\n",
    "    \"\"\"\n",
    "    orig_height, orig_width = orig_size\n",
    "    target_height, target_width = target_size\n",
    "    # 상단 패딩 계산: 원본 높이가 target보다 작을 때만 패딩 적용\n",
    "    pad_top = (target_height - orig_height) // 2 if target_height > orig_height else 0\n",
    "    # 하단 패딩: 전체 패딩에서 상단 패딩을 뺀 나머지\n",
    "    pad_bottom = target_height - orig_height - pad_top if target_height > orig_height else 0\n",
    "    # 좌측 패딩 계산\n",
    "    pad_left = (target_width - orig_width) // 2 if target_width > orig_width else 0\n",
    "    # 우측 패딩 계산\n",
    "    pad_right = target_width - orig_width - pad_left if target_width > orig_width else 0\n",
    "    return pad_top, pad_bottom, pad_left, pad_right\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 모델 예측 결과에서 원본 영역(패딩이 제거된 영역)만 추출하는 함수\n",
    "# ---------------------------------------------------------------------------------\n",
    "def remove_padding(pred, orig_size, target_size):\n",
    "    \"\"\"\n",
    "    모델 출력(pred)에서 원본 이미지 영역만 크롭합니다.\n",
    "    :param pred: 모델 예측 결과 텐서, shape: (C, target_height, target_width)\n",
    "    :param orig_size: 원본 이미지 크기 (높이, 너비)\n",
    "    :param target_size: 패딩 적용된 크기 (높이, 너비)\n",
    "    :return: 패딩이 제거된 텐서 (원본 영역만), shape: (C, orig_height, orig_width)\n",
    "    \"\"\"\n",
    "    orig_height, orig_width = orig_size\n",
    "    # 위에서 정의한 compute_padding 함수를 이용하여, 상단 및 좌측 패딩 값을 구함\n",
    "    pad_top, _, pad_left, _ = compute_padding(orig_size, target_size)\n",
    "    # pred 텐서에서, (채널, target_height, target_width) 중 원본 영역만 크롭\n",
    "    return pred[:, pad_top:pad_top+orig_height, pad_left:pad_left+orig_width]\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 테스트 이미지 파일 목록 생성 (파일 이름이 숫자로 되어 있다고 가정하고 정렬)\n",
    "# ---------------------------------------------------------------------------------\n",
    "test_file_paths = sorted(\n",
    "    [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith('.png')],\n",
    "    key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    ")\n",
    "\n",
    "# CSV 제출 파일 생성을 위한 픽셀 id와 value를 저장할 리스트\n",
    "submission_data = []\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 각 테스트 이미지에 대해 순차적으로 처리\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 여기서는 예시로 처음 30개의 이미지만 처리합니다.\n",
    "for i in range(72):\n",
    "    # i번째 파일 경로를 가져옴\n",
    "    file_path = test_file_paths[i]\n",
    "    # 파일명에서 이미지 id를 추출 (예: '1.png' -> '1')\n",
    "    image_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # PIL을 사용하여 원본 이미지 크기 읽기 (convert('L')로 그레이스케일 변환)\n",
    "    orig_img = Image.open(file_path).convert('L')\n",
    "    # PIL의 size는 (width, height) 순서이므로, 뒤집어 (높이, 너비)로 저장\n",
    "    orig_width, orig_height = orig_img.size\n",
    "    orig_size = (orig_height, orig_width)  # (높이, 너비)\n",
    "    print(\"orig_size\", orig_size)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 모델 예측 결과 가져오기\n",
    "    # all_outputs는 모델 예측 결과가 순서대로 저장된 변수이며, 각 결과는 (1, 420, 540) 크기임\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    idx = test_file_paths.index(file_path)\n",
    "    pred = all_outputs[idx]  # shape: (1, 420, 540)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 원래 PadToSize 클래스와 동일한 로직을 사용해, 예측 결과에서 패딩 영역 제거 (크롭)\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    cropped_pred = remove_padding(pred, orig_size, target_size)\n",
    "    print('crop size', cropped_pred.shape)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 크롭된 결과 텐서를 numpy 배열로 변환 (채널 차원 제거하여 2D 배열로)\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    pred_np = cropped_pred.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 결과 이미지 확인: matplotlib를 사용해 크롭된(패딩 제거된) 이미지 출력\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    plt.figure()\n",
    "    plt.imshow(pred_np, cmap='gray')\n",
    "    plt.title(f\"{i}th Cropped Output for image {image_id}\\n(Original height: {orig_height} width: {orig_width})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # CSV 파일 제출을 위해, 크롭된 이미지를 픽셀 단위로 flatten\n",
    "    # 각 픽셀의 id는 \"imageID_row_col\" 형식 (행과 열 번호는 1부터 시작)\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    for row in range(orig_height):\n",
    "        for col in range(orig_width):\n",
    "            pixel_id = f\"{image_id}_{row+1}_{col+1}\"\n",
    "            pixel_value = pred_np[row, col]\n",
    "            submission_data.append((pixel_id, pixel_value))\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 최종적으로 CSV 파일로 제출 데이터를 저장\n",
    "# ---------------------------------------------------------------------------------\n",
    "submission_file = 'submission.csv'\n",
    "with open(submission_file, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"value\"])  # CSV 헤더 작성\n",
    "    for row in submission_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Submission file '{submission_file}'이(가) 생성되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
