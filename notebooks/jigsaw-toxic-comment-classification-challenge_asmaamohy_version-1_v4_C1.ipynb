{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944c50b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf1cb50",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf620090",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9393f2ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training data\n",
        "train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
        "# Testing data\n",
        "test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n",
        "# sample \n",
        "sample = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860f5414",
      "metadata": {},
      "outputs": [],
      "source": [
        "#train['comment_text'].iloc[125698]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a19f5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "train['comment_text'].sample(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c12551",
      "metadata": {},
      "outputs": [],
      "source": [
        "# problems in data\n",
        "#[User:Cirt]]\n",
        "#200.83.101.199\n",
        "#(Romania)\n",
        "#hi moron\n",
        "# URLs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a58d623",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label column names\n",
        "labels = list(train.columns[2:])\n",
        "labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3177070a",
      "metadata": {},
      "outputs": [],
      "source": [
        "label = train[['toxic', 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']]\n",
        "print(label.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff9874c",
      "metadata": {},
      "outputs": [],
      "source": [
        "ct1,ct2 = 0,0\n",
        "for i in range(label.shape[0]):\n",
        "    ct = np.count_nonzero(label.iloc[i])\n",
        "    if ct :\n",
        "        ct1 = ct1+1\n",
        "    if ct>1 :\n",
        "        ct2 = ct2+1\n",
        "print(ct1)\n",
        "print(ct2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97b6027",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = [len(train['comment_text'][i]) for i in range(train['comment_text'].shape[0])]\n",
        "\n",
        "print('average length of comment: {:.3f}'.format(sum(x)/len(x)) )\n",
        "bins = [1,200,400,600,800,1000,1200]\n",
        "plt.hist(x, bins=bins)\n",
        "plt.xlabel('Length of comments')\n",
        "plt.ylabel('Number of comments')       \n",
        "plt.axis([0, 1200, 0, 90000])\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd18451c",
      "metadata": {},
      "outputs": [],
      "source": [
        "label.head(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa2f34b",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = np.zeros(label.shape)\n",
        "for ix in range(train['comment_text'].shape[0]):\n",
        "    l = len(train['comment_text'][ix])\n",
        "    if label['toxic'].iloc[ix] :\n",
        "        y[ix][0] = l\n",
        "    if label['severe_toxic'].iloc[ix] :\n",
        "        y[ix][1] = l\n",
        "    if label['obscene'].iloc[ix] :\n",
        "        y[ix][2] = l\n",
        "    if label['threat'].iloc[ix] :\n",
        "        y[ix][3] = l\n",
        "    if label['insult'].iloc[ix] :\n",
        "        y[ix][4] = l\n",
        "    if label['identity_hate'].iloc[ix] :\n",
        "        y[ix][5] = l\n",
        "\n",
        "labelsplt = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "color = ['red','green','blue','yellow','orange','chartreuse']  \n",
        "plt.figure(figsize=(20,20))\n",
        "plt.hist(y,bins = bins,label = labelsplt,color = color)\n",
        "plt.axis([0, 1200, 0, 8000])\n",
        "plt.xlabel('Length of comments')\n",
        "plt.ylabel('Number of comments') \n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e489d74",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.barh(labelsplt,train[labels].sum(axis = 0),color = color)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649d4b92",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get value counts for each class\n",
        "train[labels].sum(axis = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d31121",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get number of comments that have been classified (toxic)\n",
        "# comments_classified = sum(train[labels].sum(axis = 1) !=0)\n",
        "# comments_classified\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440cdf59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# comments_not_classified = comments - comments_classified\n",
        "# print(f\"there are {comments_not_classified} not toxic comments in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b313cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # adding not toxic column\n",
        "# condition = train[labels].sum(axis = 1) == 0\n",
        "# train['not_toxic'] = np.where(condition, 1,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb6fa7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # checking if there is an undefined toxicity type\n",
        "# sum(train[labels].sum(axis = 1) == 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb4c09e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # adding toxicity undefined column\n",
        "# condition = train[labels].sum(axis = 1) == 1\n",
        "# train['undefined_toxic'] = np.where(condition, 1,0) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6f6ebc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # checking if there is a comment classified with a toxic type without classifying it as toxic\n",
        "# train['toxicity_type_defined'] = train[['insult','obscene','identity_hate','threat']].max(axis=1)\n",
        "# condition = (train['toxicity_type_defined']==1) & (train['toxic']==0)\n",
        "# train['soft_toxic'] = np.where(condition, 1,0)\n",
        "# train.drop(['toxicity_type_defined'], axis = 1, inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1bc617a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # adding new labels\n",
        "# labels.extend(['not_toxic', 'undefined_toxic', 'soft_toxic'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef41aa22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# label_counts = train[labels].sum()\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# ax = sns.barplot(x=label_counts.index, y=label_counts.values)\n",
        "# ax.set_yscale(\"log\")\n",
        "# ax.tick_params(labelsize=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a5f115",
      "metadata": {},
      "outputs": [],
      "source": [
        "# heatmap_data = train[labels]\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# ax = sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True)\n",
        "# ax.tick_params(labelsize=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41272e71",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Confirm that all severly toxic comments (n=1595) are toxic:\n",
        "# train.loc[train['severe_toxic']==1,'toxic'].sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fb8b27",
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Remove website links\n",
        "def remove_links(text):\n",
        "    template = re.compile(r'https?://\\S+|www\\.\\S+') \n",
        "    text = template.sub(r'', text)\n",
        "    return text\n",
        "\n",
        "# Remove HTML tags\n",
        "def remove_html(text):\n",
        "    template = re.compile(r'<[^>]*>') \n",
        "    text = template.sub(r'', text)\n",
        "    return text\n",
        "\n",
        "def text2words(text):\n",
        "      return word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "def remove_stopwords(words, stop_words):\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "# Remove none ascii characters\n",
        "def remove_non_ascii(text):\n",
        "    template = re.compile(r'[^\\x00-\\x7E]+') \n",
        "    text = template.sub(r'', text)\n",
        "    return text\n",
        "\n",
        "# Replace none printable characters\n",
        "def remove_non_printable(text):\n",
        "    template = re.compile(r'[\\x00-\\x0F]+') \n",
        "    text = template.sub(r' ', text)\n",
        "    return text\n",
        "\n",
        "# Remove special characters\n",
        "def remove_special_chars(text):\n",
        "        text = re.sub(\"'s\", '', text)\n",
        "        template = re.compile('[\"#$%&\\'()\\*\\+-/:;<=>@\\[\\]\\\\\\\\^_`{|}~]') \n",
        "        text = template.sub(r' ', text)\n",
        "        return text\n",
        "\n",
        "# Replace multiple punctuation \n",
        "def replace_multiplt_punc(text):\n",
        "        text = re.sub('[.!?]{2,}', '.', text)\n",
        "        text = re.sub(',+', ',', text) \n",
        "        return text\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "        text = re.sub('\\d+', ' ', text)\n",
        "        return text\n",
        "\n",
        "def handle_spaces(text):\n",
        "    # Remove extra spaces\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    \n",
        "    # Remove spaces at the beginning and at the end of string\n",
        "    text = text.strip() \n",
        "    \n",
        "    return text\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in text\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize words in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
        "\n",
        "def remove_pattern(text): \n",
        "    # remove hi moron \n",
        "    text= re.sub(r'(hi)(.*)\\1', r'\\1', text)\n",
        "    # remove duplicate words\n",
        "    text= re.sub(r\"\\b(\\w+)(?:\\W+\\1\\b)+\",r'\\1', text,flags=re.IGNORECASE)\n",
        "    # remove [User:Cirt]] \n",
        "    text= re.sub(r\"\\[.*?\\]\", ' ', text)\n",
        "    # remove \\n\\n\n",
        "    text= re.sub(r\"\\n\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def clean_text( text):\n",
        "    text = remove_pattern(text)\n",
        "    text = remove_links(text)\n",
        "    text = remove_html(text)\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_non_printable(text)\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = handle_spaces(text)\n",
        "    words = text2words(text)\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    #words = stem_words(words) #either stem or lemmatize\n",
        "    words = lemmatize_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "\n",
        "    return ' '.join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fbc73dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train['comment_text'].iloc[2])\n",
        "print('-------------------')\n",
        "sample = clean_text(train['comment_text'].iloc[2])\n",
        "sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0aa89a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def return_tweets(df):\n",
        "    texts=[]\n",
        "    for index, item in df.drop(df.columns.difference(['comment_text']), axis=1).iterrows():\n",
        "        message = item[\"comment_text\"]\n",
        "        texts.append(str(message))\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8471eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts = return_tweets(train)\n",
        "test_texts = return_tweets(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791d69bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_corpus(corpus):\n",
        "    return [clean_text(t) for t in corpus]\n",
        "train_texts = clean_corpus(train_texts)\n",
        "test_texts = clean_corpus(test_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8cdf760",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9844082b",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = [len(train_texts[i]) for i in range(len(train_texts))]\n",
        "\n",
        "print('average length of comment: {:.3f}'.format(sum(x)/len(x)) )\n",
        "bins = [1,200,400,600,800,1000,1200]\n",
        "plt.hist(x, bins=bins)\n",
        "plt.xlabel('Length of comments')\n",
        "plt.ylabel('Number of comments')       \n",
        "plt.axis([0, 1200, 0, 90000])\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd648358",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer  \n",
        "tok = Tokenizer(num_words=1000, oov_token='UNK')\n",
        "#tok = Tokenizer(oov_token='UNK')\n",
        "tok.fit_on_texts(train_texts + test_texts)\n",
        "# Extract binary BoW features\n",
        "x_train = tok.texts_to_matrix(train_texts, mode='tfidf')\n",
        "x_test = tok.texts_to_matrix(test_texts, mode='tfidf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d96cf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = np.asarray(label.values).astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad8fffb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c7ba4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tr_X, val_X, tr_y, val_y = train_test_split(x_train, y_train, train_size=0.95, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec50f038",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tr_X.shape)\n",
        "print(tr_y.shape)\n",
        "print(val_X.shape)\n",
        "print(val_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fd4f8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(1000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(6, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(),\n",
        "              loss=losses.binary_crossentropy,\n",
        "              metrics=[metrics.binary_accuracy])\n",
        "\n",
        "history = model.fit(tr_X,\n",
        "                    tr_y,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(val_X, val_y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49cfc1df",
      "metadata": {},
      "outputs": [],
      "source": [
        "acc = history.history['binary_accuracy']\n",
        "val_acc = history.history['val_binary_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6573b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd78222c",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(1000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(6, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(),\n",
        "              loss=losses.binary_crossentropy,\n",
        "              metrics=[metrics.binary_accuracy])\n",
        "\n",
        "history = model.fit(tr_X,\n",
        "                    tr_y,\n",
        "                    epochs=9,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(val_X, val_y))\n",
        "# compute roc_auc score  \n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_pred = model.predict(val_X)\n",
        "roc_auc_score(val_y, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c94f7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#How to create a submission csv file for Kaggle\n",
        "df_sample = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\")\n",
        "y_pred = model.predict(x_test)\n",
        "df_sample[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
        "df_sample.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f530a58a",
      "metadata": {},
      "outputs": [],
      "source": [
        " \n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}