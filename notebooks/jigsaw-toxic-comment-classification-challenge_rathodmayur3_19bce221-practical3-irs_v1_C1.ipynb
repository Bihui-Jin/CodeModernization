{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c91d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is Rathod Mayur || 19BCE221 || This is my 3rd practical of IRS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# I this fetch_20newsgroups dataset for make prediction using CountVectorizer.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c302f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names_out())\n",
    "print(\"***Vector Repersentation***\\n\",X2.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Example for sklearn datasets.\n",
    "# In this is I perform CountVectorizer and then make Prediction.\n",
    "\n",
    "# Create our vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\n",
    "test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Get the training vectors\n",
    "vectors = vectorizer.fit_transform(train.data)\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, train.target)\n",
    "\n",
    "# Get the test vectors\n",
    "vectors_test = vectorizer.transform(test.data)\n",
    "\n",
    "# Predict and score the vectors\n",
    "pred = clf.predict(vectors_test)\n",
    "acc_score = metrics.accuracy_score(test.target, pred)\n",
    "f1_score = metrics.f1_score(test.target, pred, average='macro')\n",
    "\n",
    "print('Total accuracy classification score: {}'.format(acc_score))\n",
    "print('Total F1 classification score: {}'.format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c41b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [\"He is ::having a great Time, at the park time?\",\n",
    "       \"She, unlike most women, is a big player on the park's grass.\",\n",
    "       \"she can't be going\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e845e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(txt)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d794ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not so perfect', 'she not sang', 'she was not perfect']\n",
    "print (\"The text: \", txt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print out a list of words used, and their index in the vectors\n",
    "print('Vocabulary: ')\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b205b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted = tf.fit(txt1)\n",
    "txt_transformed = tf.fit_transform(txt1)\n",
    "txt_transformed.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we wanted to get the vector for one word:\n",
    "print('Hot vector: ')\n",
    "print(vectorizer.transform(['smile']).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fitted.get_feature_names(), idf)))\n",
    "\n",
    "# We see that the tokens 'sang','she' have the most idf weight because \n",
    "# they are the only tokens that appear in one document only.\n",
    "# The token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = dict(zip(txt_fitted.get_feature_names(), idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\n",
    "token_weight.columns=('token','weight')\n",
    "token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
    "token_weight \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar graph on this txt.\n",
    "sns.barplot(x='token', y='weight', data=token_weight)            \n",
    "plt.title(\"Inverse Document Frequency(idf) per token\")\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4895ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names \n",
    "feature_names = np.array(tf.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "\n",
    "print(\"Features with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:3]]))\n",
    "print(\"\\nFeatures with highest idf:\\n{}\".format(feature_names[sorted_by_idf[-3:]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9484562",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13345fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c69202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import string\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a82339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>','',text).strip() # remove html chars\n",
    "    text = re.sub('\\[|\\(.*\\]|\\)','', text).strip() # remove text in square brackets and parenthesis\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation marks\n",
    "    text = re.sub(\"(\\\\W)\",\" \",text).strip() # remove non-ascii chars\n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  # remove words containing numbers\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f75f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert into string\n",
    "train_data.comment_text = train_data.comment_text.astype(str)\n",
    "\n",
    "# now clean the data using clean_comment.\n",
    "train_data.comment_text = train_data.comment_text.apply(clean_comment)\n",
    "train_data.comment_text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "def apply_stemmer(text):\n",
    "    words = text.split()\n",
    "    sent = [snow_stemmer.stem(word) for word in words if not word in set(stopwords)]\n",
    "    return ' '.join(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdfc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stemming\n",
    "train_data.comment_text = train_data.comment_text.apply(apply_stemmer)\n",
    "train_data.comment_text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.comment_text\n",
    "y = train_data.drop(['id','comment_text'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5746e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test =  train_test_split(X,y,test_size = 0.2,random_state = 45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    strip_accents='unicode',     \n",
    "    analyzer='word',            \n",
    "    token_pattern=r'\\w{1,}',    \n",
    "    ngram_range=(1, 3),         \n",
    "    stop_words='english',\n",
    "    sublinear_tf=True)\n",
    "\n",
    "word_vectorizer.fit(x_train)    \n",
    "train_word_features = word_vectorizer.transform(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24447f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = word_vectorizer.transform(x_train)\n",
    "X_test_transformed = word_vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f23b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "seed=100\n",
    "\n",
    "log_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=seed)\n",
    "\n",
    "# fit model\n",
    "classifier_ovr_log = OneVsRestClassifier(log_reg)\n",
    "classifier_ovr_log.fit(X_train_transformed, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_proba = classifier_ovr_log.predict_proba(X_train_transformed)\n",
    "y_test_pred_proba = classifier_ovr_log.predict_proba(X_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_predictions(df,classifier):\n",
    "    df.comment_text = df.comment_text.apply(clean_comment)\n",
    "    df.comment_text = df.comment_text.apply(apply_stemmer)\n",
    "    X_test = df.comment_text\n",
    "    X_test_transformed = word_vectorizer.transform(X_test)\n",
    "    y_test_pred = classifier.predict_proba(X_test_transformed)\n",
    "    return y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=make_test_predictions(test_data,classifier_ovr_log)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred,columns=y.columns)\n",
    "y_pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.concat([test_data.id, y_pred_df], axis=1)\n",
    "submission_df.to_csv('submission.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
