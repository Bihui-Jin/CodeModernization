{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae550b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69459ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# from numpy.random import seed\n",
    "# seed(101)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(101)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "# import inspect\n",
    "import gc\n",
    "\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# from imgaug import augmenters as iaa\n",
    "# import imgaug as ia\n",
    "\n",
    "\n",
    "from keras import models\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
    "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.applications.nasnet import NASNetMobile, NASNetLarge\n",
    "\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "\n",
    "\n",
    "\n",
    "# from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, SGD , RMSprop\n",
    "\n",
    "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "# K.set_image_dim_ordering('th')\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight as cw\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72895c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(r\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa34add",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = r\"../input/\"\n",
    "\n",
    "training_dir = input_directory + r\"train\"\n",
    "testing_dir = input_directory + r\"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = os.listdir(training_dir)\n",
    "test_files = os.listdir(testing_dir)\n",
    "\n",
    "train_labels = []\n",
    "\n",
    "for file in train_files:\n",
    "    train_labels.append(file.split(\".\")[0])\n",
    "    \n",
    "df_train = pd.DataFrame({\"id\": train_files, \"label\": train_labels})\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c35369",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    test_ids.append(int(file.split(\".\")[0]))\n",
    "    test_labels.append(-1)\n",
    "    \n",
    "df_test_1 = pd.DataFrame({\"id\": test_files, \"label\": test_labels})\n",
    "\n",
    "df_test = pd.DataFrame({\"id\": test_ids, \"label\": test_labels})\n",
    "df_test['id'] = df_test.id.astype('int32')\n",
    "df_test['label'] = df_test.label.astype('int32')\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d17a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset tensorflow graph tp free up memory and resource allocation \n",
    "def reset_graph(model=None):\n",
    "    if model:\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# reset callbacks \n",
    "def reset_callbacks(checkpoint=None, reduce_lr=None, early_stopping=None, tensorboard=None):\n",
    "    checkpoint = None\n",
    "    reduce_lr = None\n",
    "    early_stopping = None\n",
    "    tensorboard = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fe115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_graph()\n",
    "# reset_callbacks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size=32, target_size=(96,96), class_mode=\"categorical\", training_dir=training_dir, testing_dir=testing_dir, df_train=df_train, df_test=df_test):\n",
    "    print(\"Generating data following preprocessing...\")\n",
    "    \n",
    "    rescale = 1.0/255\n",
    "\n",
    "    train_batch_size = batch_size\n",
    "    test_batch_size = batch_size\n",
    "\n",
    "    train_shuffle = True\n",
    "    val_shuffle = True\n",
    "    test_shuffle = False\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "#         vertical_flip=True,\n",
    "#         rotation_range=45,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rescale=rescale,\n",
    "        validation_split=0.3\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        df_train, \n",
    "        training_dir, \n",
    "        x_col='id',\n",
    "        y_col='label', \n",
    "        has_ext=True, \n",
    "        target_size=target_size, \n",
    "        class_mode=class_mode, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        seed=42,\n",
    "        subset='training')\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_dataframe(\n",
    "        df_train, \n",
    "        training_dir, \n",
    "        x_col='id',\n",
    "        y_col='label', \n",
    "#         has_ext=True, \n",
    "        target_size=target_size, \n",
    "        class_mode=class_mode, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        seed=42,\n",
    "        subset='validation')\n",
    "\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=rescale)\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        df_test, \n",
    "        testing_dir, \n",
    "        x_col='id',\n",
    "        y_col='label', \n",
    "        has_ext=True, \n",
    "        target_size=target_size, \n",
    "        class_mode=class_mode, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        seed=42)\n",
    "    \n",
    "    class_weights = get_weight(train_generator.classes)\n",
    "    \n",
    "    steps_per_epoch = len(train_generator)\n",
    "    validation_steps = len(validation_generator)\n",
    "    \n",
    "    print(\"Data batches generated.\")\n",
    "    \n",
    "    return train_generator, validation_generator, test_generator, class_weights, steps_per_epoch, validation_steps\n",
    "\n",
    "\n",
    "def get_weight(y):\n",
    "    class_weight_current =  cw.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return class_weight_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7475788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, input_shape=(96, 96, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    if model_name == \"Xception\":\n",
    "        base_model = Xception(include_top=False, input_shape=input_shape)\n",
    "    elif model_name == \"ResNet50\":\n",
    "        base_model = ResNet50(include_top=False, input_shape=input_shape)\n",
    "    elif model_name == \"InceptionV3\":\n",
    "        # base_model = InceptionV3(include_top=False, input_shape=input_shape)\n",
    "        # included weights\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape) \n",
    "    elif model_name == \"InceptionResNetV2\":\n",
    "        base_model = InceptionResNetV2(include_top=False, input_shape=input_shape)\n",
    "    if model_name == \"DenseNet201\":\n",
    "        base_model = DenseNet201(include_top=False, input_shape=input_shape)\n",
    "    if model_name == \"NASNetMobile\":\n",
    "        base_model = NASNetMobile(include_top=False, input_shape=input_shape)\n",
    "    if model_name == \"NASNetLarge\":\n",
    "        base_model = NASNetLarge(include_top=False, input_shape=input_shape)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8663060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(history=None):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba76766",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model_dir = r\"models/\"\n",
    "main_log_dir = r\"logs/\"\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(main_model_dir)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(main_log_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir(main_model_dir)\n",
    "os.mkdir(main_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = main_model_dir + time.strftime('%Y-%m-%d %H-%M-%S') + \"/\"\n",
    "log_dir = main_log_dir + time.strftime('%Y-%m-%d %H-%M-%S')\n",
    "\n",
    "os.mkdir(model_dir)\n",
    "os.mkdir(log_dir)\n",
    "\n",
    "model_file = model_dir + \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4218b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_graph()\n",
    "# reset_callbacks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Settting Callbacks\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_file, \n",
    "    monitor='val_acc', \n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    update_freq='batch')\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    verbose=1)\n",
    "\n",
    "callbacks = [reduce_lr, early_stopping, checkpoint]\n",
    "\n",
    "print(\"Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe14ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Getting Base Model...\")\n",
    "# model = get_model(\"Xception\")\n",
    "# model = get_model(\"ResNet50\")\n",
    "# model = get_model(\"InceptionV3\")\n",
    "# model = get_model(\"InceptionResNetV2\")\n",
    "# model = get_model(\"DenseNet201\")\n",
    "# model = get_model(\"NASNetMobile\")\n",
    "# model = get_model(\"NASNetLarge\")\n",
    "# print(\"complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting...\\n\")\n",
    "start_time = time.time()\n",
    "# print(date_time(1))\n",
    "\n",
    "batch_size = 32\n",
    "target_size = (299, 299)\n",
    "df_test=df_test_1\n",
    "\n",
    "train_generator, validation_generator, test_generator, class_weights, steps_per_epoch, validation_steps = get_data(batch_size=batch_size, target_size=target_size, df_test=df_test)\n",
    "print(\"\\n\\nCompleted ...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2605a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['acc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting...\\n\")\n",
    "start_time = time.time()\n",
    "# print(date_time(1))\n",
    "\n",
    "\n",
    "# create the base pre-trained model\n",
    "# input_shape = (96, 96, 3)\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"\\n\\nCompliling Model ...\\n\")\n",
    "learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "verbose = 1\n",
    "epochs = 1\n",
    "\n",
    "print(\"Trainning Model ...\\n\")\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1,\n",
    "    epochs=1,\n",
    "    verbose=verbose,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=1, \n",
    "    class_weight=class_weights)\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "print(\"\\nElapsed Time: \" + elapsed_time)\n",
    "# print(\"Completed\\n\", date_time(1))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Starting...\\n\")\n",
    "# start_time = time.time()\n",
    "# # print(date_time(1))\n",
    "\n",
    "\n",
    "# # for i, layer in enumerate(base_model.layers):\n",
    "# #    print(i, layer.name)\n",
    "\n",
    "# for layer in model.layers[:249]:\n",
    "#    layer.trainable = False\n",
    "# for layer in model.layers[249:]:\n",
    "#    layer.trainable = True\n",
    "\n",
    "\n",
    "# print(\"\\n\\nCompliling Model ...\\n\")\n",
    "# learning_rate = 0.0001\n",
    "# optimizer = Adam(learning_rate)\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# verbose = 1\n",
    "# epochs = 2\n",
    "\n",
    "# print(\"Trainning Model ...\\n\")\n",
    "# history = model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     epochs=epochs,\n",
    "#     verbose=verbose,\n",
    "#     callbacks=callbacks,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=validation_steps, \n",
    "#     class_weight=class_weights)\n",
    "\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "# print(\"\\nElapsed Time: \" + elapsed_time)\n",
    "# # print(\"Completed\\n\", date_time(1))\n",
    "# print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37001583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_performance(history=history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38670597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(model, test_generator, nsteps=len(test_generator)):\n",
    "    y_preds = model.predict_generator(test_generator, steps=nsteps, verbose=1) \n",
    "    return y_preds[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = generate_result(model, test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    test_ids.append(int(file.split(\".\")[0]))\n",
    "    test_labels.append(-1)\n",
    "\n",
    "df_test = pd.DataFrame({\"id\": test_ids, \"label\": test_labels})\n",
    "df_test['id'] = df_test.id.astype('int32')\n",
    "df_test['label'] = df_test.label.astype('int32')\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv = \"submission.csv\"\n",
    "df_test['label'] = y_preds\n",
    "df_test = df_test.sort_values('id')\n",
    "df_test.to_csv(submission_csv, index=False)\n",
    "\n",
    "df_test.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
