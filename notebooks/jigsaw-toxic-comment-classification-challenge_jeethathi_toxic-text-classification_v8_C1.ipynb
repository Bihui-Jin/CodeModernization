{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f095ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cfb877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558065b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['comment_text']\n",
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "x_test = test['comment_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935811fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_tokenized_test = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983df0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(comment) for comment in x_tokenized_train]\n",
    "print(f'The longest comment is {max(lengths)} words long.')\n",
    "sns.distplot(lengths, kde=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 200\n",
    "X_train = pad_sequences(x_tokenized_train, maxlen=max_length)\n",
    "X_test = pad_sequences(x_tokenized_test, maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf33f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, GlobalAveragePooling1D, Dropout, Dense, LeakyReLU, Activation\n",
    "\n",
    "num_features, embed_size = len(tokenizer.word_index), 128\n",
    "\n",
    "models = []\n",
    "\n",
    "models += [Sequential(), Sequential(), Sequential()]\n",
    "\n",
    "models[0].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n",
    "models[0].add(LSTM(64, return_sequences=True))\n",
    "models[0].add(GlobalAveragePooling1D())\n",
    "models[0].add(Dropout(0.1))\n",
    "models[0].add(Dense(48))\n",
    "models[0].add(LeakyReLU())\n",
    "models[0].add(Dropout(0.1))\n",
    "models[0].add(Dense(6, activation='sigmoid'))\n",
    "models[0].compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "models[1].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n",
    "models[1].add(LSTM(64, return_sequences=True))\n",
    "models[1].add(GlobalAveragePooling1D())\n",
    "models[1].add(Dropout(0.1))\n",
    "models[1].add(Dense(48))\n",
    "models[1].add(Activation('relu'))\n",
    "models[1].add(Dropout(0.1))\n",
    "models[1].add(Dense(6, activation='sigmoid'))\n",
    "models[1].compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "models[2].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n",
    "models[2].add(LSTM(64, return_sequences=True))\n",
    "models[2].add(GlobalAveragePooling1D())\n",
    "models[2].add(Dropout(0.1))\n",
    "models[2].add(Dense(32))\n",
    "models[2].add(Activation('relu'))\n",
    "models[2].add(Dropout(0.1))\n",
    "models[2].add(Dense(16))\n",
    "models[2].add(Activation('relu'))\n",
    "models[2].add(Dropout(0.1))\n",
    "models[2].add(Dense(6, activation='sigmoid'))\n",
    "models[2].compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfbbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "validation_split = 0.1\n",
    "epochs = 1\n",
    "\n",
    "histories = []\n",
    "\n",
    "for model in models:\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split=validation_split,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs)\n",
    "    histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf20f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f'Started predicting for model {i}')\n",
    "    y_pred = model.predict(X_test, batch_size=4096)\n",
    "    y_preds.append(y_pred)\n",
    "    print(f'Predicted stuff for model {i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a78f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i, model in enumerate(models):\n",
    "    y_i = pd.DataFrame(data=y_preds[i],\n",
    "                        columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "    y_i = pd.concat([test['id'], y_i], axis=1)\n",
    "    y.append(y_i)\n",
    "# y = pd.DataFrame(data=y_pred, columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "# y = pd.concat([test['id'], y], axis=1)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y_i in enumerate(y):\n",
    "    filename = f'submision_{i}.csv'\n",
    "    y_i.to_csv(filename, index=False)\n",
    "    print(f'Created file {filename}')\n",
    "# y.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
