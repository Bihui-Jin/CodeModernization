{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# keras libraries\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# text libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98746b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "print(train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df423f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the class distribution for the author label in train_df?\n",
    "train_df['author'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2216b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the character length for the rows and record these\n",
    "train_df['text_length'] = train_df['text'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the histogram plot for text length\n",
    "train_df.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the text characters length in test_df and record these\n",
    "test_df['text_length'] = test_df['text'].str.len()\n",
    "test_df.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert author labels into one-hot encodings\n",
    "train_df['author'] = pd.Categorical(train_df['author'])\n",
    "df_Dummies = pd.get_dummies(train_df['author'], prefix='author')\n",
    "train_df = pd.concat([train_df, df_Dummies], axis=1)\n",
    "# Check the conversion\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to clean text and separate into words\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean train_df['text']\n",
    "train_df['text'] = train_df['text'].map(lambda com : clean_text(com))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean test_df['text']\n",
    "test_df['text'] = test_df['text'].map(lambda com : clean_text(com))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e6bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text']\n",
    "y = train_df[['author_EAP', 'author_HPL', 'author_MWS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "print(X_train.shape, y_train.shape, X_dev.shape, y_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04435cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the class distribution in y_train and y_dev\n",
    "print(y_train.sum(axis=0),'\\n', y_dev.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = CountVectorizer()\n",
    "# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\;|\\:')\n",
    "# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\?|\\;|\\:|\\!|\\'')\n",
    "vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the vocabulary in the training data, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "# examine the document-term matrix created from X_train\n",
    "X_train_dtm = X_train_dtm.toarray()\n",
    "X_train_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n",
    "X_dev_dtm = vect.transform(X_dev)\n",
    "# examine the document-term matrix from X_test\n",
    "X_dev_dtm = X_dev_dtm.toarray()\n",
    "X_dev_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_dtm.shape, y_train.shape)\n",
    "print(X_dev_dtm.shape, y_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daac783",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_shape = X_train_dtm.shape[1]\n",
    "num_class = y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(num_input_shape,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_class, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85626e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_dtm, y_train, epochs=20, batch_size=512,\n",
    "                    validation_data=(X_dev_dtm, y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(num_input_shape,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_class, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_dtm, y_train, epochs=3, batch_size=512,\n",
    "          validation_data=(X_dev_dtm, y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77678450",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_dev_dtm, y_dev)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_df['text']\n",
    "# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n",
    "test_dtm = vect.transform(test)\n",
    "# examine the document-term matrix from X_test\n",
    "test_dtm = test_dtm.toarray()\n",
    "test_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ba91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make author (class) predictions for test_dtm\n",
    "dnn_predictions = model.predict(test_dtm)\n",
    "print(dnn_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dnn_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(dnn_predictions, columns=['EAP','HPL','MWS'])\n",
    "result.insert(0, 'id', test_df['id'])\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file in csv format\n",
    "result.to_csv('rhodium_submission_17.csv', index=False, float_format='%.20f')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
