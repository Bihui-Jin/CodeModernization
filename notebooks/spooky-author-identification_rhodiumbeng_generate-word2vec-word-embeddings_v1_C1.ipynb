{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b265a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb929ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f4db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the class distribution for the author label in train_df?\n",
    "train_df['author'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc76801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the character length for the rows and record these\n",
    "train_df['text_length'] = train_df['text'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the histogram plot for text length\n",
    "train_df.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f382ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the text characters length in test_df and record these\n",
    "test_df['text_length'] = test_df['text'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f571eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert author labels into numerical variables\n",
    "train_df['author_num'] = train_df.author.map({'EAP':0, 'HPL':1, 'MWS':2})\n",
    "# Check conversion for first 5 rows\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eceacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns={'text':'original_text'})\n",
    "train_df['text'] = train_df['original_text'].str[:700]\n",
    "train_df['text_length'] = train_df['text'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf75ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.rename(columns={'text':'original_text'})\n",
    "test_df['text'] = test_df['original_text'].str[:700]\n",
    "test_df['text_length'] = test_df['text'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text']\n",
    "y = train_df['author_num']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3833ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc081f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the class distribution in y_train and y_test\n",
    "print(y_train.value_counts(),'\\n', y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vect = CountVectorizer()\n",
    "# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\;|\\:')\n",
    "# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\?|\\;|\\:|\\!|\\'')\n",
    "vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ce0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the vocabulary in the training data, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "# examine the document-term matrix created from X_train\n",
    "X_train_dtm = X_train_dtm.toarray()\n",
    "X_train_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_y_train = to_categorical(y_train)\n",
    "onehot_y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276443b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "# examine the document-term matrix from X_test\n",
    "X_test_dtm = X_test_dtm.toarray()\n",
    "X_test_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_dtm.shape, onehot_y_train.shape)\n",
    "print(X_test_dtm.shape, onehot_y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(25149,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610414bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e943e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_dtm, onehot_y_train, epochs=20, batch_size=512,\n",
    "                    validation_data=(X_test_dtm, onehot_y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c979c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(25149,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_dtm, onehot_y_train, epochs=5, batch_size=512,\n",
    "          validation_data=(X_test_dtm, onehot_y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_dtm, onehot_y_test)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debac0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the vocabulary in the entire training data, and create the document-term matrix\n",
    "X_dtm = vect.fit_transform(X)\n",
    "# Examine the document-term matrix created from X_train\n",
    "X_dtm = X_dtm.toarray()\n",
    "X_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "onehot_y = to_categorical(y)\n",
    "\n",
    "print(X_dtm.shape, onehot_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DNN models onn entire training set using X_dtm and y\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(27457,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_dtm, onehot_y, epochs=5, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check training accuracy\n",
    "\n",
    "results = model.evaluate(X_dtm, onehot_y)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_df['text']\n",
    "# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n",
    "test_dtm = vect.transform(test)\n",
    "# examine the document-term matrix from X_test\n",
    "test_dtm = test_dtm.toarray()\n",
    "test_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make author (class) predictions for test_dtm\n",
    "dnn_predictions = model.predict(test_dtm)\n",
    "print(dnn_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dnn_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(dnn_predictions, columns=['EAP','HPL','MWS'])\n",
    "result.insert(0, 'id', test_df['id'])\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file in csv format\n",
    "result.to_csv('rhodium_submission_17.csv', index=False, float_format='%.20f')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
