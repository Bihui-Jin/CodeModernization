{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53671d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('../input/siim-isic-melanoma-classification/train.csv')\n",
    "test_csv = pd.read_csv('../input/siim-isic-melanoma-classification/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2638fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.target.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed823f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = [1024, 1024]\n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "train_files = tf.io.gfile.glob(GCS_PATH + '/tfrecords/train*.tfrec')\n",
    "test_files = tf.io.gfile.glob(GCS_PATH + '/tfrecords/test*.tfrec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ba16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMG_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example, labeled):\n",
    "    tfrecord_format = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    } if labeled else {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    if labeled:\n",
    "        label = tf.cast(example['target'], tf.int32)\n",
    "        return image, label\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def get_train_vald_dataset(vald_split=0.2, ordered=False):\n",
    "    dataset = load_dataset(train_files, labeled=True, ordered=ordered)\n",
    "    n = sum(1 for record in dataset)\n",
    "    n_vald = int(vald_split * n)\n",
    "    n_train = n - n_vald\n",
    "    #train_dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    train_dataset = dataset.take(n_train)\n",
    "    train_dataset = train_dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    train_dataset = train_dataset.shuffle(2048)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "    train_dataset = train_dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    \n",
    "    vald_dataset = dataset.skip(n_train)\n",
    "    n1 = sum(1 for rec in vald_dataset)\n",
    "    vald_dataset = vald_dataset.batch(BATCH_SIZE)\n",
    "    vald_dataset = vald_dataset.cache()\n",
    "    vald_dataset = vald_dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    if n_vald != n1:\n",
    "        print(\"Validation Dataset sizes - \", n_vald, n1)\n",
    "    return n_train, train_dataset, n1, vald_dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(test_files, labeled=False, ordered=ordered)\n",
    "    n = sum(1 for record in dataset)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return n, dataset\n",
    "\n",
    "n_train, train_dataset, n_vald, vald_dataset = get_train_vald_dataset()\n",
    "n_test, test_dataset = get_test_dataset(True)\n",
    "print(f'Dataset: {n_train} training images, {n_vald} validation images, {n_test} unlabeled test images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306708b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name=\"\"):\n",
    "    #pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
    "    pretrained_model = tf.keras.applications.Xception(input_shape=[*IMG_SIZE, 3], include_top=False, weights='imagenet')\n",
    "    #pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
    "    #pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
    "    #pretrained_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
    "    # EfficientNet can be loaded through efficientnet.tfkeras library (https://github.com/qubvel/efficientnet)\n",
    "    #pretrained_model = efficientnet.tfkeras.EfficientNetB0(weights='imagenet', include_top=False)\n",
    "    \n",
    "    pretrained_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        pretrained_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        #tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = initialize_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d92bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS = n_train // BATCH_SIZE\n",
    "VALID_STEPS = n_vald // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=TRAIN_STEPS, class_weight={0: 1, 1: 2},\n",
    "                    validation_data=vald_dataset, validation_steps=VALID_STEPS)#, callbacks=[lr_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.predict(test_dataset.map(lambda image, idnum: image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame({'image_name': test_csv['image_name'], 'target': outs.ravel()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717110e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv('submissions.csv', header=True, index=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
