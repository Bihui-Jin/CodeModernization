{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2713b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "main_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
    "test_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n",
    "test_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\n",
    "sample_submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a63f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "def data_cleaning(df):\n",
    "    df['comment_text'] = df['comment_text'].replace('\\n',' ', regex=True).replace('\\r',' ', regex=True).replace('\\t',' ', regex=True) #spaces\n",
    "    df['comment_text'] = df['comment_text'].replace('\\\\n',' ', regex=True)\n",
    "    df['comment_text'] = df['comment_text'].replace('http[S|s]*\\S+|www.\\S+',' ', regex=True) #Links\n",
    "    df['comment_text'] = df['comment_text'].replace('\\d+',' ', regex=True) #numbers\n",
    "    df['comment_text'] = df['comment_text'].replace('\\s{2,}',' ', regex=True) #multiple spaces\n",
    "    df['comment_text'] = df['comment_text'].map(lambda x: x.strip()) #removing trailSpaces\n",
    "    df['comment_text'] = df['comment_text'].str.replace('\\W', ' ') #special characters => most of the puntuat\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acedb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = data_cleaning(main_df)\n",
    "test_df = data_cleaning(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#def tokenizer(text):\n",
    "#    return word_tokenize(text)\n",
    "#print(stopwords.words('english'))\n",
    "new_stopwords = stopwords.words('english')\n",
    "\n",
    "def stopwords_remover(words_list):\n",
    "    text_list = [word for word in words_list if word not in new_stopwords]\n",
    "    return text_list\n",
    "\n",
    "lemmitize_obj = nltk.WordNetLemmatizer()\n",
    "\n",
    "def _lemmatizer(tokenized_text):\n",
    "    text = [lemmitize_obj.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa741e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['comment_text_tokenized'] = main_df['comment_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "main_df['comment_text_stopword_removed'] = main_df['comment_text_tokenized'].apply(lambda x: stopwords_remover(x))\n",
    "main_df['comment_text_lemmatized'] = main_df['comment_text_stopword_removed'].apply(lambda x: _lemmatizer(x))\n",
    "\n",
    "test_df['comment_text_tokenized'] = test_df['comment_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "test_df['comment_text_stopword_removed'] = test_df['comment_text_tokenized'].apply(lambda x: stopwords_remover(x))\n",
    "test_df['comment_text_lemmatized'] = test_df['comment_text_stopword_removed'].apply(lambda x: _lemmatizer(x))\n",
    "\n",
    "wrk_df = main_df.copy()\n",
    "wrk_df =  wrk_df.drop(['comment_text','comment_text_stopword_removed','comment_text_tokenized'],axis = 1)\n",
    "wrk_df['comment_text_lemmatized']= wrk_df['comment_text_lemmatized'].map(lambda x: \" \".join(x) ) \n",
    "test_df['comment_text_lemmatized']= test_df['comment_text_lemmatized'].map(lambda x: \" \".join(x) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e201a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(lowercase = True, max_features=2000 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d377ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_sparse = tf_vect.fit_transform(wrk_df['comment_text_lemmatized'])\n",
    "test_features_sparse = tf_vect.transform(test_df['comment_text_lemmatized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f48da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['comment_text_lemmatized'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ClassifierChain(MultinomialNB(fit_prior=True)).fit(train_features_sparse, wrk_df[['toxic','severe_toxic','obscene','threat','insult','identity_hate']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(test_features_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_proba = clf.predict_proba(test_features_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dense = y_predict_proba.todense()\n",
    "print(predict_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(predict_dense,columns= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "\n",
    "submission_df_1 = pd.concat([test_df['id'], data_df] ,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_1.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
