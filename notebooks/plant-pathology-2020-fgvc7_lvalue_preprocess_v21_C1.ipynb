{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "tqdm.pandas()\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "!pip install efficientnet pandarallel\n",
    "import efficientnet.tfkeras as efn \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TPU = 'TPU_NAME' in os.environ\n",
    "if USE_TPU:\n",
    "    # detect and init the TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "    # instantiate a distribution strategy\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "IMAGE_PATH = \"../input/plant-pathology-2020-fgvc7/images/\"\n",
    "TEST_PATH = \"../input/plant-pathology-2020-fgvc7/test.csv\"\n",
    "TRAIN_PATH = \"../input/plant-pathology-2020-fgvc7/train.csv\"\n",
    "SUB_PATH = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"\n",
    "\n",
    "sub = pd.read_csv(SUB_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "train_data = pd.read_csv(TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_grabcut_mask(h, w):\n",
    "    mask = np.ones((h, w), np.uint8) * cv2.GC_PR_BGD\n",
    "    mask[h//4:3*h//4, w//4:3*w//4] = cv2.GC_PR_FGD\n",
    "    mask[2*h//5:3*h//5, 2*w//5:3*w//5] = cv2.GC_FGD\n",
    "    #mask[h//2, w//2] = cv2.GC_FGD\n",
    "    return mask\n",
    "\n",
    "\n",
    "def remove_background(image, h=136, w=205):\n",
    "    orig_image = image\n",
    "    image = cv2.resize(image, (w, h))\n",
    "    mask = init_grabcut_mask(h, w)\n",
    "    bgm = np.zeros((1, 65), np.float64)\n",
    "    fgm = np.zeros((1, 65), np.float64)\n",
    "    cv2.grabCut(image, mask, None, bgm, fgm, 1, cv2.GC_INIT_WITH_MASK)\n",
    "    mask_binary = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "    h, w = orig_image.shape[:2]\n",
    "    mask_binary = cv2.resize(mask_binary, (w, h))\n",
    "    result = cv2.bitwise_and(orig_image, orig_image, mask=mask_binary)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af363ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Rotation augmentation\n",
    "\n",
    "    Args:\n",
    "        x: Image\n",
    "\n",
    "    Returns:\n",
    "        Augmented image\n",
    "    \"\"\"\n",
    "\n",
    "    # Rotate 0, 90, 180, 270 degrees\n",
    "    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "\n",
    "\n",
    "def flip(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Flip augmentation\n",
    "\n",
    "    Args:\n",
    "        x: Image to flip\n",
    "\n",
    "    Returns:\n",
    "        Augmented image\n",
    "    \"\"\"\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    x = tf.image.random_flip_up_down(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def color(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Color augmentation\n",
    "\n",
    "    Args:\n",
    "        x: Image\n",
    "\n",
    "    Returns:\n",
    "        Augmented image\n",
    "    \"\"\"\n",
    "    x = tf.image.random_hue(x, 0.08)\n",
    "    x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    x = tf.image.random_brightness(x, 0.05)\n",
    "    x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    return x\n",
    "\n",
    "\n",
    "def zoom(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Zoom augmentation\n",
    "\n",
    "    Args:\n",
    "        x: Image\n",
    "\n",
    "    Returns:\n",
    "        Augmented image\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate 20 crop settings, ranging from a 1% to 20% crop.\n",
    "    scales = list(np.arange(0.8, 1.0, 0.01))\n",
    "    boxes = np.zeros((len(scales), 4))\n",
    "\n",
    "    for i, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - (0.5 * scale)\n",
    "        x2 = y2 = 0.5 + (0.5 * scale)\n",
    "        boxes[i] = [x1, y1, x2, y2]\n",
    "\n",
    "    def random_crop(img):\n",
    "        # Create different crops for an image\n",
    "        crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=(32, 32))\n",
    "        # Return a random crop\n",
    "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
    "\n",
    "\n",
    "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "\n",
    "    # Only apply cropping 50% of the time\n",
    "    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a540918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generators(preprocess=True, augment=True, IMAGE_SIZE=(3*136, 3*205)):\n",
    "    \n",
    "    def load_image(image_id):\n",
    "        file_path = image_id + \".jpg\"\n",
    "        image = cv2.imread(IMAGE_PATH + file_path)\n",
    "        image = cv2.resize(image, IMAGE_SIZE[::-1])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if preprocess:\n",
    "            image = remove_background(image)\n",
    "        return image\n",
    "\n",
    "    print(\"Preprocessing training images...\")\n",
    "    train_images = np.stack(train_data[\"image_id\"].parallel_apply(load_image))\n",
    "    plt.imshow(train_images[0])\n",
    "    labels = train_data[['healthy', 'multiple_diseases', 'rust', 'scab']]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_images, np.stack(labels.values)))\n",
    "    \n",
    "    def map_func(image, label):\n",
    "        image = tf.cast(image, tf.float32) / 255\n",
    "        return image, label\n",
    "    dataset = dataset.map(map_func, \n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                          deterministic=False)\n",
    "\n",
    "    if augment:\n",
    "        # Add augmentations\n",
    "        augmentations = [flip, color, rotate, zoom]\n",
    "    \n",
    "        # Add the augmentations to the dataset\n",
    "        for f in augmentations:\n",
    "            # Apply the augmentation, run 4 jobs in parallel.\n",
    "            dataset = dataset.map(lambda x, y: (f(x), y),\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                                  deterministic=False)\n",
    "\n",
    "        # Make sure that the values are still in [0, 1]\n",
    "        dataset = dataset.map(lambda x, y: (tf.clip_by_value(x, 0, 1), y), \n",
    "                              num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                              deterministic=False)\n",
    "    \n",
    "    train_size = int(len(train_data) * 0.85)\n",
    "    valid_size = len(train_data) - train_size\n",
    "    \n",
    "    train = dataset.take(train_size)\n",
    "    train = train.repeat().batch(32)\n",
    "    train = train.prefetch(2)\n",
    "    \n",
    "    valid = dataset.skip(train_size).take(valid_size)\n",
    "    valid = valid.repeat().batch(32)\n",
    "    valid = valid.prefetch(2)\n",
    "    \n",
    "    print(\"Preprocessing test images...\")\n",
    "    test_images = np.stack(test_data[\"image_id\"].parallel_apply(load_image))\n",
    "    test = tf.data.Dataset.from_tensor_slices((np.stack(test_images,)))\n",
    "    \n",
    "    test = test.map(lambda image: tf.cast(image, tf.float32) / 255, \n",
    "                    num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                    deterministic=False)\n",
    "    test = test.batch(32).prefetch(2)\n",
    "    \n",
    "    return train, valid, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = get_data_generators(False, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(): \n",
    "    model = keras.Sequential()\n",
    "    model.add(efn.EfficientNetB7(\n",
    "        include_top=False, weights='imagenet', input_tensor=None, input_shape=None,\n",
    "        pooling=None, classes=4))\n",
    "    model.add(keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(4, activation=\"softmax\"))\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = get_model()\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        metrics=[keras.metrics.categorical_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8553e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "history = model.fit(train,                                    \n",
    "    steps_per_epoch=50, \n",
    "    epochs=40,\n",
    "    validation_data=val,\n",
    "    validation_steps=50,\n",
    "    validation_freq=1,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        #ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.000001),\n",
    "        #TensorBoard(log_dir=\"tensorboard/\")\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34894990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "test_pr = model.predict(test, verbose=1)\n",
    "sub.loc[:, 'healthy':] = test_pr\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
