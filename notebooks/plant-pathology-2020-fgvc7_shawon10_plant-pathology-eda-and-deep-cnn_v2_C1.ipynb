{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271989cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684afb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import time, gc\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import clone_model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization,Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    #for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/sample_submission.csv\")\n",
    "test = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/test.csv\")\n",
    "train = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train['image_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac855617",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24917dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=[]\n",
    "for name in train['image_id']:\n",
    "    path='/kaggle/input/plant-pathology-2020-fgvc7/images/'+name+'.jpg'\n",
    "    img=cv2.imread(path)\n",
    "    image=cv2.resize(img,(img_size,img_size),interpolation=cv2.INTER_AREA)\n",
    "    train_image.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "for i in range(4):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(train_image[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=[]\n",
    "for name in test['image_id']:\n",
    "    path='/kaggle/input/plant-pathology-2020-fgvc7/images/'+name+'.jpg'\n",
    "    img=cv2.imread(path)\n",
    "    image=cv2.resize(img,(img_size,img_size),interpolation=cv2.INTER_AREA)\n",
    "    test_image.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab309f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "for i in range(4):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(test_image[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.image import img_to_array\n",
    "X_Train = np.ndarray(shape=(len(train_image), img_size, img_size, 3),dtype = np.float32)\n",
    "i=0\n",
    "for image in train_image:\n",
    "    #X_Train[i]=img_to_array(image)\n",
    "    X_Train[i]=train_image[i]\n",
    "    i=i+1\n",
    "X_Train=X_Train/255\n",
    "print('Train Shape: {}'.format(X_Train.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test = np.ndarray(shape=(len(test_image), img_size, img_size, 3),dtype = np.float32)\n",
    "i=0\n",
    "for image in test_image:\n",
    "    #X_Test[i]=img_to_array(image)\n",
    "    X_Test[i]=test_image[i]\n",
    "    i=i+1\n",
    "    \n",
    "X_Test=X_Test/255\n",
    "print('Test Shape: {}'.format(X_Test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7177fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.copy()\n",
    "del y['image_id']\n",
    "y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y.values)\n",
    "print(y_train.shape,y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', input_shape=(img_size, img_size, 3),\n",
    "                activation='relu'))\n",
    "    model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "   \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten()) # Flatten the input\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    # Configure the learning process\n",
    "    # The loss function is the objective that the model will try to minimize\n",
    "    # For any classification problem, use accuracy metric\n",
    "    optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_Train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = construct_model()\n",
    "annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
    "checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "# Generates batches of image data with data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
    "                        width_shift_range=0.2, # Range for random horizontal shifts\n",
    "                        height_shift_range=0.2, # Range for random vertical shifts\n",
    "                        zoom_range=0.2, # Range for random zoom\n",
    "                        horizontal_flip=True, # Randomly flip inputs horizontally\n",
    "                        vertical_flip=True) # Randomly flip inputs vertically\n",
    "\n",
    "datagen.fit(X_train)\n",
    "# Fits the model on batches with real-time data augmentation\n",
    "hist = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n",
    "               steps_per_epoch=X_train.shape[0] // 32,\n",
    "               epochs=80,\n",
    "               verbose=1,\n",
    "               callbacks=[annealer, checkpoint],\n",
    "               validation_data=(X_val, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4455c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_Test)\n",
    "all_predict = np.ndarray(shape = (test.shape[0],4),dtype = np.float32)\n",
    "for i in range(0,test.shape[0]):\n",
    "    for j in range(0,4):\n",
    "        if predict[i][j]==max(predict[i]):\n",
    "            all_predict[i][j] = 1\n",
    "        else:\n",
    "            all_predict[i][j] = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0740483",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy = [y_test[0] for y_test in all_predict]\n",
    "multiple_diseases = [y_test[1] for y_test in all_predict]\n",
    "rust = [y_test[2] for y_test in all_predict]\n",
    "scab = [y_test[3] for y_test in all_predict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'image_id':test.image_id,'healthy':healthy,'multiple_diseases':multiple_diseases,'rust':rust,'scab':scab}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f84467",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df)\n",
    "data.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('submission.csv',index = False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
